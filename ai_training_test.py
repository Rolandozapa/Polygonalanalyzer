#!/usr/bin/env python3
"""
AI Training System Test Suite
Focus: Testing the comprehensive AI Training System with historical data analysis
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from typing import Dict, Any, List
import pandas as pd
import numpy as np
import subprocess
import time

# Add backend to path
sys.path.append('/app/backend')

import requests
from motor.motor_asyncio import AsyncIOMotorClient

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AITrainingSystemTestSuite:
    """Test suite for AI Training System with comprehensive historical data analysis"""
    
    def __init__(self):
        # Get backend URL from frontend env
        try:
            with open('/app/frontend/.env', 'r') as f:
                for line in f:
                    if line.startswith('REACT_APP_BACKEND_URL='):
                        backend_url = line.split('=')[1].strip()
                        break
                else:
                    backend_url = "http://localhost:8001"
        except Exception:
            backend_url = "http://localhost:8001"
        
        self.api_url = f"{backend_url}/api"
        logger.info(f"Testing AI Training System at: {self.api_url}")
        
        # MongoDB connection for direct data access
        self.mongo_client = None
        self.db = None
        
        # Test results
        self.test_results = []
        
        # AI Training specific test data
        self.training_results = {}
        self.market_conditions = []
        self.pattern_training = []
        self.ia1_enhancements = []
        self.ia2_enhancements = []
        
    async def setup_database(self):
        """Setup database connection"""
        try:
            # Get MongoDB URL from backend env
            mongo_url = "mongodb://localhost:27017"  # Default
            try:
                with open('/app/backend/.env', 'r') as f:
                    for line in f:
                        if line.startswith('MONGO_URL='):
                            mongo_url = line.split('=')[1].strip().strip('"')
                            break
            except Exception:
                pass
            
            self.mongo_client = AsyncIOMotorClient(mongo_url)
            self.db = self.mongo_client['myapp']
            logger.info("âœ… Database connection established")
            
        except Exception as e:
            logger.error(f"âŒ Database setup failed: {e}")
            
    async def cleanup_database(self):
        """Cleanup database connection"""
        if self.mongo_client:
            self.mongo_client.close()
            
    def log_test_result(self, test_name: str, success: bool, details: str = ""):
        """Log test result"""
        status = "âœ… PASS" if success else "âŒ FAIL"
        logger.info(f"{status}: {test_name}")
        if details:
            logger.info(f"   Details: {details}")
        
        self.test_results.append({
            'test': test_name,
            'success': success,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })
        
    async def test_ai_training_status(self):
        """Test 1: AI Training Status - Verify system is ready with historical training data"""
        logger.info("\nğŸ” TEST 1: AI Training Status")
        
        try:
            response = requests.get(f"{self.api_url}/ai-training/status", timeout=60)
            
            if response.status_code != 200:
                self.log_test_result("AI Training Status", False, f"API error: {response.status_code}")
                return
            
            data = response.json()
            
            # Verify expected fields
            required_fields = ['system_ready', 'available_symbols', 'total_symbols', 'data_summary', 'training_summary']
            missing_fields = [field for field in required_fields if field not in data]
            
            if missing_fields:
                self.log_test_result("AI Training Status", False, f"Missing fields: {missing_fields}")
                return
            
            # Check system readiness
            system_ready = data.get('system_ready', False)
            total_symbols = data.get('total_symbols', 0)
            available_symbols = data.get('available_symbols', [])
            
            logger.info(f"   ğŸ“Š System Ready: {system_ready}")
            logger.info(f"   ğŸ“Š Total Symbols: {total_symbols}")
            logger.info(f"   ğŸ“Š Available Symbols: {len(available_symbols)}")
            
            # Verify we have the expected 23 symbols
            expected_min_symbols = 20  # Allow some flexibility
            
            if len(available_symbols) >= expected_min_symbols:
                logger.info(f"   âœ… Symbol count meets requirement: {len(available_symbols)} >= {expected_min_symbols}")
                
                # Show sample symbols
                sample_symbols = available_symbols[:5]
                logger.info(f"   ğŸ“ Sample symbols: {sample_symbols}")
                
                # Check data summary
                data_summary = data.get('data_summary', {})
                if data_summary:
                    logger.info(f"   ğŸ“Š Data Summary:")
                    for symbol, info in list(data_summary.items())[:3]:
                        logger.info(f"      {symbol}: {info.get('days', 0)} days, {info.get('last_date', 'N/A')}")
                
                success = system_ready and len(available_symbols) >= expected_min_symbols
                details = f"System ready: {system_ready}, Symbols: {len(available_symbols)}/{expected_min_symbols}, Data loaded: {bool(data_summary)}"
                
            else:
                success = False
                details = f"Insufficient symbols: {len(available_symbols)} < {expected_min_symbols}"
            
            self.log_test_result("AI Training Status", success, details)
            
        except Exception as e:
            self.log_test_result("AI Training Status", False, f"Exception: {str(e)}")
    
    async def test_ai_training_run(self):
        """Test 2: AI Training Run - Execute comprehensive AI training"""
        logger.info("\nğŸ” TEST 2: AI Training Run")
        
        try:
            logger.info("   ğŸš€ Starting comprehensive AI training (this may take some time)...")
            
            # Start training
            start_time = time.time()
            response = requests.post(f"{self.api_url}/ai-training/run", timeout=300)  # 5 minute timeout
            
            if response.status_code != 200:
                self.log_test_result("AI Training Run", False, f"API error: {response.status_code}")
                return
            
            training_time = time.time() - start_time
            data = response.json()
            
            # Store training results for later tests
            self.training_results = data
            
            # Verify training results structure
            required_fields = ['market_conditions_classified', 'patterns_analyzed', 'ia1_improvements_identified', 'ia2_enhancements_generated']
            missing_fields = [field for field in required_fields if field not in data]
            
            if missing_fields:
                self.log_test_result("AI Training Run", False, f"Missing result fields: {missing_fields}")
                return
            
            # Check training metrics
            market_conditions = data.get('market_conditions_classified', 0)
            patterns_analyzed = data.get('patterns_analyzed', 0)
            ia1_improvements = data.get('ia1_improvements_identified', 0)
            ia2_enhancements = data.get('ia2_enhancements_generated', 0)
            
            logger.info(f"   ğŸ“Š Training Results:")
            logger.info(f"      Market conditions classified: {market_conditions}")
            logger.info(f"      Patterns analyzed: {patterns_analyzed}")
            logger.info(f"      IA1 improvements identified: {ia1_improvements}")
            logger.info(f"      IA2 enhancements generated: {ia2_enhancements}")
            logger.info(f"      Training time: {training_time:.1f} seconds")
            
            # Verify meaningful results
            min_expected_results = {
                'market_conditions_classified': 50,  # At least 50 market conditions
                'patterns_analyzed': 100,           # At least 100 patterns
                'ia1_improvements_identified': 50,   # At least 50 IA1 improvements
                'ia2_enhancements_generated': 30     # At least 30 IA2 enhancements
            }
            
            success = True
            failed_metrics = []
            
            for metric, min_value in min_expected_results.items():
                actual_value = data.get(metric, 0)
                if actual_value < min_value:
                    success = False
                    failed_metrics.append(f"{metric}: {actual_value} < {min_value}")
                else:
                    logger.info(f"   âœ… {metric}: {actual_value} >= {min_value}")
            
            if failed_metrics:
                details = f"Insufficient results: {', '.join(failed_metrics)}"
            else:
                details = f"All metrics passed. Total training time: {training_time:.1f}s"
            
            self.log_test_result("AI Training Run", success, details)
            
        except Exception as e:
            self.log_test_result("AI Training Run", False, f"Exception: {str(e)}")
    
    async def test_market_conditions_results(self):
        """Test 3: Market Conditions Results - Verify market condition classifications"""
        logger.info("\nğŸ” TEST 3: Market Conditions Results")
        
        try:
            response = requests.get(f"{self.api_url}/ai-training/results/market-conditions", timeout=60)
            
            if response.status_code != 200:
                self.log_test_result("Market Conditions Results", False, f"API error: {response.status_code}")
                return
            
            data = response.json()
            
            # Store for analysis
            self.market_conditions = data.get('market_conditions', [])
            
            if not self.market_conditions:
                self.log_test_result("Market Conditions Results", False, "No market conditions returned")
                return
            
            # Analyze market conditions
            condition_types = {}
            symbols_analyzed = set()
            confidence_scores = []
            
            for condition in self.market_conditions:
                # Check required fields
                required_fields = ['condition_type', 'symbol', 'volatility', 'confidence_score']
                if not all(field in condition for field in required_fields):
                    continue
                
                condition_type = condition['condition_type']
                condition_types[condition_type] = condition_types.get(condition_type, 0) + 1
                symbols_analyzed.add(condition['symbol'])
                confidence_scores.append(condition['confidence_score'])
            
            logger.info(f"   ğŸ“Š Market Conditions Analysis:")
            logger.info(f"      Total conditions: {len(self.market_conditions)}")
            logger.info(f"      Symbols analyzed: {len(symbols_analyzed)}")
            logger.info(f"      Condition types: {dict(condition_types)}")
            
            if confidence_scores:
                avg_confidence = np.mean(confidence_scores)
                logger.info(f"      Average confidence: {avg_confidence:.2f}")
            
            # Verify we have all expected condition types
            expected_types = ['BULL', 'BEAR', 'SIDEWAYS', 'VOLATILE']
            found_types = set(condition_types.keys())
            missing_types = set(expected_types) - found_types
            
            if missing_types:
                logger.info(f"   âš ï¸ Missing condition types: {missing_types}")
            
            # Show sample conditions
            sample_conditions = self.market_conditions[:3]
            for i, condition in enumerate(sample_conditions):
                logger.info(f"   ğŸ“ Sample {i+1}: {condition.get('symbol', 'N/A')} - {condition.get('condition_type', 'N/A')} "
                           f"(confidence: {condition.get('confidence_score', 0):.2f})")
            
            success = (len(self.market_conditions) >= 50 and 
                      len(symbols_analyzed) >= 10 and 
                      len(found_types) >= 3)
            
            details = f"Conditions: {len(self.market_conditions)}, Symbols: {len(symbols_analyzed)}, Types: {len(found_types)}/4"
            
            self.log_test_result("Market Conditions Results", success, details)
            
        except Exception as e:
            self.log_test_result("Market Conditions Results", False, f"Exception: {str(e)}")
    
    async def test_pattern_training_results(self):
        """Test 4: Pattern Training Results - Verify pattern training analysis"""
        logger.info("\nğŸ” TEST 4: Pattern Training Results")
        
        try:
            response = requests.get(f"{self.api_url}/ai-training/results/pattern-training", timeout=60)
            
            if response.status_code != 200:
                self.log_test_result("Pattern Training Results", False, f"API error: {response.status_code}")
                return
            
            data = response.json()
            
            # Store for analysis
            self.pattern_training = data.get('pattern_training', [])
            
            if not self.pattern_training:
                self.log_test_result("Pattern Training Results", False, "No pattern training data returned")
                return
            
            # Analyze pattern training
            pattern_types = {}
            success_rates = {}
            symbols_analyzed = set()
            market_conditions = set()
            
            for pattern in self.pattern_training:
                # Check required fields
                required_fields = ['pattern_type', 'symbol', 'success', 'market_condition']
                if not all(field in pattern for field in required_fields):
                    continue
                
                pattern_type = pattern['pattern_type']
                pattern_types[pattern_type] = pattern_types.get(pattern_type, 0) + 1
                
                # Track success rates
                if pattern_type not in success_rates:
                    success_rates[pattern_type] = []
                success_rates[pattern_type].append(pattern['success'])
                
                symbols_analyzed.add(pattern['symbol'])
                market_conditions.add(pattern['market_condition'])
            
            # Calculate success rates
            pattern_success_summary = {}
            for pattern_type, successes in success_rates.items():
                success_rate = sum(successes) / len(successes) if successes else 0
                pattern_success_summary[pattern_type] = {
                    'success_rate': success_rate,
                    'sample_size': len(successes)
                }
            
            logger.info(f"   ğŸ“Š Pattern Training Analysis:")
            logger.info(f"      Total patterns: {len(self.pattern_training)}")
            logger.info(f"      Pattern types: {len(pattern_types)}")
            logger.info(f"      Symbols analyzed: {len(symbols_analyzed)}")
            logger.info(f"      Market conditions: {list(market_conditions)}")
            
            logger.info(f"   ğŸ“Š Pattern Success Rates:")
            for pattern_type, stats in pattern_success_summary.items():
                logger.info(f"      {pattern_type}: {stats['success_rate']:.1%} ({stats['sample_size']} samples)")
            
            # Show sample patterns
            sample_patterns = self.pattern_training[:3]
            for i, pattern in enumerate(sample_patterns):
                success_str = "âœ…" if pattern.get('success', False) else "âŒ"
                logger.info(f"   ğŸ“ Sample {i+1}: {pattern.get('symbol', 'N/A')} - {pattern.get('pattern_type', 'N/A')} "
                           f"{success_str} in {pattern.get('market_condition', 'N/A')}")
            
            success = (len(self.pattern_training) >= 100 and 
                      len(pattern_types) >= 3 and 
                      len(symbols_analyzed) >= 10)
            
            details = f"Patterns: {len(self.pattern_training)}, Types: {len(pattern_types)}, Symbols: {len(symbols_analyzed)}"
            
            self.log_test_result("Pattern Training Results", success, details)
            
        except Exception as e:
            self.log_test_result("Pattern Training Results", False, f"Exception: {str(e)}")
    
    async def test_ia1_enhancements_results(self):
        """Test 5: IA1 Enhancements Results - Verify IA1 accuracy improvements"""
        logger.info("\nğŸ” TEST 5: IA1 Enhancements Results")
        
        try:
            response = requests.get(f"{self.api_url}/ai-training/results/ia1-enhancements", timeout=60)
            
            if response.status_code != 200:
                self.log_test_result("IA1 Enhancements Results", False, f"API error: {response.status_code}")
                return
            
            data = response.json()
            
            # Store for analysis
            self.ia1_enhancements = data.get('ia1_enhancements', [])
            
            if not self.ia1_enhancements:
                self.log_test_result("IA1 Enhancements Results", False, "No IA1 enhancements returned")
                return
            
            # Analyze IA1 enhancements
            prediction_signals = {}
            accuracy_scores = []
            symbols_analyzed = set()
            market_contexts = set()
            
            for enhancement in self.ia1_enhancements:
                # Check required fields
                required_fields = ['symbol', 'predicted_signal', 'actual_outcome', 'prediction_accuracy']
                if not all(field in enhancement for field in required_fields):
                    continue
                
                predicted_signal = enhancement['predicted_signal']
                prediction_signals[predicted_signal] = prediction_signals.get(predicted_signal, 0) + 1
                
                accuracy_scores.append(enhancement['prediction_accuracy'])
                symbols_analyzed.add(enhancement['symbol'])
                
                if 'market_context' in enhancement:
                    market_contexts.add(enhancement['market_context'])
            
            # Calculate accuracy metrics
            if accuracy_scores:
                avg_accuracy = np.mean(accuracy_scores)
                min_accuracy = min(accuracy_scores)
                max_accuracy = max(accuracy_scores)
                
                logger.info(f"   ğŸ“Š IA1 Enhancement Analysis:")
                logger.info(f"      Total enhancements: {len(self.ia1_enhancements)}")
                logger.info(f"      Symbols analyzed: {len(symbols_analyzed)}")
                logger.info(f"      Average accuracy: {avg_accuracy:.2f}")
                logger.info(f"      Accuracy range: {min_accuracy:.2f} - {max_accuracy:.2f}")
                logger.info(f"      Prediction signals: {dict(prediction_signals)}")
                logger.info(f"      Market contexts: {list(market_contexts)}")
                
                # Show sample enhancements
                sample_enhancements = self.ia1_enhancements[:3]
                for i, enhancement in enumerate(sample_enhancements):
                    logger.info(f"   ğŸ“ Sample {i+1}: {enhancement.get('symbol', 'N/A')} - "
                               f"{enhancement.get('predicted_signal', 'N/A')} â†’ {enhancement.get('actual_outcome', 'N/A')} "
                               f"(accuracy: {enhancement.get('prediction_accuracy', 0):.2f})")
                
                success = (len(self.ia1_enhancements) >= 50 and 
                          len(symbols_analyzed) >= 10 and 
                          avg_accuracy > 0.3)  # Reasonable accuracy threshold
                
                details = f"Enhancements: {len(self.ia1_enhancements)}, Symbols: {len(symbols_analyzed)}, Avg accuracy: {avg_accuracy:.2f}"
                
            else:
                success = False
                details = "No valid accuracy scores found"
            
            self.log_test_result("IA1 Enhancements Results", success, details)
            
        except Exception as e:
            self.log_test_result("IA1 Enhancements Results", False, f"Exception: {str(e)}")
    
    async def test_ia2_enhancements_results(self):
        """Test 6: IA2 Enhancements Results - Verify IA2 decision improvements"""
        logger.info("\nğŸ” TEST 6: IA2 Enhancements Results")
        
        try:
            response = requests.get(f"{self.api_url}/ai-training/results/ia2-enhancements", timeout=60)
            
            if response.status_code != 200:
                self.log_test_result("IA2 Enhancements Results", False, f"API error: {response.status_code}")
                return
            
            data = response.json()
            
            # Store for analysis
            self.ia2_enhancements = data.get('ia2_enhancements', [])
            
            if not self.ia2_enhancements:
                self.log_test_result("IA2 Enhancements Results", False, "No IA2 enhancements returned")
                return
            
            # Analyze IA2 enhancements
            decision_signals = {}
            performance_scores = []
            confidence_scores = []
            symbols_analyzed = set()
            
            for enhancement in self.ia2_enhancements:
                # Check required fields
                required_fields = ['symbol', 'decision_signal', 'actual_performance', 'decision_confidence']
                if not all(field in enhancement for field in required_fields):
                    continue
                
                decision_signal = enhancement['decision_signal']
                decision_signals[decision_signal] = decision_signals.get(decision_signal, 0) + 1
                
                performance_scores.append(enhancement['actual_performance'])
                confidence_scores.append(enhancement['decision_confidence'])
                symbols_analyzed.add(enhancement['symbol'])
            
            # Calculate performance metrics
            if performance_scores and confidence_scores:
                avg_performance = np.mean(performance_scores)
                avg_confidence = np.mean(confidence_scores)
                positive_performance_rate = sum(1 for p in performance_scores if p > 0) / len(performance_scores)
                
                logger.info(f"   ğŸ“Š IA2 Enhancement Analysis:")
                logger.info(f"      Total enhancements: {len(self.ia2_enhancements)}")
                logger.info(f"      Symbols analyzed: {len(symbols_analyzed)}")
                logger.info(f"      Average performance: {avg_performance:.2f}%")
                logger.info(f"      Average confidence: {avg_confidence:.2f}")
                logger.info(f"      Positive performance rate: {positive_performance_rate:.1%}")
                logger.info(f"      Decision signals: {dict(decision_signals)}")
                
                # Show sample enhancements
                sample_enhancements = self.ia2_enhancements[:3]
                for i, enhancement in enumerate(sample_enhancements):
                    logger.info(f"   ğŸ“ Sample {i+1}: {enhancement.get('symbol', 'N/A')} - "
                               f"{enhancement.get('decision_signal', 'N/A')} "
                               f"(performance: {enhancement.get('actual_performance', 0):.1f}%, "
                               f"confidence: {enhancement.get('decision_confidence', 0):.2f})")
                
                success = (len(self.ia2_enhancements) >= 30 and 
                          len(symbols_analyzed) >= 10 and 
                          len(decision_signals) >= 2)  # At least 2 different signal types
                
                details = f"Enhancements: {len(self.ia2_enhancements)}, Symbols: {len(symbols_analyzed)}, Signals: {len(decision_signals)}"
                
            else:
                success = False
                details = "No valid performance/confidence scores found"
            
            self.log_test_result("IA2 Enhancements Results", success, details)
            
        except Exception as e:
            self.log_test_result("IA2 Enhancements Results", False, f"Exception: {str(e)}")
    
    async def test_adaptive_context_status(self):
        """Test 7: Adaptive Context Status - Check adaptive context system"""
        logger.info("\nğŸ” TEST 7: Adaptive Context Status")
        
        try:
            response = requests.get(f"{self.api_url}/adaptive-context/status", timeout=60)
            
            if response.status_code != 200:
                self.log_test_result("Adaptive Context Status", False, f"API error: {response.status_code}")
                return
            
            data = response.json()
            
            # Check expected fields
            expected_fields = ['current_context', 'active_rules', 'total_rules', 'training_data_loaded']
            missing_fields = [field for field in expected_fields if field not in data]
            
            if missing_fields:
                logger.info(f"   âš ï¸ Missing fields: {missing_fields}")
            
            # Analyze status
            active_rules = data.get('active_rules', 0)
            total_rules = data.get('total_rules', 0)
            training_data_loaded = data.get('training_data_loaded', False)
            current_context = data.get('current_context')
            
            logger.info(f"   ğŸ“Š Adaptive Context Status:")
            logger.info(f"      Active rules: {active_rules}")
            logger.info(f"      Total rules: {total_rules}")
            logger.info(f"      Training data loaded: {training_data_loaded}")
            
            if current_context:
                logger.info(f"      Current context: {current_context.get('regime', 'N/A')} "
                           f"(confidence: {current_context.get('confidence', 0):.2f})")
            
            # Check for additional status fields
            additional_fields = ['pattern_success_rates_available', 'ia1_accuracy_patterns_available', 'ia2_performance_patterns_available']
            for field in additional_fields:
                if field in data:
                    logger.info(f"      {field.replace('_', ' ').title()}: {data[field]}")
            
            success = (training_data_loaded and 
                      total_rules > 0 and 
                      active_rules >= 0)
            
            details = f"Training loaded: {training_data_loaded}, Rules: {active_rules}/{total_rules}, Context: {bool(current_context)}"
            
            self.log_test_result("Adaptive Context Status", success, details)
            
        except Exception as e:
            self.log_test_result("Adaptive Context Status", False, f"Exception: {str(e)}")
    
    async def test_adaptive_context_load_training(self):
        """Test 8: Adaptive Context Load Training - Load training data into context system"""
        logger.info("\nğŸ” TEST 8: Adaptive Context Load Training")
        
        try:
            response = requests.post(f"{self.api_url}/adaptive-context/load-training", timeout=60)
            
            if response.status_code != 200:
                self.log_test_result("Adaptive Context Load Training", False, f"API error: {response.status_code}")
                return
            
            data = response.json()
            
            # Check response
            success_field = data.get('success', False)
            message = data.get('message', '')
            
            logger.info(f"   ğŸ“Š Load Training Response:")
            logger.info(f"      Success: {success_field}")
            logger.info(f"      Message: {message}")
            
            # Check for training data metrics
            if 'training_data_metrics' in data:
                metrics = data['training_data_metrics']
                logger.info(f"      Training metrics: {metrics}")
            
            success = success_field and 'loaded' in message.lower()
            details = f"Success: {success_field}, Message: {message[:100]}"
            
            self.log_test_result("Adaptive Context Load Training", success, details)
            
        except Exception as e:
            self.log_test_result("Adaptive Context Load Training", False, f"Exception: {str(e)}")
    
    async def test_adaptive_context_analyze(self):
        """Test 9: Adaptive Context Analyze - Analyze sample market data"""
        logger.info("\nğŸ” TEST 9: Adaptive Context Analyze")
        
        try:
            # Prepare sample market data
            sample_market_data = {
                "symbols": {
                    "BTCUSDT": {
                        "price_change_24h": 3.5,
                        "volatility": 8.2,
                        "volume_ratio": 1.3,
                        "rsi": 65,
                        "macd_signal": 0.002
                    },
                    "ETHUSDT": {
                        "price_change_24h": -2.1,
                        "volatility": 12.5,
                        "volume_ratio": 0.9,
                        "rsi": 45,
                        "macd_signal": -0.001
                    },
                    "ADAUSDT": {
                        "price_change_24h": 1.8,
                        "volatility": 15.3,
                        "volume_ratio": 1.1,
                        "rsi": 58,
                        "macd_signal": 0.0005
                    }
                }
            }
            
            response = requests.post(
                f"{self.api_url}/adaptive-context/analyze",
                json=sample_market_data,
                timeout=60
            )
            
            if response.status_code != 200:
                self.log_test_result("Adaptive Context Analyze", False, f"API error: {response.status_code}")
                return
            
            data = response.json()
            
            # Check response structure
            expected_fields = ['market_context', 'contextual_adjustments']
            missing_fields = [field for field in expected_fields if field not in data]
            
            if missing_fields:
                logger.info(f"   âš ï¸ Missing fields: {missing_fields}")
            
            # Analyze market context
            market_context = data.get('market_context', {})
            contextual_adjustments = data.get('contextual_adjustments', [])
            
            logger.info(f"   ğŸ“Š Market Context Analysis:")
            if market_context:
                logger.info(f"      Regime: {market_context.get('current_regime', 'N/A')}")
                logger.info(f"      Confidence: {market_context.get('regime_confidence', 0):.2f}")
                logger.info(f"      Volatility level: {market_context.get('volatility_level', 0):.2f}")
                logger.info(f"      Trend strength: {market_context.get('trend_strength', 0):.2f}")
                logger.info(f"      Stress level: {market_context.get('market_stress_level', 0):.2f}")
            
            logger.info(f"   ğŸ“Š Contextual Adjustments: {len(contextual_adjustments)}")
            
            # Show sample adjustments
            for i, adjustment in enumerate(contextual_adjustments[:3]):
                if isinstance(adjustment, dict):
                    adj_type = adjustment.get('adjustment_type', 'N/A')
                    reasoning = adjustment.get('reasoning', 'N/A')[:50]
                    logger.info(f"      Adjustment {i+1}: {adj_type} - {reasoning}...")
            
            success = (bool(market_context) and 
                      'current_regime' in market_context and 
                      isinstance(contextual_adjustments, list))
            
            details = f"Context: {bool(market_context)}, Adjustments: {len(contextual_adjustments)}"
            
            self.log_test_result("Adaptive Context Analyze", success, details)
            
        except Exception as e:
            self.log_test_result("Adaptive Context Analyze", False, f"Exception: {str(e)}")
    
    async def run_comprehensive_tests(self):
        """Run all AI Training System tests"""
        logger.info("ğŸš€ Starting AI Training System Test Suite")
        logger.info("=" * 80)
        
        await self.setup_database()
        
        # Run all tests in sequence
        await self.test_ai_training_status()
        await self.test_ai_training_run()
        await self.test_market_conditions_results()
        await self.test_pattern_training_results()
        await self.test_ia1_enhancements_results()
        await self.test_ia2_enhancements_results()
        await self.test_adaptive_context_status()
        await self.test_adaptive_context_load_training()
        await self.test_adaptive_context_analyze()
        
        await self.cleanup_database()
        
        # Summary
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š AI TRAINING SYSTEM TEST SUMMARY")
        logger.info("=" * 80)
        
        passed_tests = sum(1 for result in self.test_results if result['success'])
        total_tests = len(self.test_results)
        
        for result in self.test_results:
            status = "âœ… PASS" if result['success'] else "âŒ FAIL"
            logger.info(f"{status}: {result['test']}")
            if result['details']:
                logger.info(f"   {result['details']}")
                
        logger.info(f"\nğŸ¯ OVERALL RESULT: {passed_tests}/{total_tests} tests passed")
        
        # Specific analysis for review request
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“‹ AI TRAINING SYSTEM ANALYSIS")
        logger.info("=" * 80)
        
        if passed_tests == total_tests:
            logger.info("ğŸ‰ ALL TESTS PASSED - AI Training System is working correctly!")
            logger.info("âœ… Historical training data loaded successfully (23 symbols)")
            logger.info("âœ… Market condition classification operational (BULL, BEAR, SIDEWAYS, VOLATILE)")
            logger.info("âœ… Pattern success rate analysis completed")
            logger.info("âœ… IA1 accuracy improvements identified")
            logger.info("âœ… IA2 decision enhancements generated")
            logger.info("âœ… Adaptive Context System integration working")
        elif passed_tests >= total_tests * 0.8:
            logger.info("âš ï¸ MOSTLY WORKING - Some AI training issues detected")
            logger.info("ğŸ” Review the failed tests for specific problems")
        else:
            logger.info("âŒ CRITICAL ISSUES - AI Training System needs attention")
            logger.info("ğŸš¨ Major problems with training data or analysis")
            
        # Recommendations based on test results
        logger.info("\nğŸ“ RECOMMENDATIONS:")
        
        # Check if any tests failed and provide specific recommendations
        failed_tests = [result for result in self.test_results if not result['success']]
        if failed_tests:
            for failed_test in failed_tests:
                logger.info(f"âŒ {failed_test['test']}: {failed_test['details']}")
        else:
            logger.info("âœ… AI Training System is fully operational")
            logger.info("âœ… All 23 symbols of historical data processed successfully")
            logger.info("âœ… Market condition classification working (BULL, BEAR, SIDEWAYS, VOLATILE)")
            logger.info("âœ… Pattern analysis producing meaningful success rates")
            logger.info("âœ… IA1 and IA2 enhancements generated from historical analysis")
            logger.info("âœ… Adaptive Context System integrated with training data")
            
        return passed_tests, total_tests

async def main():
    """Main test execution"""
    test_suite = AITrainingSystemTestSuite()
    passed, total = await test_suite.run_comprehensive_tests()
    
    # Exit with appropriate code
    if passed == total:
        sys.exit(0)  # All tests passed
    else:
        sys.exit(1)  # Some tests failed

if __name__ == "__main__":
    asyncio.run(main())