#!/usr/bin/env python3
"""
CONFLUENCE ANALYSIS FIX TESTING SUITE
Focus: Test the confluence analysis fix to validate that confluence values are displaying correctly.

CRITICAL VALIDATION POINTS:

1. **API Force IA1 Analysis** - Test with 3 symbols (BTCUSDT, ETHUSDT, LINKUSDT):
   - Verify confluence_grade is not null (should be A, B, C, or D)  
   - Verify confluence_score is not null (should be 0-100)
   - Verify should_trade is not null (should be true/false)
   - Confirm values match IA1 reasoning

2. **API Analyses Endpoint** - Check consistency:
   - Confluence values in /api/analyses are consistent
   - No default values (50/100) used as fallbacks
   - Scores reflect real market conditions

3. **Validation of Calculations** - Check logic:
   - Confluence scores 0 correspond to Grade D and should_trade=false
   - Values reflect real conditions (not fallbacks)
   - Consistency between IA1 reasoning and API values

4. **Test Diversity** - Check with multiple symbols:
   - Different confluence grades according to conditions
   - Variety in scores (not always 0 or 50)
   - Should_trade varies according to setup quality

FOCUS: Confirm confluence analysis fix is complete and working correctly - real calculated values appear in API instead of null/fallbacks.
"""

import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List
import requests
import subprocess
import re
from pymongo import MongoClient

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ConfluenceAnalysisTestSuite:
    """Comprehensive test suite for confluence analysis fix validation"""
    
    def __init__(self):
        # Get backend URL from frontend env
        try:
            with open('/app/frontend/.env', 'r') as f:
                for line in f:
                    if line.startswith('REACT_APP_BACKEND_URL='):
                        backend_url = line.split('=')[1].strip()
                        break
                else:
                    backend_url = "http://localhost:8001"
        except Exception:
            backend_url = "http://localhost:8001"
        
        self.api_url = f"{backend_url}/api"
        logger.info(f"Testing Confluence Analysis Fix at: {self.api_url}")
        
        # Test results
        self.test_results = []
        
        # Test symbols for analysis (from review request)
        self.test_symbols = ['BTCUSDT', 'ETHUSDT', 'LINKUSDT']  # Specific symbols from review request
        self.actual_test_symbols = []  # Will be populated from available opportunities
        
        # Expected confluence fields that should be present and not null
        self.expected_confluence_fields = ['confluence_grade', 'confluence_score', 'should_trade']
        
        # Valid confluence grades
        self.valid_confluence_grades = ['A', 'B', 'C', 'D']
        
        # Error patterns to check for in logs
        self.error_patterns = [
            "confluence_grade.*null",
            "confluence_score.*null", 
            "should_trade.*null",
            "confluence.*fallback",
            "confluence.*default"
        ]
        
        # Technical analysis data storage
        self.technical_analyses = []
        self.backend_logs = []
        
        # Database connection info
        self.mongo_url = "mongodb://localhost:27017"
        self.db_name = "myapp"
        
    def log_test_result(self, test_name: str, success: bool, details: str = ""):
        """Log test result"""
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        logger.info(f"{status}: {test_name}")
        if details:
            logger.info(f"   Details: {details}")
        
        self.test_results.append({
            'test': test_name,
            'success': success,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })
    
    async def _capture_backend_logs(self):
        """Capture backend logs for analysis"""
        try:
            # Try to capture supervisor backend logs
            result = subprocess.run(
                ['tail', '-n', '200', '/var/log/supervisor/backend.out.log'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0 and result.stdout:
                return result.stdout.split('\n')
            else:
                # Try alternative log location
                result = subprocess.run(
                    ['tail', '-n', '200', '/var/log/supervisor/backend.err.log'],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                
                if result.returncode == 0 and result.stdout:
                    return result.stdout.split('\n')
                else:
                    return []
                    
        except Exception as e:
            logger.warning(f"Could not capture backend logs: {e}")
            return []
    
    async def test_1_api_force_ia1_analysis_confluence(self):
        """Test 1: API Force IA1 Analysis - Test confluence values with 3 symbols (BTCUSDT, ETHUSDT, LINKUSDT)"""
        logger.info("\nüîç TEST 1: API Force IA1 Analysis - Confluence Analysis Fix Validation")
        
        try:
            analysis_results = {
                'analyses_attempted': 0,
                'analyses_successful': 0,
                'confluence_grade_not_null': 0,
                'confluence_score_not_null': 0,
                'should_trade_not_null': 0,
                'valid_confluence_grades': 0,
                'valid_confluence_scores': 0,
                'confluence_reasoning_present': 0,
                'successful_analyses': [],
                'error_details': [],
                'response_times': [],
                'confluence_data': []
            }
            
            logger.info("   üöÄ Testing IA1 analysis with specific symbols to verify confluence values are not null...")
            logger.info("   üìä Expected: confluence_grade (A,B,C,D), confluence_score (0-100), should_trade (true/false)")
            
            # Get available symbols from scout system
            logger.info("   üìû Getting available symbols from scout system...")
            
            try:
                response = requests.get(f"{self.api_url}/opportunities", timeout=60)
                
                if response.status_code == 200:
                    opportunities = response.json()
                    if isinstance(opportunities, dict) and 'opportunities' in opportunities:
                        opportunities = opportunities['opportunities']
                    
                    # Get available symbols for testing
                    available_symbols = [opp.get('symbol') for opp in opportunities[:15] if opp.get('symbol')]
                    
                    # Prefer specific symbols from review request
                    preferred_symbols = ['BTCUSDT', 'ETHUSDT', 'LINKUSDT']
                    test_symbols = []
                    
                    for symbol in preferred_symbols:
                        if symbol in available_symbols:
                            test_symbols.append(symbol)
                    
                    # Fill remaining slots with available symbols if needed
                    for symbol in available_symbols:
                        if symbol not in test_symbols and len(test_symbols) < 3:
                            test_symbols.append(symbol)
                    
                    self.actual_test_symbols = test_symbols[:3]  # Limit to 3 symbols as per review request
                    logger.info(f"      ‚úÖ Test symbols selected: {self.actual_test_symbols}")
                    
                else:
                    logger.warning(f"      ‚ö†Ô∏è Could not get opportunities, using default symbols")
                    self.actual_test_symbols = self.test_symbols
                    
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Error getting opportunities: {e}, using default symbols")
                self.actual_test_symbols = self.test_symbols
            
            # Test each symbol for confluence analysis validation
            for symbol in self.actual_test_symbols:
                logger.info(f"\n   üìû Testing IA1 analysis for {symbol} - checking confluence values...")
                analysis_results['analyses_attempted'] += 1
                
                try:
                    start_time = time.time()
                    response = requests.post(
                        f"{self.api_url}/force-ia1-analysis",
                        json={"symbol": symbol},
                        timeout=120
                    )
                    response_time = time.time() - start_time
                    analysis_results['response_times'].append(response_time)
                    
                    if response.status_code == 200:
                        analysis_data = response.json()
                        analysis_results['analyses_successful'] += 1
                        
                        logger.info(f"      ‚úÖ {symbol} analysis successful (response time: {response_time:.2f}s)")
                        
                        # Extract IA1 analysis data
                        ia1_analysis = analysis_data.get('ia1_analysis', {})
                        if not isinstance(ia1_analysis, dict):
                            ia1_analysis = {}
                        
                        # Check confluence_grade
                        confluence_grade = ia1_analysis.get('confluence_grade')
                        if confluence_grade is not None and confluence_grade != 'null':
                            analysis_results['confluence_grade_not_null'] += 1
                            logger.info(f"         ‚úÖ confluence_grade present: {confluence_grade}")
                            
                            if confluence_grade in self.valid_confluence_grades:
                                analysis_results['valid_confluence_grades'] += 1
                                logger.info(f"         ‚úÖ confluence_grade valid: {confluence_grade}")
                            else:
                                logger.warning(f"         ‚ö†Ô∏è confluence_grade invalid: {confluence_grade} (expected A,B,C,D)")
                        else:
                            logger.error(f"         ‚ùå confluence_grade is null or missing")
                        
                        # Check confluence_score
                        confluence_score = ia1_analysis.get('confluence_score')
                        if confluence_score is not None and confluence_score != 'null':
                            analysis_results['confluence_score_not_null'] += 1
                            logger.info(f"         ‚úÖ confluence_score present: {confluence_score}")
                            
                            # Check if score is in valid range (0-100)
                            try:
                                score_value = float(confluence_score)
                                if 0 <= score_value <= 100:
                                    analysis_results['valid_confluence_scores'] += 1
                                    logger.info(f"         ‚úÖ confluence_score valid range: {score_value}")
                                else:
                                    logger.warning(f"         ‚ö†Ô∏è confluence_score out of range: {score_value} (expected 0-100)")
                            except (ValueError, TypeError):
                                logger.warning(f"         ‚ö†Ô∏è confluence_score not numeric: {confluence_score}")
                        else:
                            logger.error(f"         ‚ùå confluence_score is null or missing")
                        
                        # Check should_trade
                        should_trade = ia1_analysis.get('should_trade')
                        if should_trade is not None and should_trade != 'null':
                            analysis_results['should_trade_not_null'] += 1
                            logger.info(f"         ‚úÖ should_trade present: {should_trade}")
                            
                            if isinstance(should_trade, bool) or should_trade in ['true', 'false', True, False]:
                                logger.info(f"         ‚úÖ should_trade valid boolean: {should_trade}")
                            else:
                                logger.warning(f"         ‚ö†Ô∏è should_trade not boolean: {should_trade}")
                        else:
                            logger.error(f"         ‚ùå should_trade is null or missing")
                        
                        # Check for confluence reasoning in IA1 reasoning
                        reasoning = ia1_analysis.get('reasoning', '')
                        if reasoning and 'confluence' in reasoning.lower():
                            analysis_results['confluence_reasoning_present'] += 1
                            logger.info(f"         ‚úÖ Confluence reasoning present in IA1 analysis")
                        else:
                            logger.warning(f"         ‚ö†Ô∏è No confluence reasoning found in IA1 analysis")
                        
                        # Store confluence data for analysis
                        confluence_data = {
                            'symbol': symbol,
                            'confluence_grade': confluence_grade,
                            'confluence_score': confluence_score,
                            'should_trade': should_trade,
                            'has_reasoning': bool(reasoning and 'confluence' in reasoning.lower()),
                            'response_time': response_time
                        }
                        analysis_results['confluence_data'].append(confluence_data)
                        
                        # Store successful analysis details
                        analysis_results['successful_analyses'].append({
                            'symbol': symbol,
                            'response_time': response_time,
                            'confluence_data': confluence_data,
                            'analysis_data': ia1_analysis
                        })
                        
                    elif response.status_code == 500:
                        # Check for confluence-related errors
                        error_text = response.text
                        logger.error(f"      ‚ùå {symbol} analysis failed: HTTP 500")
                        logger.error(f"         Error response: {error_text[:500]}")
                        
                        # Check for confluence calculation errors
                        confluence_error_found = False
                        if any(pattern in error_text.lower() for pattern in ["confluence", "grade", "score"]):
                            confluence_error_found = True
                            logger.error(f"         üö® CONFLUENCE ERROR DETECTED in {symbol}")
                        
                        analysis_results['error_details'].append({
                            'symbol': symbol,
                            'error_type': 'HTTP_500',
                            'error_text': error_text[:500],
                            'has_confluence_error': confluence_error_found
                        })
                        
                    else:
                        logger.error(f"      ‚ùå {symbol} analysis failed: HTTP {response.status_code}")
                        if response.text:
                            error_text = response.text[:300]
                            logger.error(f"         Error response: {error_text}")
                            analysis_results['error_details'].append({
                                'symbol': symbol,
                                'error_type': f'HTTP_{response.status_code}',
                                'error_text': error_text
                            })
                
                except Exception as e:
                    logger.error(f"      ‚ùå {symbol} analysis exception: {e}")
                    analysis_results['error_details'].append({
                        'symbol': symbol,
                        'error_type': 'EXCEPTION',
                        'error_text': str(e)
                    })
                
                # Wait between analyses
                if symbol != self.actual_test_symbols[-1]:
                    logger.info(f"      ‚è≥ Waiting 5 seconds before next analysis...")
                    await asyncio.sleep(5)
            
            # Capture backend logs to check for confluence calculation logs
            logger.info("   üìã Capturing backend logs to check for confluence calculation logs...")
            
            try:
                backend_logs = await self._capture_backend_logs()
                if backend_logs:
                    # Look for confluence-related log patterns
                    confluence_calculation_logs = []
                    confluence_error_logs = []
                    success_logs = []
                    
                    for log_line in backend_logs:
                        log_lower = log_line.lower()
                        
                        # Check for confluence calculation logs
                        if any(pattern in log_lower for pattern in ['confluence', 'grade', 'score']):
                            confluence_calculation_logs.append(log_line.strip())
                        
                        # Check for confluence errors
                        if any(pattern in log_lower for pattern in ['confluence.*error', 'confluence.*null', 'confluence.*failed']):
                            confluence_error_logs.append(log_line.strip())
                        
                        # Check for successful IA1 completions
                        if any(pattern in log_lower for pattern in ['ia1 analysis completed', 'ia1 ultra analysis', 'analysis successful']):
                            success_logs.append(log_line.strip())
                    
                    logger.info(f"      üìä Backend logs analysis:")
                    logger.info(f"         - Confluence calculation logs: {len(confluence_calculation_logs)}")
                    logger.info(f"         - Confluence error logs: {len(confluence_error_logs)}")
                    logger.info(f"         - Success logs: {len(success_logs)}")
                    
                    # Show sample logs
                    if confluence_calculation_logs:
                        logger.info(f"      ‚úÖ Sample confluence log: {confluence_calculation_logs[0]}")
                    if confluence_error_logs:
                        logger.error(f"      üö® CONFLUENCE ERROR FOUND: {confluence_error_logs[0]}")
                    if success_logs:
                        logger.info(f"      ‚úÖ Sample success log: {success_logs[0]}")
                        
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Could not analyze backend logs: {e}")
            
            # Final analysis and results
            success_rate = analysis_results['analyses_successful'] / max(analysis_results['analyses_attempted'], 1)
            confluence_grade_rate = analysis_results['confluence_grade_not_null'] / max(analysis_results['analyses_successful'], 1)
            confluence_score_rate = analysis_results['confluence_score_not_null'] / max(analysis_results['analyses_successful'], 1)
            should_trade_rate = analysis_results['should_trade_not_null'] / max(analysis_results['analyses_successful'], 1)
            valid_grades_rate = analysis_results['valid_confluence_grades'] / max(analysis_results['confluence_grade_not_null'], 1)
            valid_scores_rate = analysis_results['valid_confluence_scores'] / max(analysis_results['confluence_score_not_null'], 1)
            avg_response_time = sum(analysis_results['response_times']) / max(len(analysis_results['response_times']), 1)
            
            logger.info(f"\n   üìä API FORCE IA1 ANALYSIS CONFLUENCE RESULTS:")
            logger.info(f"      Analyses attempted: {analysis_results['analyses_attempted']}")
            logger.info(f"      Analyses successful: {analysis_results['analyses_successful']}")
            logger.info(f"      Success rate: {success_rate:.2f}")
            logger.info(f"      confluence_grade not null: {analysis_results['confluence_grade_not_null']} ({confluence_grade_rate:.2f})")
            logger.info(f"      confluence_score not null: {analysis_results['confluence_score_not_null']} ({confluence_score_rate:.2f})")
            logger.info(f"      should_trade not null: {analysis_results['should_trade_not_null']} ({should_trade_rate:.2f})")
            logger.info(f"      Valid confluence grades: {analysis_results['valid_confluence_grades']} ({valid_grades_rate:.2f})")
            logger.info(f"      Valid confluence scores: {analysis_results['valid_confluence_scores']} ({valid_scores_rate:.2f})")
            logger.info(f"      Confluence reasoning present: {analysis_results['confluence_reasoning_present']}")
            logger.info(f"      Average response time: {avg_response_time:.2f}s")
            
            # Show confluence data details
            if analysis_results['confluence_data']:
                logger.info(f"      üìä Confluence Data Details:")
                for data in analysis_results['confluence_data']:
                    logger.info(f"         - {data['symbol']}: grade={data['confluence_grade']}, score={data['confluence_score']}, trade={data['should_trade']}, reasoning={data['has_reasoning']}")
            
            # Show error details if any
            if analysis_results['error_details']:
                logger.info(f"      üìä Error Details:")
                for error in analysis_results['error_details']:
                    logger.info(f"         - {error['symbol']}: {error['error_type']}")
            
            # Calculate test success based on review requirements
            success_criteria = [
                analysis_results['analyses_successful'] >= 2,  # At least 2 successful analyses
                analysis_results['confluence_grade_not_null'] >= 2,  # At least 2 with confluence_grade not null
                analysis_results['confluence_score_not_null'] >= 2,  # At least 2 with confluence_score not null
                analysis_results['should_trade_not_null'] >= 2,  # At least 2 with should_trade not null
                analysis_results['valid_confluence_grades'] >= 1,  # At least 1 valid grade
                success_rate >= 0.67  # At least 67% success rate (2/3)
            ]
            success_count = sum(success_criteria)
            test_success_rate = success_count / len(success_criteria)
            
            if test_success_rate >= 0.83:  # 83% success threshold (5/6 criteria)
                self.log_test_result("API Force IA1 Analysis Confluence", True, 
                                   f"Confluence analysis successful: {success_count}/{len(success_criteria)} criteria met. Grade rate: {confluence_grade_rate:.2f}, Score rate: {confluence_score_rate:.2f}, Trade rate: {should_trade_rate:.2f}")
            else:
                self.log_test_result("API Force IA1 Analysis Confluence", False, 
                                   f"Confluence analysis issues: {success_count}/{len(success_criteria)} criteria met. Many confluence values still null or invalid")
                
        except Exception as e:
            self.log_test_result("API Force IA1 Analysis", False, f"Exception: {str(e)}")
    async def test_2_api_analyses_confluence(self):
        """Test 2: API Analyses Endpoint - Check confluence consistency in stored analyses"""
        logger.info("\nüîç TEST 2: API Analyses Endpoint - Confluence Consistency Validation")
        
        try:
            analyses_results = {
                'api_call_successful': False,
                'analyses_returned': 0,
                'confluence_fields_present': 0,
                'confluence_grades_not_null': 0,
                'confluence_scores_not_null': 0,
                'should_trade_not_null': 0,
                'default_fallback_values': 0,
                'diverse_confluence_scores': 0,
                'analyses_data': [],
                'error_details': []
            }
            
            logger.info("   üöÄ Testing /api/analyses endpoint for confluence consistency...")
            logger.info("   üìä Expected: Confluence values consistent, no default fallbacks (50/100), real market conditions")
            
            # Test /api/analyses endpoint
            logger.info("   üìû Calling /api/analyses endpoint...")
            
            try:
                start_time = time.time()
                response = requests.get(f"{self.api_url}/analyses", timeout=60)
                response_time = time.time() - start_time
                
                if response.status_code == 200:
                    analyses_results['api_call_successful'] = True
                    logger.info(f"      ‚úÖ /api/analyses successful (response time: {response_time:.2f}s)")
                    
                    # Parse response
                    try:
                        data = response.json()
                        
                        # Handle different response formats
                        if isinstance(data, dict) and 'analyses' in data:
                            analyses = data['analyses']
                        elif isinstance(data, list):
                            analyses = data
                        else:
                            analyses = []
                        
                        analyses_results['analyses_returned'] = len(analyses)
                        logger.info(f"      üìä Analyses returned: {len(analyses)}")
                        
                        if len(analyses) > 0:
                            confluence_scores = []
                            
                            # Analyze first 20 analyses for confluence validation
                            for i, analysis in enumerate(analyses[:20]):
                                if not isinstance(analysis, dict):
                                    continue
                                
                                # Check for confluence fields
                                confluence_grade = analysis.get('confluence_grade')
                                confluence_score = analysis.get('confluence_score')
                                should_trade = analysis.get('should_trade')
                                
                                has_confluence_fields = any([
                                    confluence_grade is not None,
                                    confluence_score is not None,
                                    should_trade is not None
                                ])
                                
                                if has_confluence_fields:
                                    analyses_results['confluence_fields_present'] += 1
                                
                                # Check confluence_grade
                                if confluence_grade is not None and confluence_grade != 'null':
                                    analyses_results['confluence_grades_not_null'] += 1
                                
                                # Check confluence_score
                                if confluence_score is not None and confluence_score != 'null':
                                    analyses_results['confluence_scores_not_null'] += 1
                                    
                                    # Check for default fallback values (50, 100)
                                    try:
                                        score_value = float(confluence_score)
                                        confluence_scores.append(score_value)
                                        
                                        if score_value in [50.0, 100.0]:
                                            analyses_results['default_fallback_values'] += 1
                                    except (ValueError, TypeError):
                                        pass
                                
                                # Check should_trade
                                if should_trade is not None and should_trade != 'null':
                                    analyses_results['should_trade_not_null'] += 1
                                
                                # Store sample analysis data
                                if i < 5:  # Store first 5 for analysis
                                    analyses_results['analyses_data'].append({
                                        'symbol': analysis.get('symbol', 'UNKNOWN'),
                                        'confluence_grade': confluence_grade,
                                        'confluence_score': confluence_score,
                                        'should_trade': should_trade,
                                        'timestamp': analysis.get('timestamp', 'N/A'),
                                        'has_confluence_fields': has_confluence_fields
                                    })
                                    
                                    logger.info(f"         üìã Sample {i+1} ({analysis.get('symbol', 'UNKNOWN')}): grade={confluence_grade}, score={confluence_score}, trade={should_trade}")
                            
                            # Check for diversity in confluence scores (not all the same)
                            if confluence_scores:
                                unique_scores = len(set(confluence_scores))
                                if unique_scores > 1:
                                    analyses_results['diverse_confluence_scores'] = unique_scores
                                    logger.info(f"         ‚úÖ Diverse confluence scores found: {unique_scores} unique values")
                                else:
                                    logger.warning(f"         ‚ö†Ô∏è All confluence scores are the same: {confluence_scores[0] if confluence_scores else 'N/A'}")
                        
                        else:
                            logger.warning(f"      ‚ö†Ô∏è No analyses returned from API")
                            
                    except json.JSONDecodeError as e:
                        logger.error(f"      ‚ùå Invalid JSON response: {e}")
                        analyses_results['error_details'].append(f"JSON decode error: {e}")
                        
                else:
                    logger.error(f"      ‚ùå /api/analyses failed: HTTP {response.status_code}")
                    if response.text:
                        error_text = response.text[:500]
                        logger.error(f"         Error response: {error_text}")
                        analyses_results['error_details'].append(f"HTTP {response.status_code}: {error_text}")
                    
            except Exception as e:
                logger.error(f"      ‚ùå /api/analyses exception: {e}")
                analyses_results['error_details'].append(f"Exception: {str(e)}")
            
            # Final analysis and results
            confluence_fields_rate = analyses_results['confluence_fields_present'] / max(analyses_results['analyses_returned'], 1)
            confluence_grades_rate = analyses_results['confluence_grades_not_null'] / max(analyses_results['analyses_returned'], 1)
            confluence_scores_rate = analyses_results['confluence_scores_not_null'] / max(analyses_results['analyses_returned'], 1)
            should_trade_rate = analyses_results['should_trade_not_null'] / max(analyses_results['analyses_returned'], 1)
            fallback_rate = analyses_results['default_fallback_values'] / max(analyses_results['confluence_scores_not_null'], 1)
            
            logger.info(f"\n   üìä API ANALYSES CONFLUENCE RESULTS:")
            logger.info(f"      API call successful: {analyses_results['api_call_successful']}")
            logger.info(f"      Analyses returned: {analyses_results['analyses_returned']}")
            logger.info(f"      Confluence fields present: {analyses_results['confluence_fields_present']} ({confluence_fields_rate:.2f})")
            logger.info(f"      Confluence grades not null: {analyses_results['confluence_grades_not_null']} ({confluence_grades_rate:.2f})")
            logger.info(f"      Confluence scores not null: {analyses_results['confluence_scores_not_null']} ({confluence_scores_rate:.2f})")
            logger.info(f"      Should trade not null: {analyses_results['should_trade_not_null']} ({should_trade_rate:.2f})")
            logger.info(f"      Default fallback values (50/100): {analyses_results['default_fallback_values']} ({fallback_rate:.2f})")
            logger.info(f"      Diverse confluence scores: {analyses_results['diverse_confluence_scores']}")
            
            # Show sample analyses data
            if analyses_results['analyses_data']:
                logger.info(f"      üìä Sample Analyses Data:")
                for analysis in analyses_results['analyses_data']:
                    logger.info(f"         - {analysis['symbol']}: grade={analysis['confluence_grade']}, score={analysis['confluence_score']}, trade={analysis['should_trade']}")
            
            # Show error details if any
            if analyses_results['error_details']:
                logger.info(f"      üìä Error Details:")
                for error in analyses_results['error_details']:
                    logger.info(f"         - {error}")
            
            # Calculate test success based on review requirements
            success_criteria = [
                analyses_results['api_call_successful'],  # API call successful
                analyses_results['analyses_returned'] > 0,  # Returns data
                analyses_results['confluence_fields_present'] > 0,  # Has confluence fields
                analyses_results['confluence_grades_not_null'] > 0,  # Some grades not null
                analyses_results['confluence_scores_not_null'] > 0,  # Some scores not null
                analyses_results['default_fallback_values'] < analyses_results['confluence_scores_not_null'] * 0.5  # Less than 50% fallbacks
            ]
            success_count = sum(success_criteria)
            test_success_rate = success_count / len(success_criteria)
            
            if test_success_rate >= 0.83:  # 83% success threshold (5/6 criteria)
                self.log_test_result("API Analyses Confluence", True, 
                                   f"Analyses API confluence successful: {success_count}/{len(success_criteria)} criteria met. Grades rate: {confluence_grades_rate:.2f}, Scores rate: {confluence_scores_rate:.2f}, Fallback rate: {fallback_rate:.2f}")
            else:
                self.log_test_result("API Analyses Confluence", False, 
                                   f"Analyses API confluence issues: {success_count}/{len(success_criteria)} criteria met. Too many null values or fallbacks")
                
        except Exception as e:
            self.log_test_result("API Analyses Confluence", False, f"Exception: {str(e)}")

    async def test_3_confluence_calculation_logic(self):
        """Test 3: Confluence Calculation Logic - Verify calculation consistency and diversity"""
        logger.info("\nüîç TEST 3: Confluence Calculation Logic - Validation and Diversity")
        
        try:
            calculation_results = {
                'analyses_tested': 0,
                'grade_d_with_score_0': 0,
                'grade_d_with_should_trade_false': 0,
                'consistent_grade_score_mapping': 0,
                'diverse_grades_found': set(),
                'diverse_scores_found': set(),
                'should_trade_variations': set(),
                'calculation_consistency': 0,
                'sample_data': [],
                'error_details': []
            }
            
            logger.info("   üöÄ Testing confluence calculation logic and diversity...")
            logger.info("   üìä Expected: Grade D = score 0 = should_trade false, diverse grades/scores across symbols")
            
            # Test multiple symbols to check for diversity
            test_symbols = ['BTCUSDT', 'ETHUSDT', 'LINKUSDT', 'SOLUSDT', 'ADAUSDT']
            
            for symbol in test_symbols:
                logger.info(f"\n   üìû Testing confluence calculation for {symbol}...")
                calculation_results['analyses_tested'] += 1
                
                try:
                    start_time = time.time()
                    response = requests.post(
                        f"{self.api_url}/force-ia1-analysis",
                        json={"symbol": symbol},
                        timeout=120
                    )
                    response_time = time.time() - start_time
                    
                    if response.status_code == 200:
                        analysis_data = response.json()
                        ia1_analysis = analysis_data.get('ia1_analysis', {})
                        
                        if isinstance(ia1_analysis, dict):
                            confluence_grade = ia1_analysis.get('confluence_grade')
                            confluence_score = ia1_analysis.get('confluence_score')
                            should_trade = ia1_analysis.get('should_trade')
                            
                            logger.info(f"      ‚úÖ {symbol}: grade={confluence_grade}, score={confluence_score}, trade={should_trade}")
                            
                            # Collect diversity data
                            if confluence_grade:
                                calculation_results['diverse_grades_found'].add(confluence_grade)
                            if confluence_score is not None:
                                try:
                                    score_val = float(confluence_score)
                                    calculation_results['diverse_scores_found'].add(score_val)
                                except (ValueError, TypeError):
                                    pass
                            if should_trade is not None:
                                calculation_results['should_trade_variations'].add(str(should_trade))
                            
                            # Check Grade D logic
                            if confluence_grade == 'D':
                                try:
                                    score_val = float(confluence_score) if confluence_score is not None else None
                                    if score_val == 0 or score_val is None:
                                        calculation_results['grade_d_with_score_0'] += 1
                                        logger.info(f"         ‚úÖ Grade D with score 0 logic correct")
                                    else:
                                        logger.warning(f"         ‚ö†Ô∏è Grade D but score not 0: {score_val}")
                                    
                                    if should_trade in [False, 'false', 'False']:
                                        calculation_results['grade_d_with_should_trade_false'] += 1
                                        logger.info(f"         ‚úÖ Grade D with should_trade false logic correct")
                                    else:
                                        logger.warning(f"         ‚ö†Ô∏è Grade D but should_trade not false: {should_trade}")
                                except (ValueError, TypeError):
                                    logger.warning(f"         ‚ö†Ô∏è Grade D but score not numeric: {confluence_score}")
                            
                            # Check general consistency
                            if confluence_grade and confluence_score is not None and should_trade is not None:
                                calculation_results['consistent_grade_score_mapping'] += 1
                                logger.info(f"         ‚úÖ All confluence fields present and consistent")
                            
                            # Store sample data
                            calculation_results['sample_data'].append({
                                'symbol': symbol,
                                'confluence_grade': confluence_grade,
                                'confluence_score': confluence_score,
                                'should_trade': should_trade,
                                'response_time': response_time
                            })
                        
                        else:
                            logger.warning(f"      ‚ö†Ô∏è {symbol}: Invalid IA1 analysis structure")
                    
                    else:
                        logger.error(f"      ‚ùå {symbol} analysis failed: HTTP {response.status_code}")
                        calculation_results['error_details'].append(f"{symbol}: HTTP {response.status_code}")
                
                except Exception as e:
                    logger.error(f"      ‚ùå {symbol} analysis exception: {e}")
                    calculation_results['error_details'].append(f"{symbol}: {str(e)}")
                
                # Wait between analyses
                if symbol != test_symbols[-1]:
                    logger.info(f"      ‚è≥ Waiting 8 seconds before next analysis...")
                    await asyncio.sleep(8)
            
            # Final analysis and results
            diversity_grade_count = len(calculation_results['diverse_grades_found'])
            diversity_score_count = len(calculation_results['diverse_scores_found'])
            diversity_trade_count = len(calculation_results['should_trade_variations'])
            consistency_rate = calculation_results['consistent_grade_score_mapping'] / max(calculation_results['analyses_tested'], 1)
            
            logger.info(f"\n   üìä CONFLUENCE CALCULATION LOGIC RESULTS:")
            logger.info(f"      Analyses tested: {calculation_results['analyses_tested']}")
            logger.info(f"      Grade D with score 0: {calculation_results['grade_d_with_score_0']}")
            logger.info(f"      Grade D with should_trade false: {calculation_results['grade_d_with_should_trade_false']}")
            logger.info(f"      Consistent grade-score mapping: {calculation_results['consistent_grade_score_mapping']} ({consistency_rate:.2f})")
            logger.info(f"      Diverse grades found: {sorted(calculation_results['diverse_grades_found'])} ({diversity_grade_count})")
            logger.info(f"      Diverse scores found: {sorted(calculation_results['diverse_scores_found'])} ({diversity_score_count})")
            logger.info(f"      Should trade variations: {sorted(calculation_results['should_trade_variations'])} ({diversity_trade_count})")
            
            # Show sample data
            if calculation_results['sample_data']:
                logger.info(f"      üìä Sample Calculation Data:")
                for data in calculation_results['sample_data']:
                    logger.info(f"         - {data['symbol']}: grade={data['confluence_grade']}, score={data['confluence_score']}, trade={data['should_trade']}")
            
            # Show error details if any
            if calculation_results['error_details']:
                logger.info(f"      üìä Error Details:")
                for error in calculation_results['error_details']:
                    logger.info(f"         - {error}")
            
            # Calculate test success based on review requirements
            success_criteria = [
                calculation_results['analyses_tested'] >= 3,  # At least 3 analyses tested
                diversity_grade_count >= 2,  # At least 2 different grades
                diversity_score_count >= 3,  # At least 3 different scores
                diversity_trade_count >= 1,  # At least some should_trade variation
                calculation_results['consistent_grade_score_mapping'] >= 2,  # At least 2 consistent mappings
                consistency_rate >= 0.6  # At least 60% consistency
            ]
            success_count = sum(success_criteria)
            test_success_rate = success_count / len(success_criteria)
            
            if test_success_rate >= 0.83:  # 83% success threshold (5/6 criteria)
                self.log_test_result("Confluence Calculation Logic", True, 
                                   f"Calculation logic successful: {success_count}/{len(success_criteria)} criteria met. Diversity: {diversity_grade_count} grades, {diversity_score_count} scores, consistency: {consistency_rate:.2f}")
            else:
                self.log_test_result("Confluence Calculation Logic", False, 
                                   f"Calculation logic issues: {success_count}/{len(success_criteria)} criteria met. Limited diversity or consistency problems")
                
        except Exception as e:
            self.log_test_result("Confluence Calculation Logic", False, f"Exception: {str(e)}")

    async def test_4_backend_logs_confluence_validation(self):
        """Test 4: Backend Logs Confluence Validation - Check for confluence calculation logs"""
        logger.info("\nüîç TEST 4: Backend Logs Confluence Validation")
        
        try:
            logs_results = {
                'logs_captured': False,
                'total_log_lines': 0,
                'mfi_errors_found': 0,
                'stochastic_errors_found': 0,
                'nameerror_count': 0,
                'success_indicators': 0,
                'error_patterns_found': [],
                'success_patterns_found': [],
                'sample_errors': [],
                'sample_successes': []
            }
            
            logger.info("   üöÄ Analyzing backend logs for MFI/Stochastic errors...")
            logger.info("   üìä Expected: No MFI/Stochastic NameError patterns, successful IA1 completions")
            
            # Capture backend logs
            logger.info("   üìã Capturing recent backend logs...")
            
            try:
                backend_logs = await self._capture_backend_logs()
                if backend_logs:
                    logs_results['logs_captured'] = True
                    logs_results['total_log_lines'] = len(backend_logs)
                    logger.info(f"      ‚úÖ Captured {len(backend_logs)} log lines")
                    
                    # Analyze each log line for error patterns
                    for log_line in backend_logs:
                        log_lower = log_line.lower()
                        
                        # Check for specific MFI error patterns
                        for pattern in self.error_patterns:
                            if pattern.lower() in log_line:
                                if 'mfi' in pattern.lower():
                                    logs_results['mfi_errors_found'] += 1
                                    logs_results['error_patterns_found'].append(pattern)
                                    if len(logs_results['sample_errors']) < 3:
                                        logs_results['sample_errors'].append(log_line.strip())
                                    logger.error(f"      üö® MFI ERROR PATTERN FOUND: {pattern}")
                                    logger.error(f"         Log: {log_line.strip()}")
                                
                                elif 'stochastic' in pattern.lower():
                                    logs_results['stochastic_errors_found'] += 1
                                    logs_results['error_patterns_found'].append(pattern)
                                    if len(logs_results['sample_errors']) < 3:
                                        logs_results['sample_errors'].append(log_line.strip())
                                    logger.error(f"      üö® STOCHASTIC ERROR PATTERN FOUND: {pattern}")
                                    logger.error(f"         Log: {log_line.strip()}")
                        
                        # Check for general NameError patterns
                        if 'nameerror' in log_lower:
                            logs_results['nameerror_count'] += 1
                        
                        # Check for success indicators
                        success_patterns = [
                            'ia1 analysis completed successfully',
                            'ia1 ultra analysis completed',
                            'analysis successful for',
                            'technical analysis completed',
                            'force-ia1-analysis completed'
                        ]
                        
                        for pattern in success_patterns:
                            if pattern in log_lower:
                                logs_results['success_indicators'] += 1
                                if pattern not in logs_results['success_patterns_found']:
                                    logs_results['success_patterns_found'].append(pattern)
                                if len(logs_results['sample_successes']) < 3:
                                    logs_results['sample_successes'].append(log_line.strip())
                                break
                    
                    logger.info(f"      üìä Log analysis completed:")
                    logger.info(f"         - Total log lines analyzed: {logs_results['total_log_lines']}")
                    logger.info(f"         - MFI errors found: {logs_results['mfi_errors_found']}")
                    logger.info(f"         - Stochastic errors found: {logs_results['stochastic_errors_found']}")
                    logger.info(f"         - Total NameErrors: {logs_results['nameerror_count']}")
                    logger.info(f"         - Success indicators: {logs_results['success_indicators']}")
                    
                    # Show error patterns found
                    if logs_results['error_patterns_found']:
                        logger.error(f"      üö® ERROR PATTERNS DETECTED:")
                        for pattern in set(logs_results['error_patterns_found']):
                            logger.error(f"         - {pattern}")
                    else:
                        logger.info(f"      ‚úÖ No MFI/Stochastic error patterns found")
                    
                    # Show success patterns found
                    if logs_results['success_patterns_found']:
                        logger.info(f"      ‚úÖ SUCCESS PATTERNS FOUND:")
                        for pattern in logs_results['success_patterns_found']:
                            logger.info(f"         - {pattern}")
                    
                    # Show sample errors
                    if logs_results['sample_errors']:
                        logger.error(f"      üìã Sample Error Logs:")
                        for i, error in enumerate(logs_results['sample_errors']):
                            logger.error(f"         {i+1}. {error}")
                    
                    # Show sample successes
                    if logs_results['sample_successes']:
                        logger.info(f"      üìã Sample Success Logs:")
                        for i, success in enumerate(logs_results['sample_successes']):
                            logger.info(f"         {i+1}. {success}")
                
                else:
                    logger.warning(f"      ‚ö†Ô∏è No backend logs captured")
                    
            except Exception as e:
                logger.error(f"      ‚ùå Failed to capture backend logs: {e}")
            
            # Final analysis and results
            error_free_rate = 1.0 if (logs_results['mfi_errors_found'] == 0 and logs_results['stochastic_errors_found'] == 0) else 0.0
            success_rate = 1.0 if logs_results['success_indicators'] > 0 else 0.0
            
            logger.info(f"\n   üìä BACKEND LOGS VALIDATION RESULTS:")
            logger.info(f"      Logs captured: {logs_results['logs_captured']}")
            logger.info(f"      Total log lines: {logs_results['total_log_lines']}")
            logger.info(f"      MFI errors found: {logs_results['mfi_errors_found']}")
            logger.info(f"      Stochastic errors found: {logs_results['stochastic_errors_found']}")
            logger.info(f"      Total NameErrors: {logs_results['nameerror_count']}")
            logger.info(f"      Success indicators: {logs_results['success_indicators']}")
            logger.info(f"      Error-free rate: {error_free_rate:.2f}")
            logger.info(f"      Success indicators rate: {success_rate:.2f}")
            
            # Calculate test success based on review requirements
            success_criteria = [
                logs_results['logs_captured'],  # Logs captured successfully
                logs_results['mfi_errors_found'] == 0,  # No MFI errors
                logs_results['stochastic_errors_found'] == 0,  # No Stochastic errors
                logs_results['success_indicators'] > 0,  # Some success indicators
                logs_results['total_log_lines'] > 50  # Sufficient log data
            ]
            success_count = sum(success_criteria)
            test_success_rate = success_count / len(success_criteria)
            
            if test_success_rate >= 0.8:  # 80% success threshold (4/5 criteria)
                self.log_test_result("Backend Logs Validation", True, 
                                   f"Backend logs clean: {success_count}/{len(success_criteria)} criteria met. No MFI/Stochastic errors, {logs_results['success_indicators']} success indicators found")
            else:
                self.log_test_result("Backend Logs Validation", False, 
                                   f"Backend logs issues: {success_count}/{len(success_criteria)} criteria met. MFI errors: {logs_results['mfi_errors_found']}, Stochastic errors: {logs_results['stochastic_errors_found']}")
                
        except Exception as e:
            self.log_test_result("Backend Logs Validation", False, f"Exception: {str(e)}")

    async def test_removed_old_method_1(self):
        """Test 0: detected_pattern_names Crash Fix Validation - Critical Bug Fix Test"""
        logger.info("\nüîç TEST 0: detected_pattern_names Crash Fix Validation")
        
        try:
            crash_fix_results = {
                'analyses_attempted': 0,
                'analyses_successful': 0,
                'crash_errors_detected': 0,
                'variable_initialization_errors': 0,
                'backend_stability': True,
                'error_logs': [],
                'successful_analyses': [],
                'system_exceptions': 0
            }
            
            logger.info("   üöÄ Testing critical detected_pattern_names variable initialization fix...")
            logger.info("   üìä Expected: 100% analyses complete without 'cannot access local variable detected_pattern_names' errors")
            
            # Get available symbols from scout system
            logger.info("   üìû Getting available symbols from scout system...")
            
            try:
                response = requests.get(f"{self.api_url}/opportunities", timeout=60)
                
                if response.status_code == 200:
                    opportunities = response.json()
                    if isinstance(opportunities, dict) and 'opportunities' in opportunities:
                        opportunities = opportunities['opportunities']
                    
                    # Get first 3 available symbols for testing
                    available_symbols = [opp.get('symbol') for opp in opportunities[:10] if opp.get('symbol')]
                    
                    # Prefer BTCUSDT, ETHUSDT, SOLUSDT if available, otherwise use first 3
                    preferred_symbols = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT']
                    test_symbols = []
                    
                    for symbol in preferred_symbols:
                        if symbol in available_symbols:
                            test_symbols.append(symbol)
                    
                    # Fill remaining slots with available symbols
                    for symbol in available_symbols:
                        if symbol not in test_symbols and len(test_symbols) < 3:
                            test_symbols.append(symbol)
                    
                    self.actual_test_symbols = test_symbols[:3]  # Limit to 3 symbols
                    logger.info(f"      ‚úÖ Test symbols selected: {self.actual_test_symbols}")
                    
                else:
                    logger.warning(f"      ‚ö†Ô∏è Could not get opportunities, using default symbols")
                    self.actual_test_symbols = self.test_symbols
                    
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Error getting opportunities: {e}, using default symbols")
                self.actual_test_symbols = self.test_symbols
            
            # Test each symbol for detected_pattern_names crash
            for symbol in self.actual_test_symbols:
                logger.info(f"\n   üìû Testing IA1 analysis for {symbol} - checking for detected_pattern_names crash...")
                crash_fix_results['analyses_attempted'] += 1
                
                try:
                    start_time = time.time()
                    response = requests.post(
                        f"{self.api_url}/force-ia1-analysis",
                        json={"symbol": symbol},
                        timeout=120
                    )
                    response_time = time.time() - start_time
                    
                    if response.status_code == 200:
                        analysis_data = response.json()
                        crash_fix_results['analyses_successful'] += 1
                        crash_fix_results['successful_analyses'].append({
                            'symbol': symbol,
                            'response_time': response_time,
                            'analysis_data': analysis_data
                        })
                        
                        logger.info(f"      ‚úÖ {symbol} analysis successful (response time: {response_time:.2f}s)")
                        
                        # Check for fallback analysis indicators (sign of crash recovery)
                        reasoning = ""
                        if 'ia1_analysis' in analysis_data and isinstance(analysis_data['ia1_analysis'], dict):
                            reasoning = analysis_data['ia1_analysis'].get('reasoning', '')
                        elif 'reasoning' in analysis_data:
                            reasoning = analysis_data.get('reasoning', '')
                        
                        if reasoning:
                            # Check for fallback indicators
                            fallback_indicators = [
                                'fallback analysis', 'json parsing failed', 'pattern-only analysis',
                                'technical analysis fallback', 'using detected pattern'
                            ]
                            
                            is_fallback = any(indicator in reasoning.lower() for indicator in fallback_indicators)
                            
                            if is_fallback:
                                logger.warning(f"         ‚ö†Ô∏è Fallback analysis detected - may indicate underlying issues")
                            else:
                                logger.info(f"         ‚úÖ Full analysis completed - no fallback detected")
                        
                    elif response.status_code == 502:
                        logger.error(f"      ‚ùå {symbol} analysis failed: HTTP 502 (Backend Error)")
                        crash_fix_results['system_exceptions'] += 1
                        crash_fix_results['backend_stability'] = False
                        
                    else:
                        logger.error(f"      ‚ùå {symbol} analysis failed: HTTP {response.status_code}")
                        if response.text:
                            error_text = response.text[:300]
                            logger.error(f"         Error response: {error_text}")
                            crash_fix_results['error_logs'].append(f"{symbol}: {error_text}")
                
                except Exception as e:
                    logger.error(f"      ‚ùå {symbol} analysis exception: {e}")
                    crash_fix_results['system_exceptions'] += 1
                    crash_fix_results['error_logs'].append(f"{symbol}: {str(e)}")
                
                # Wait between analyses
                if symbol != self.actual_test_symbols[-1]:
                    logger.info(f"      ‚è≥ Waiting 10 seconds before next analysis...")
                    await asyncio.sleep(10)
            
            # Capture backend logs to check for detected_pattern_names errors
            logger.info("   üìã Capturing backend logs to check for detected_pattern_names errors...")
            
            try:
                backend_logs = await self._capture_backend_logs()
                if backend_logs:
                    # Look for specific detected_pattern_names errors
                    pattern_name_errors = []
                    variable_errors = []
                    
                    for log_line in backend_logs:
                        if 'detected_pattern_names' in log_line.lower() and 'error' in log_line.lower():
                            pattern_name_errors.append(log_line.strip())
                        
                        if any(error_pattern in log_line.lower() for error_pattern in [
                            'cannot access local variable', 'variable referenced before assignment',
                            'unbound local error', 'name error'
                        ]):
                            variable_errors.append(log_line.strip())
                    
                    crash_fix_results['crash_errors_detected'] = len(pattern_name_errors)
                    crash_fix_results['variable_initialization_errors'] = len(variable_errors)
                    
                    if pattern_name_errors:
                        logger.error(f"      ‚ùå detected_pattern_names errors found: {len(pattern_name_errors)}")
                        for error in pattern_name_errors[:3]:  # Show first 3
                            logger.error(f"         - {error}")
                    else:
                        logger.info(f"      ‚úÖ No detected_pattern_names errors found in backend logs")
                    
                    if variable_errors:
                        logger.warning(f"      ‚ö†Ô∏è Variable initialization errors found: {len(variable_errors)}")
                        for error in variable_errors[:2]:  # Show first 2
                            logger.warning(f"         - {error}")
                    else:
                        logger.info(f"      ‚úÖ No variable initialization errors found")
                        
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Could not analyze backend logs: {e}")
            
            # Final analysis
            success_rate = crash_fix_results['analyses_successful'] / max(crash_fix_results['analyses_attempted'], 1)
            
            logger.info(f"\n   üìä DETECTED_PATTERN_NAMES CRASH FIX VALIDATION RESULTS:")
            logger.info(f"      Analyses attempted: {crash_fix_results['analyses_attempted']}")
            logger.info(f"      Analyses successful: {crash_fix_results['analyses_successful']}")
            logger.info(f"      Success rate: {success_rate:.2f}")
            logger.info(f"      detected_pattern_names errors: {crash_fix_results['crash_errors_detected']}")
            logger.info(f"      Variable initialization errors: {crash_fix_results['variable_initialization_errors']}")
            logger.info(f"      Backend stability: {crash_fix_results['backend_stability']}")
            logger.info(f"      System exceptions: {crash_fix_results['system_exceptions']}")
            
            # Calculate test success based on review requirements (100% analyses without crash)
            success_criteria = [
                crash_fix_results['analyses_successful'] >= 2,  # At least 2 successful analyses
                crash_fix_results['crash_errors_detected'] == 0,  # No detected_pattern_names errors
                crash_fix_results['variable_initialization_errors'] == 0,  # No variable errors
                success_rate >= 0.67,  # At least 67% success rate (2/3)
                crash_fix_results['backend_stability']  # Backend stable
            ]
            success_count = sum(success_criteria)
            test_success_rate = success_count / len(success_criteria)
            
            if test_success_rate >= 0.8:  # 80% success threshold
                self.log_test_result("detected_pattern_names Crash Fix Validation", True, 
                                   f"Critical fix successful: {success_count}/{len(success_criteria)} criteria met. Success rate: {success_rate:.2f}, No crashes: {crash_fix_results['crash_errors_detected'] == 0}, Backend stable: {crash_fix_results['backend_stability']}")
            else:
                self.log_test_result("detected_pattern_names Crash Fix Validation", False, 
                                   f"Critical fix issues: {success_count}/{len(success_criteria)} criteria met. May still have detected_pattern_names crashes or system instability")
                
        except Exception as e:
            self.log_test_result("detected_pattern_names Crash Fix Validation", False, f"Exception: {str(e)}")

    async def test_ia2_decisions_database_analysis(self):
        """Test IA2: Database Analysis - Examine IA2 decisions in trading_decisions collection"""
        logger.info("\nüîç TEST IA2: Database Analysis - IA2 Decisions Structure")
        
        try:
            ia2_db_results = {
                'total_ia2_decisions': 0,
                'ia2_reasoning_populated': 0,
                'strategic_reasoning_populated': 0,
                'calculated_rr_present': 0,
                'ia2_calculated_rr_present': 0,
                'technical_levels_complete': 0,
                'recent_ia2_decisions': [],
                'reasoning_field_analysis': {},
                'rr_calculation_analysis': {},
                'database_connection_success': False
            }
            
            logger.info("   üöÄ Analyzing IA2 decisions in trading_decisions collection...")
            logger.info("   üìä Expected: IA2 decisions with populated reasoning and calculated RR fields")
            
            try:
                # Connect to MongoDB to analyze IA2 decisions
                from pymongo import MongoClient
                mongo_url = "mongodb://localhost:27017"
                client = MongoClient(mongo_url)
                db = client["myapp"]
                ia2_db_results['database_connection_success'] = True
                
                logger.info("      ‚úÖ Database connection successful")
                
                # Get recent IA2 decisions (last 50)
                ia2_decisions = list(db.trading_decisions.find({
                    "$or": [
                        {"ia2_reasoning": {"$exists": True}},
                        {"strategic_reasoning": {"$exists": True}},
                        {"ia2_calculated_rr": {"$exists": True}},
                        {"signal": {"$in": ["LONG", "SHORT"]}}  # IA2 typically uses LONG/SHORT
                    ]
                }).sort("timestamp", -1).limit(50))
                
                ia2_db_results['total_ia2_decisions'] = len(ia2_decisions)
                logger.info(f"      üìä Found {len(ia2_decisions)} potential IA2 decisions")
                
                if ia2_decisions:
                    # Analyze each IA2 decision
                    for i, decision in enumerate(ia2_decisions[:10]):  # Analyze first 10 in detail
                        symbol = decision.get('symbol', 'UNKNOWN')
                        timestamp = decision.get('timestamp', 'UNKNOWN')
                        
                        # Check reasoning fields
                        ia2_reasoning = decision.get('ia2_reasoning', '')
                        strategic_reasoning = decision.get('strategic_reasoning', '')
                        reasoning = decision.get('reasoning', '')
                        
                        # Check RR calculation fields
                        calculated_rr = decision.get('calculated_rr')
                        ia2_calculated_rr = decision.get('ia2_calculated_rr')
                        risk_reward_ratio = decision.get('risk_reward_ratio')
                        
                        # Check technical levels
                        ia2_entry_price = decision.get('ia2_entry_price')
                        ia2_stop_loss = decision.get('ia2_stop_loss')
                        ia2_take_profit_1 = decision.get('ia2_take_profit_1')
                        
                        # Count populated fields
                        if ia2_reasoning and len(str(ia2_reasoning).strip()) > 10:
                            ia2_db_results['ia2_reasoning_populated'] += 1
                        
                        if strategic_reasoning and len(str(strategic_reasoning).strip()) > 10:
                            ia2_db_results['strategic_reasoning_populated'] += 1
                        
                        if calculated_rr and calculated_rr not in [None, 0, 1.0]:
                            ia2_db_results['calculated_rr_present'] += 1
                        
                        if ia2_calculated_rr and ia2_calculated_rr not in [None, 0, 1.0]:
                            ia2_db_results['ia2_calculated_rr_present'] += 1
                        
                        if all([ia2_entry_price, ia2_stop_loss, ia2_take_profit_1]):
                            ia2_db_results['technical_levels_complete'] += 1
                        
                        # Store detailed analysis for first 5 decisions
                        if i < 5:
                            ia2_db_results['recent_ia2_decisions'].append({
                                'symbol': symbol,
                                'timestamp': str(timestamp),
                                'ia2_reasoning_length': len(str(ia2_reasoning)) if ia2_reasoning else 0,
                                'strategic_reasoning_length': len(str(strategic_reasoning)) if strategic_reasoning else 0,
                                'reasoning_length': len(str(reasoning)) if reasoning else 0,
                                'calculated_rr': calculated_rr,
                                'ia2_calculated_rr': ia2_calculated_rr,
                                'risk_reward_ratio': risk_reward_ratio,
                                'technical_levels': {
                                    'entry': ia2_entry_price,
                                    'sl': ia2_stop_loss,
                                    'tp1': ia2_take_profit_1
                                }
                            })
                            
                            logger.info(f"         üìã {symbol}: ia2_reasoning={len(str(ia2_reasoning)) if ia2_reasoning else 0}chars, strategic_reasoning={len(str(strategic_reasoning)) if strategic_reasoning else 0}chars, calculated_rr={calculated_rr}")
                    
                    # Analyze reasoning field patterns
                    reasoning_analysis = {
                        'ia2_reasoning_avg_length': 0,
                        'strategic_reasoning_avg_length': 0,
                        'reasoning_avg_length': 0,
                        'empty_ia2_reasoning_count': 0,
                        'empty_strategic_reasoning_count': 0,
                        'empty_reasoning_count': 0
                    }
                    
                    total_ia2_reasoning_length = 0
                    total_strategic_reasoning_length = 0
                    total_reasoning_length = 0
                    
                    for decision in ia2_decisions[:20]:  # Analyze first 20
                        ia2_reasoning = decision.get('ia2_reasoning', '')
                        strategic_reasoning = decision.get('strategic_reasoning', '')
                        reasoning = decision.get('reasoning', '')
                        
                        ia2_len = len(str(ia2_reasoning)) if ia2_reasoning else 0
                        strategic_len = len(str(strategic_reasoning)) if strategic_reasoning else 0
                        reasoning_len = len(str(reasoning)) if reasoning else 0
                        
                        total_ia2_reasoning_length += ia2_len
                        total_strategic_reasoning_length += strategic_len
                        total_reasoning_length += reasoning_len
                        
                        if ia2_len <= 10:
                            reasoning_analysis['empty_ia2_reasoning_count'] += 1
                        if strategic_len <= 10:
                            reasoning_analysis['empty_strategic_reasoning_count'] += 1
                        if reasoning_len <= 10:
                            reasoning_analysis['empty_reasoning_count'] += 1
                    
                    reasoning_analysis['ia2_reasoning_avg_length'] = total_ia2_reasoning_length / min(len(ia2_decisions), 20)
                    reasoning_analysis['strategic_reasoning_avg_length'] = total_strategic_reasoning_length / min(len(ia2_decisions), 20)
                    reasoning_analysis['reasoning_avg_length'] = total_reasoning_length / min(len(ia2_decisions), 20)
                    
                    ia2_db_results['reasoning_field_analysis'] = reasoning_analysis
                    
                    # Analyze RR calculation patterns
                    rr_analysis = {
                        'calculated_rr_values': [],
                        'ia2_calculated_rr_values': [],
                        'risk_reward_ratio_values': [],
                        'null_calculated_rr_count': 0,
                        'fixed_rr_values_count': 0,
                        'dynamic_rr_values_count': 0
                    }
                    
                    for decision in ia2_decisions[:20]:  # Analyze first 20
                        calculated_rr = decision.get('calculated_rr')
                        ia2_calculated_rr = decision.get('ia2_calculated_rr')
                        risk_reward_ratio = decision.get('risk_reward_ratio')
                        
                        if calculated_rr is not None:
                            rr_analysis['calculated_rr_values'].append(calculated_rr)
                            if calculated_rr in [1.0, 2.0, 2.2]:
                                rr_analysis['fixed_rr_values_count'] += 1
                            elif calculated_rr > 0:
                                rr_analysis['dynamic_rr_values_count'] += 1
                        else:
                            rr_analysis['null_calculated_rr_count'] += 1
                        
                        if ia2_calculated_rr is not None:
                            rr_analysis['ia2_calculated_rr_values'].append(ia2_calculated_rr)
                        
                        if risk_reward_ratio is not None:
                            rr_analysis['risk_reward_ratio_values'].append(risk_reward_ratio)
                    
                    ia2_db_results['rr_calculation_analysis'] = rr_analysis
                    
                    logger.info(f"      üìä Reasoning Analysis:")
                    logger.info(f"         - IA2 reasoning populated: {ia2_db_results['ia2_reasoning_populated']}/{len(ia2_decisions)}")
                    logger.info(f"         - Strategic reasoning populated: {ia2_db_results['strategic_reasoning_populated']}/{len(ia2_decisions)}")
                    logger.info(f"         - Average ia2_reasoning length: {reasoning_analysis['ia2_reasoning_avg_length']:.1f} chars")
                    logger.info(f"         - Average strategic_reasoning length: {reasoning_analysis['strategic_reasoning_avg_length']:.1f} chars")
                    
                    logger.info(f"      üìä RR Calculation Analysis:")
                    logger.info(f"         - Calculated RR present: {ia2_db_results['calculated_rr_present']}/{len(ia2_decisions)}")
                    logger.info(f"         - IA2 calculated RR present: {ia2_db_results['ia2_calculated_rr_present']}/{len(ia2_decisions)}")
                    logger.info(f"         - Null calculated RR: {rr_analysis['null_calculated_rr_count']}")
                    logger.info(f"         - Fixed RR values: {rr_analysis['fixed_rr_values_count']}")
                    logger.info(f"         - Dynamic RR values: {rr_analysis['dynamic_rr_values_count']}")
                
                else:
                    logger.warning("      ‚ö†Ô∏è No IA2 decisions found in database")
                
                client.close()
                
            except Exception as e:
                logger.error(f"      ‚ùå Database analysis failed: {e}")
            
            # Final analysis
            logger.info(f"\n   üìä IA2 DATABASE ANALYSIS RESULTS:")
            logger.info(f"      Total IA2 decisions: {ia2_db_results['total_ia2_decisions']}")
            logger.info(f"      IA2 reasoning populated: {ia2_db_results['ia2_reasoning_populated']}")
            logger.info(f"      Strategic reasoning populated: {ia2_db_results['strategic_reasoning_populated']}")
            logger.info(f"      Calculated RR present: {ia2_db_results['calculated_rr_present']}")
            logger.info(f"      Technical levels complete: {ia2_db_results['technical_levels_complete']}")
            
            # Calculate success based on IA2 reasoning and RR calculation presence
            success_criteria = [
                ia2_db_results['database_connection_success'],
                ia2_db_results['total_ia2_decisions'] > 0,
                ia2_db_results['ia2_reasoning_populated'] > 0 or ia2_db_results['strategic_reasoning_populated'] > 0,
                ia2_db_results['calculated_rr_present'] > 0 or ia2_db_results['ia2_calculated_rr_present'] > 0
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("IA2 Database Analysis", True, 
                                   f"IA2 database analysis successful: {success_count}/{len(success_criteria)} criteria met. IA2 decisions: {ia2_db_results['total_ia2_decisions']}, Reasoning populated: {ia2_db_results['ia2_reasoning_populated'] + ia2_db_results['strategic_reasoning_populated']}")
            else:
                self.log_test_result("IA2 Database Analysis", False, 
                                   f"IA2 database analysis issues: {success_count}/{len(success_criteria)} criteria met. May have missing IA2 decisions or empty reasoning fields")
                
        except Exception as e:
            self.log_test_result("IA2 Database Analysis", False, f"Exception: {str(e)}")

    async def test_ia2_api_decisions_structure(self):
        """Test IA2: API Decisions Structure - Test /api/decisions endpoint for IA2 data exposure"""
        logger.info("\nüîç TEST IA2: API Decisions Structure")
        
        try:
            api_results = {
                'decisions_api_accessible': False,
                'total_decisions_returned': 0,
                'ia2_decisions_found': 0,
                'ia2_reasoning_exposed': 0,
                'calculated_rr_exposed': 0,
                'technical_levels_exposed': 0,
                'api_response_structure': {},
                'sample_ia2_decisions': []
            }
            
            logger.info("   üöÄ Testing /api/decisions endpoint for IA2 data structure...")
            logger.info("   üìä Expected: API exposes IA2 reasoning and calculated RR fields")
            
            try:
                # Test /api/decisions endpoint
                response = requests.get(f"{self.api_url}/decisions", timeout=60)
                
                if response.status_code == 200:
                    decisions_data = response.json()
                    api_results['decisions_api_accessible'] = True
                    
                    # Handle different response structures
                    if isinstance(decisions_data, dict) and 'decisions' in decisions_data:
                        decisions = decisions_data['decisions']
                    elif isinstance(decisions_data, list):
                        decisions = decisions_data
                    else:
                        decisions = []
                    
                    api_results['total_decisions_returned'] = len(decisions)
                    logger.info(f"      ‚úÖ Decisions API accessible: {len(decisions)} decisions returned")
                    
                    if decisions:
                        # Analyze API response structure
                        sample_decision = decisions[0]
                        api_results['api_response_structure'] = {
                            'fields_present': list(sample_decision.keys()) if isinstance(sample_decision, dict) else [],
                            'has_ia2_reasoning': 'ia2_reasoning' in sample_decision if isinstance(sample_decision, dict) else False,
                            'has_strategic_reasoning': 'strategic_reasoning' in sample_decision if isinstance(sample_decision, dict) else False,
                            'has_calculated_rr': 'calculated_rr' in sample_decision if isinstance(sample_decision, dict) else False,
                            'has_ia2_calculated_rr': 'ia2_calculated_rr' in sample_decision if isinstance(sample_decision, dict) else False
                        }
                        
                        logger.info(f"      üìä API Response Structure:")
                        logger.info(f"         - Fields present: {len(api_results['api_response_structure']['fields_present'])}")
                        logger.info(f"         - Has ia2_reasoning: {api_results['api_response_structure']['has_ia2_reasoning']}")
                        logger.info(f"         - Has strategic_reasoning: {api_results['api_response_structure']['has_strategic_reasoning']}")
                        logger.info(f"         - Has calculated_rr: {api_results['api_response_structure']['has_calculated_rr']}")
                        
                        # Analyze IA2 decisions in API response
                        for i, decision in enumerate(decisions[:20]):  # Check first 20
                            if not isinstance(decision, dict):
                                continue
                            
                            # Identify potential IA2 decisions
                            is_ia2_decision = False
                            if any(field in decision for field in ['ia2_reasoning', 'strategic_reasoning', 'ia2_calculated_rr']):
                                is_ia2_decision = True
                            elif decision.get('signal') in ['LONG', 'SHORT']:  # IA2 typically uses LONG/SHORT
                                is_ia2_decision = True
                            
                            if is_ia2_decision:
                                api_results['ia2_decisions_found'] += 1
                                
                                # Check reasoning exposure
                                ia2_reasoning = decision.get('ia2_reasoning', '')
                                strategic_reasoning = decision.get('strategic_reasoning', '')
                                reasoning = decision.get('reasoning', '')
                                
                                if (ia2_reasoning and len(str(ia2_reasoning).strip()) > 10) or \
                                   (strategic_reasoning and len(str(strategic_reasoning).strip()) > 10):
                                    api_results['ia2_reasoning_exposed'] += 1
                                
                                # Check RR calculation exposure
                                calculated_rr = decision.get('calculated_rr')
                                ia2_calculated_rr = decision.get('ia2_calculated_rr')
                                
                                if (calculated_rr and calculated_rr not in [None, 0, 1.0]) or \
                                   (ia2_calculated_rr and ia2_calculated_rr not in [None, 0, 1.0]):
                                    api_results['calculated_rr_exposed'] += 1
                                
                                # Check technical levels exposure
                                ia2_entry = decision.get('ia2_entry_price')
                                ia2_sl = decision.get('ia2_stop_loss')
                                ia2_tp1 = decision.get('ia2_take_profit_1')
                                
                                if all([ia2_entry, ia2_sl, ia2_tp1]):
                                    api_results['technical_levels_exposed'] += 1
                                
                                # Store sample for first 3 IA2 decisions
                                if len(api_results['sample_ia2_decisions']) < 3:
                                    api_results['sample_ia2_decisions'].append({
                                        'symbol': decision.get('symbol', 'UNKNOWN'),
                                        'signal': decision.get('signal', 'UNKNOWN'),
                                        'ia2_reasoning_length': len(str(ia2_reasoning)) if ia2_reasoning else 0,
                                        'strategic_reasoning_length': len(str(strategic_reasoning)) if strategic_reasoning else 0,
                                        'calculated_rr': calculated_rr,
                                        'ia2_calculated_rr': ia2_calculated_rr,
                                        'technical_levels_present': all([ia2_entry, ia2_sl, ia2_tp1])
                                    })
                        
                        logger.info(f"      üìä IA2 Decisions Analysis:")
                        logger.info(f"         - IA2 decisions found: {api_results['ia2_decisions_found']}")
                        logger.info(f"         - IA2 reasoning exposed: {api_results['ia2_reasoning_exposed']}")
                        logger.info(f"         - Calculated RR exposed: {api_results['calculated_rr_exposed']}")
                        logger.info(f"         - Technical levels exposed: {api_results['technical_levels_exposed']}")
                        
                        # Show sample IA2 decisions
                        for i, sample in enumerate(api_results['sample_ia2_decisions']):
                            logger.info(f"         üìã Sample IA2 #{i+1}: {sample['symbol']} {sample['signal']}, reasoning={sample['ia2_reasoning_length']}+{sample['strategic_reasoning_length']}chars, rr={sample['calculated_rr']}")
                    
                    else:
                        logger.warning("      ‚ö†Ô∏è No decisions returned from API")
                
                else:
                    logger.error(f"      ‚ùå Decisions API failed: HTTP {response.status_code}")
                    if response.text:
                        logger.error(f"         Error response: {response.text[:200]}...")
            
            except Exception as e:
                logger.error(f"      ‚ùå Decisions API exception: {e}")
            
            # Final analysis
            logger.info(f"\n   üìä IA2 API DECISIONS STRUCTURE RESULTS:")
            logger.info(f"      Decisions API accessible: {api_results['decisions_api_accessible']}")
            logger.info(f"      Total decisions returned: {api_results['total_decisions_returned']}")
            logger.info(f"      IA2 decisions found: {api_results['ia2_decisions_found']}")
            logger.info(f"      IA2 reasoning exposed: {api_results['ia2_reasoning_exposed']}")
            logger.info(f"      Calculated RR exposed: {api_results['calculated_rr_exposed']}")
            
            # Calculate success based on API exposure of IA2 data
            success_criteria = [
                api_results['decisions_api_accessible'],
                api_results['total_decisions_returned'] > 0,
                api_results['ia2_decisions_found'] > 0,
                api_results['ia2_reasoning_exposed'] > 0 or api_results['calculated_rr_exposed'] > 0
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("IA2 API Decisions Structure", True, 
                                   f"IA2 API structure analysis successful: {success_count}/{len(success_criteria)} criteria met. IA2 decisions: {api_results['ia2_decisions_found']}, Reasoning exposed: {api_results['ia2_reasoning_exposed']}")
            else:
                self.log_test_result("IA2 API Decisions Structure", False, 
                                   f"IA2 API structure issues: {success_count}/{len(success_criteria)} criteria met. May have missing IA2 data exposure or API problems")
                
        except Exception as e:
            self.log_test_result("IA2 API Decisions Structure", False, f"Exception: {str(e)}")

    async def test_ia2_rr_calculation_formulas(self):
        """Test IA2: RR Calculation Formulas - Verify IA2 uses correct RR formulas"""
        logger.info("\nüîç TEST IA2: RR Calculation Formulas Verification")
        
        try:
            rr_formula_results = {
                'ia2_decisions_analyzed': 0,
                'correct_rr_formulas': 0,
                'incorrect_rr_formulas': 0,
                'missing_technical_levels': 0,
                'formula_verification_details': [],
                'rr_calculation_patterns': {},
                'database_connection_success': False
            }
            
            logger.info("   üöÄ Verifying IA2 RR calculation formulas...")
            logger.info("   üìä Expected: LONG: (TP1-Entry)/(Entry-SL), SHORT: (Entry-TP1)/(SL-Entry)")
            
            try:
                # Connect to MongoDB to analyze IA2 RR calculations
                from pymongo import MongoClient
                mongo_url = "mongodb://localhost:27017"
                client = MongoClient(mongo_url)
                db = client["myapp"]
                rr_formula_results['database_connection_success'] = True
                
                # Get recent IA2 decisions with technical levels
                ia2_decisions = list(db.trading_decisions.find({
                    "$and": [
                        {"$or": [
                            {"ia2_entry_price": {"$exists": True}},
                            {"entry_price": {"$exists": True}}
                        ]},
                        {"$or": [
                            {"ia2_stop_loss": {"$exists": True}},
                            {"stop_loss": {"$exists": True}}
                        ]},
                        {"$or": [
                            {"ia2_take_profit_1": {"$exists": True}},
                            {"take_profit_1": {"$exists": True}}
                        ]},
                        {"signal": {"$in": ["LONG", "SHORT"]}}
                    ]
                }).sort("timestamp", -1).limit(20))
                
                rr_formula_results['ia2_decisions_analyzed'] = len(ia2_decisions)
                logger.info(f"      üìä Found {len(ia2_decisions)} IA2 decisions with technical levels")
                
                if ia2_decisions:
                    for decision in ia2_decisions:
                        symbol = decision.get('symbol', 'UNKNOWN')
                        signal = decision.get('signal', '').upper()
                        
                        # Get technical levels (try IA2-specific fields first, then general fields)
                        entry_price = decision.get('ia2_entry_price') or decision.get('entry_price')
                        stop_loss = decision.get('ia2_stop_loss') or decision.get('stop_loss')
                        take_profit_1 = decision.get('ia2_take_profit_1') or decision.get('take_profit_1')
                        
                        # Get RR values
                        calculated_rr = decision.get('calculated_rr')
                        ia2_calculated_rr = decision.get('ia2_calculated_rr')
                        risk_reward_ratio = decision.get('risk_reward_ratio')
                        
                        # Use the most specific RR value available
                        actual_rr = ia2_calculated_rr or calculated_rr or risk_reward_ratio
                        
                        if all([entry_price, stop_loss, take_profit_1]) and all(isinstance(x, (int, float)) for x in [entry_price, stop_loss, take_profit_1]):
                            # Calculate expected RR based on formulas
                            expected_rr = None
                            formula_used = ""
                            
                            if signal == 'LONG' and entry_price > stop_loss and take_profit_1 > entry_price:
                                expected_rr = (take_profit_1 - entry_price) / (entry_price - stop_loss)
                                formula_used = "LONG: (TP1-Entry)/(Entry-SL)"
                            elif signal == 'SHORT' and stop_loss > entry_price and take_profit_1 < entry_price:
                                expected_rr = (entry_price - take_profit_1) / (stop_loss - entry_price)
                                formula_used = "SHORT: (Entry-TP1)/(SL-Entry)"
                            
                            if expected_rr is not None and actual_rr is not None:
                                # Check if calculated RR matches expected RR (within 10% tolerance)
                                rr_difference = abs(actual_rr - expected_rr)
                                rr_tolerance = expected_rr * 0.1  # 10% tolerance
                                
                                is_correct = rr_difference <= rr_tolerance
                                
                                if is_correct:
                                    rr_formula_results['correct_rr_formulas'] += 1
                                else:
                                    rr_formula_results['incorrect_rr_formulas'] += 1
                                
                                # Store verification details
                                rr_formula_results['formula_verification_details'].append({
                                    'symbol': symbol,
                                    'signal': signal,
                                    'entry': entry_price,
                                    'sl': stop_loss,
                                    'tp1': take_profit_1,
                                    'actual_rr': actual_rr,
                                    'expected_rr': expected_rr,
                                    'formula_used': formula_used,
                                    'is_correct': is_correct,
                                    'difference': rr_difference
                                })
                                
                                logger.info(f"         üìä {symbol} {signal}: actual_rr={actual_rr:.3f}, expected_rr={expected_rr:.3f}, correct={is_correct}")
                            
                            else:
                                logger.warning(f"         ‚ö†Ô∏è {symbol}: Could not calculate expected RR - signal={signal}, levels=({entry_price}, {stop_loss}, {take_profit_1})")
                        
                        else:
                            rr_formula_results['missing_technical_levels'] += 1
                            logger.warning(f"         ‚ùå {symbol}: Missing or invalid technical levels")
                    
                    # Analyze RR calculation patterns
                    if rr_formula_results['formula_verification_details']:
                        rr_values = [detail['actual_rr'] for detail in rr_formula_results['formula_verification_details']]
                        expected_rr_values = [detail['expected_rr'] for detail in rr_formula_results['formula_verification_details']]
                        
                        rr_formula_results['rr_calculation_patterns'] = {
                            'avg_actual_rr': sum(rr_values) / len(rr_values),
                            'avg_expected_rr': sum(expected_rr_values) / len(expected_rr_values),
                            'min_actual_rr': min(rr_values),
                            'max_actual_rr': max(rr_values),
                            'rr_values_distribution': {
                                'below_1': sum(1 for rr in rr_values if rr < 1.0),
                                '1_to_2': sum(1 for rr in rr_values if 1.0 <= rr < 2.0),
                                '2_to_3': sum(1 for rr in rr_values if 2.0 <= rr < 3.0),
                                'above_3': sum(1 for rr in rr_values if rr >= 3.0)
                            }
                        }
                        
                        logger.info(f"      üìä RR Calculation Patterns:")
                        logger.info(f"         - Average actual RR: {rr_formula_results['rr_calculation_patterns']['avg_actual_rr']:.3f}")
                        logger.info(f"         - Average expected RR: {rr_formula_results['rr_calculation_patterns']['avg_expected_rr']:.3f}")
                        logger.info(f"         - RR distribution: <1={rr_formula_results['rr_calculation_patterns']['rr_values_distribution']['below_1']}, 1-2={rr_formula_results['rr_calculation_patterns']['rr_values_distribution']['1_to_2']}, 2-3={rr_formula_results['rr_calculation_patterns']['rr_values_distribution']['2_to_3']}, >3={rr_formula_results['rr_calculation_patterns']['rr_values_distribution']['above_3']}")
                
                else:
                    logger.warning("      ‚ö†Ô∏è No IA2 decisions with technical levels found")
                
                client.close()
                
            except Exception as e:
                logger.error(f"      ‚ùå RR formula verification failed: {e}")
            
            # Final analysis
            total_verified = rr_formula_results['correct_rr_formulas'] + rr_formula_results['incorrect_rr_formulas']
            formula_accuracy = rr_formula_results['correct_rr_formulas'] / max(total_verified, 1)
            
            logger.info(f"\n   üìä IA2 RR CALCULATION FORMULAS RESULTS:")
            logger.info(f"      IA2 decisions analyzed: {rr_formula_results['ia2_decisions_analyzed']}")
            logger.info(f"      Correct RR formulas: {rr_formula_results['correct_rr_formulas']}")
            logger.info(f"      Incorrect RR formulas: {rr_formula_results['incorrect_rr_formulas']}")
            logger.info(f"      Missing technical levels: {rr_formula_results['missing_technical_levels']}")
            logger.info(f"      Formula accuracy: {formula_accuracy:.2f}")
            
            # Calculate success based on formula accuracy
            success_criteria = [
                rr_formula_results['database_connection_success'],
                rr_formula_results['ia2_decisions_analyzed'] > 0,
                total_verified > 0,
                formula_accuracy >= 0.7  # At least 70% formula accuracy
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("IA2 RR Calculation Formulas", True, 
                                   f"IA2 RR formulas verification successful: {success_count}/{len(success_criteria)} criteria met. Formula accuracy: {formula_accuracy:.2f}, Correct: {rr_formula_results['correct_rr_formulas']}/{total_verified}")
            else:
                self.log_test_result("IA2 RR Calculation Formulas", False, 
                                   f"IA2 RR formulas issues: {success_count}/{len(success_criteria)} criteria met. May be using incorrect formulas or missing technical levels")
                
        except Exception as e:
            self.log_test_result("IA2 RR Calculation Formulas", False, f"Exception: {str(e)}")

    async def test_ia2_json_parsing_logs_analysis(self):
        """Test IA2: JSON Parsing Logs Analysis - Check for IA2 JSON parsing errors in logs"""
        logger.info("\nüîç TEST IA2: JSON Parsing Logs Analysis")
        
        try:
            json_parsing_results = {
                'backend_logs_captured': False,
                'ia2_raw_responses_found': 0,
                'ia2_cleaned_json_found': 0,
                'ia2_json_parsing_errors': 0,
                'ia2_fallback_mode_detected': 0,
                'json_parsing_error_details': [],
                'ia2_response_patterns': {},
                'backend_log_analysis': {}
            }
            
            logger.info("   üöÄ Analyzing backend logs for IA2 JSON parsing issues...")
            logger.info("   üìä Expected: IA2 generates valid JSON without parsing errors")
            
            try:
                # Capture backend logs
                backend_logs = await self._capture_backend_logs()
                if backend_logs:
                    json_parsing_results['backend_logs_captured'] = True
                    logger.info(f"      ‚úÖ Backend logs captured: {len(backend_logs)} lines")
                    
                    # Analyze logs for IA2-specific patterns
                    ia2_raw_responses = []
                    ia2_cleaned_json = []
                    ia2_json_errors = []
                    ia2_fallback_indicators = []
                    
                    for log_line in backend_logs:
                        log_lower = log_line.lower()
                        
                        # Look for IA2 raw strategic responses
                        if 'ia2: raw strategic response' in log_lower or 'ia2 raw response' in log_lower:
                            ia2_raw_responses.append(log_line.strip())
                        
                        # Look for cleaned IA2 JSON
                        if 'cleaned ia2 json' in log_lower or 'ia2 cleaned json' in log_lower:
                            ia2_cleaned_json.append(log_line.strip())
                        
                        # Look for IA2 JSON parsing errors
                        if any(error_pattern in log_lower for error_pattern in [
                            'ia2 json parsing failed', 'ia2 json error', 'ia2 parsing error',
                            'failed to parse ia2', 'ia2 json decode error'
                        ]):
                            ia2_json_errors.append(log_line.strip())
                        
                        # Look for IA2 fallback mode indicators
                        if any(fallback_pattern in log_lower for fallback_pattern in [
                            'ia2 fallback', 'ia2 using fallback', 'ia2 fallback mode',
                            'ia2 simplified response', 'ia2 emergency response'
                        ]):
                            ia2_fallback_indicators.append(log_line.strip())
                    
                    json_parsing_results['ia2_raw_responses_found'] = len(ia2_raw_responses)
                    json_parsing_results['ia2_cleaned_json_found'] = len(ia2_cleaned_json)
                    json_parsing_results['ia2_json_parsing_errors'] = len(ia2_json_errors)
                    json_parsing_results['ia2_fallback_mode_detected'] = len(ia2_fallback_indicators)
                    
                    logger.info(f"      üìä IA2 Log Analysis:")
                    logger.info(f"         - IA2 raw responses: {len(ia2_raw_responses)}")
                    logger.info(f"         - IA2 cleaned JSON: {len(ia2_cleaned_json)}")
                    logger.info(f"         - IA2 JSON parsing errors: {len(ia2_json_errors)}")
                    logger.info(f"         - IA2 fallback mode detected: {len(ia2_fallback_indicators)}")
                    
                    # Show sample logs
                    if ia2_raw_responses:
                        logger.info(f"      üìã Sample IA2 raw response log: {ia2_raw_responses[0][:200]}...")
                    
                    if ia2_json_errors:
                        logger.error(f"      ‚ùå Sample IA2 JSON error: {ia2_json_errors[0][:200]}...")
                        json_parsing_results['json_parsing_error_details'] = ia2_json_errors[:5]  # Store first 5 errors
                    
                    if ia2_fallback_indicators:
                        logger.warning(f"      ‚ö†Ô∏è Sample IA2 fallback indicator: {ia2_fallback_indicators[0][:200]}...")
                    
                    # Analyze IA2 response patterns
                    if ia2_raw_responses or ia2_cleaned_json:
                        json_parsing_results['ia2_response_patterns'] = {
                            'total_ia2_responses': len(ia2_raw_responses),
                            'successful_json_cleaning': len(ia2_cleaned_json),
                            'json_success_rate': len(ia2_cleaned_json) / max(len(ia2_raw_responses), 1),
                            'error_rate': len(ia2_json_errors) / max(len(ia2_raw_responses), 1),
                            'fallback_rate': len(ia2_fallback_indicators) / max(len(ia2_raw_responses), 1)
                        }
                        
                        logger.info(f"      üìä IA2 Response Patterns:")
                        logger.info(f"         - JSON success rate: {json_parsing_results['ia2_response_patterns']['json_success_rate']:.2f}")
                        logger.info(f"         - Error rate: {json_parsing_results['ia2_response_patterns']['error_rate']:.2f}")
                        logger.info(f"         - Fallback rate: {json_parsing_results['ia2_response_patterns']['fallback_rate']:.2f}")
                    
                    # General backend log health analysis
                    total_errors = sum(1 for log in backend_logs if 'error' in log.lower())
                    total_warnings = sum(1 for log in backend_logs if 'warning' in log.lower())
                    
                    json_parsing_results['backend_log_analysis'] = {
                        'total_log_lines': len(backend_logs),
                        'total_errors': total_errors,
                        'total_warnings': total_warnings,
                        'error_rate': total_errors / max(len(backend_logs), 1),
                        'backend_health': 'Good' if total_errors < 10 else 'Poor'
                    }
                    
                    logger.info(f"      üìä Backend Log Health:")
                    logger.info(f"         - Total errors: {total_errors}")
                    logger.info(f"         - Total warnings: {total_warnings}")
                    logger.info(f"         - Backend health: {json_parsing_results['backend_log_analysis']['backend_health']}")
                
                else:
                    logger.warning("      ‚ö†Ô∏è Could not capture backend logs")
            
            except Exception as e:
                logger.error(f"      ‚ùå Log analysis failed: {e}")
            
            # Final analysis
            logger.info(f"\n   üìä IA2 JSON PARSING LOGS ANALYSIS RESULTS:")
            logger.info(f"      Backend logs captured: {json_parsing_results['backend_logs_captured']}")
            logger.info(f"      IA2 raw responses found: {json_parsing_results['ia2_raw_responses_found']}")
            logger.info(f"      IA2 cleaned JSON found: {json_parsing_results['ia2_cleaned_json_found']}")
            logger.info(f"      IA2 JSON parsing errors: {json_parsing_results['ia2_json_parsing_errors']}")
            logger.info(f"      IA2 fallback mode detected: {json_parsing_results['ia2_fallback_mode_detected']}")
            
            # Calculate success based on JSON parsing health
            success_criteria = [
                json_parsing_results['backend_logs_captured'],
                json_parsing_results['ia2_json_parsing_errors'] <= 2,  # Allow up to 2 parsing errors
                json_parsing_results['ia2_fallback_mode_detected'] <= 1,  # Allow up to 1 fallback
                json_parsing_results['ia2_raw_responses_found'] > 0 or json_parsing_results['ia2_cleaned_json_found'] > 0  # Some IA2 activity
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("IA2 JSON Parsing Logs Analysis", True, 
                                   f"IA2 JSON parsing analysis successful: {success_count}/{len(success_criteria)} criteria met. Parsing errors: {json_parsing_results['ia2_json_parsing_errors']}, Fallback mode: {json_parsing_results['ia2_fallback_mode_detected']}")
            else:
                self.log_test_result("IA2 JSON Parsing Logs Analysis", False, 
                                   f"IA2 JSON parsing issues: {success_count}/{len(success_criteria)} criteria met. May have JSON parsing errors or excessive fallback mode usage")
                
        except Exception as e:
            self.log_test_result("IA2 JSON Parsing Logs Analysis", False, f"Exception: {str(e)}")

    async def test_1_scout_system_3_cryptos_test(self):
        """Test 1: Scout System 3 Cryptos Test - Test scout system with 3 cryptos"""
        logger.info("\nüîç TEST 1: Scout System 3 Cryptos Test")
        
        try:
            scout_results = {
                'opportunities_api_accessible': False,
                'opportunities_found': 0,
                'target_cryptos_present': {},
                'timestamp_persistence_working': False,
                'scout_data_quality': {},
                'technical_data_available': False,
                'opportunities_data': []
            }
            
            logger.info("   üöÄ Testing Scout System with 3 cryptos (BTCUSDT, ETHUSDT, SOLUSDT or alternatives)...")
            logger.info("   üìä Expected: Scout system returns opportunities for target cryptos with persistent timestamps")
            
            # Step 1: Test /api/opportunities endpoint
            logger.info("   üìû Testing /api/opportunities endpoint...")
            
            try:
                response = requests.get(f"{self.api_url}/opportunities", timeout=60)
                
                if response.status_code == 200:
                    response_data = response.json()
                    
                    # Handle both direct list and wrapped response
                    if isinstance(response_data, dict) and 'opportunities' in response_data:
                        opportunities = response_data['opportunities']
                    elif isinstance(response_data, list):
                        opportunities = response_data
                    else:
                        opportunities = []
                    
                    scout_results['opportunities_api_accessible'] = True
                    scout_results['opportunities_found'] = len(opportunities)
                    scout_results['opportunities_data'] = opportunities
                    
                    logger.info(f"      ‚úÖ Opportunities API accessible: {len(opportunities)} opportunities found")
                    
                    # Check for target cryptos (preferred) and get available symbols
                    target_cryptos = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT']
                    found_cryptos = {}
                    available_symbols = []
                    
                    for opportunity in opportunities:
                        symbol = opportunity.get('symbol', '')
                        available_symbols.append(symbol)
                        
                        if symbol in target_cryptos:
                            found_cryptos[symbol] = {
                                'current_price': opportunity.get('current_price'),
                                'volume_24h': opportunity.get('volume_24h'),
                                'price_change_24h': opportunity.get('price_change_24h'),
                                'volatility': opportunity.get('volatility'),
                                'timestamp': opportunity.get('timestamp'),
                                'market_cap': opportunity.get('market_cap')
                            }
                            logger.info(f"         ‚úÖ {symbol} found: Price=${opportunity.get('current_price')}, Vol=${opportunity.get('volume_24h')}")
                    
                    # If preferred symbols not found, use first 3 available symbols for testing
                    if len(found_cryptos) < 2 and len(available_symbols) >= 2:
                        logger.info(f"      üìã Preferred symbols not available, using first available symbols: {available_symbols[:3]}")
                        self.actual_test_symbols = available_symbols[:3]
                        
                        # Add first 3 available symbols to found_cryptos for testing
                        for symbol in self.actual_test_symbols:
                            for opportunity in opportunities:
                                if opportunity.get('symbol') == symbol:
                                    found_cryptos[symbol] = {
                                        'current_price': opportunity.get('current_price'),
                                        'volume_24h': opportunity.get('volume_24h'),
                                        'price_change_24h': opportunity.get('price_change_24h'),
                                        'volatility': opportunity.get('volatility'),
                                        'timestamp': opportunity.get('timestamp'),
                                        'market_cap': opportunity.get('market_cap')
                                    }
                                    break
                    else:
                        self.actual_test_symbols = list(found_cryptos.keys())
                    
                    scout_results['target_cryptos_present'] = found_cryptos
                    
                    if len(found_cryptos) >= 2:
                        logger.info(f"      ‚úÖ Target cryptos found: {list(found_cryptos.keys())}")
                    else:
                        logger.warning(f"      ‚ö†Ô∏è Limited target cryptos found: {list(found_cryptos.keys())}")
                    
                    # Test timestamp persistence by calling API twice
                    logger.info("   üïê Testing timestamp persistence...")
                    await asyncio.sleep(5)  # Wait 5 seconds
                    
                    response2 = requests.get(f"{self.api_url}/opportunities", timeout=60)
                    if response2.status_code == 200:
                        opportunities2 = response2.json()
                        
                        # Compare timestamps for same symbols
                        timestamp_matches = 0
                        timestamp_total = 0
                        
                        for opp1 in opportunities:
                            symbol1 = opp1.get('symbol')
                            timestamp1 = opp1.get('timestamp')
                            
                            for opp2 in opportunities2:
                                symbol2 = opp2.get('symbol')
                                timestamp2 = opp2.get('timestamp')
                                
                                if symbol1 == symbol2:
                                    timestamp_total += 1
                                    if timestamp1 == timestamp2:
                                        timestamp_matches += 1
                                    break
                        
                        if timestamp_total > 0:
                            persistence_rate = timestamp_matches / timestamp_total
                            scout_results['timestamp_persistence_working'] = persistence_rate >= 0.8
                            logger.info(f"      üìä Timestamp persistence: {timestamp_matches}/{timestamp_total} ({persistence_rate:.2f})")
                        
                    # Analyze data quality
                    quality_metrics = {
                        'with_price': sum(1 for opp in opportunities if opp.get('current_price')),
                        'with_volume': sum(1 for opp in opportunities if opp.get('volume_24h')),
                        'with_change': sum(1 for opp in opportunities if opp.get('price_change_24h') is not None),
                        'with_volatility': sum(1 for opp in opportunities if opp.get('volatility')),
                        'with_timestamp': sum(1 for opp in opportunities if opp.get('timestamp'))
                    }
                    
                    scout_results['scout_data_quality'] = quality_metrics
                    
                    # Check if technical data is available
                    technical_fields = ['rsi_signal', 'macd_trend', 'mfi_signal', 'vwap_signal']
                    technical_count = 0
                    for opp in opportunities[:10]:  # Check first 10
                        for field in technical_fields:
                            if field in opp and opp[field] not in [None, '', 'unknown']:
                                technical_count += 1
                                break
                    
                    scout_results['technical_data_available'] = technical_count > 0
                    
                    logger.info(f"      üìä Data quality: Price={quality_metrics['with_price']}, Volume={quality_metrics['with_volume']}, Technical={technical_count}")
                
                else:
                    logger.error(f"      ‚ùå Opportunities API failed: HTTP {response.status_code}")
                    if response.text:
                        logger.error(f"         Error response: {response.text[:200]}...")
            
            except Exception as e:
                logger.error(f"      ‚ùå Opportunities API exception: {e}")
            
            # Final analysis
            logger.info(f"\n   üìä SCOUT SYSTEM 3 CRYPTOS TEST RESULTS:")
            logger.info(f"      Opportunities API accessible: {scout_results['opportunities_api_accessible']}")
            logger.info(f"      Total opportunities found: {scout_results['opportunities_found']}")
            logger.info(f"      Target cryptos present: {len(scout_results['target_cryptos_present'])}/3")
            logger.info(f"      Timestamp persistence working: {scout_results['timestamp_persistence_working']}")
            logger.info(f"      Technical data available: {scout_results['technical_data_available']}")
            
            # Calculate test success
            success_criteria = [
                scout_results['opportunities_api_accessible'],
                scout_results['opportunities_found'] > 0,
                len(scout_results['target_cryptos_present']) >= 2,  # At least 2 target cryptos
                scout_results['timestamp_persistence_working']
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("Scout System 3 Cryptos Test", True, 
                                   f"Scout system working: {success_count}/{len(success_criteria)} criteria met. Target cryptos: {len(scout_results['target_cryptos_present'])}, Opportunities: {scout_results['opportunities_found']}")
            else:
                self.log_test_result("Scout System 3 Cryptos Test", False, 
                                   f"Scout system issues: {success_count}/{len(success_criteria)} criteria met. May have API or timestamp persistence problems")
                
        except Exception as e:
            self.log_test_result("Scout System 3 Cryptos Test", False, f"Exception: {str(e)}")

    async def test_2_technical_indicators_integration_verification(self):
        """Test 2: Technical Indicators Integration Verification - Verify IA1 mentions RSI, MACD, MFI, VWAP with values"""
        logger.info("\nüîç TEST 2: Technical Indicators Integration Verification")
        
        try:
            integration_results = {
                'analyses_with_technical_indicators': 0,
                'total_analyses': 0,
                'technical_indicators_mentioned': {},
                'specific_values_found': {},
                'reasoning_quality_score': 0.0,
                'balance_technical_vs_patterns': 0.0,
                'successful_analyses': []
            }
            
            logger.info("   üöÄ Testing technical indicators integration in IA1 reasoning...")
            logger.info("   üìä Expected: >70% analyses mention RSI, MACD, MFI, VWAP with calculated values")
            
            # Use symbols from previous test or defaults
            test_symbols = self.actual_test_symbols if hasattr(self, 'actual_test_symbols') and self.actual_test_symbols else self.test_symbols
            logger.info(f"   üìä Testing symbols: {test_symbols}")
            
            # Test each symbol for technical indicators integration
            for symbol in test_symbols:
                logger.info(f"\n   üìû Testing IA1 technical indicators integration for {symbol}...")
                integration_results['total_analyses'] += 1
                
                try:
                    response = requests.post(
                        f"{self.api_url}/force-ia1-analysis",
                        json={"symbol": symbol},
                        timeout=120
                    )
                    
                    if response.status_code == 200:
                        analysis_data = response.json()
                        
                        # Extract reasoning from IA1 response
                        reasoning = ""
                        if 'ia1_analysis' in analysis_data and isinstance(analysis_data['ia1_analysis'], dict):
                            reasoning = analysis_data['ia1_analysis'].get('reasoning', '')
                        elif 'reasoning' in analysis_data:
                            reasoning = analysis_data.get('reasoning', '')
                        
                        if reasoning:
                            logger.info(f"      ‚úÖ Reasoning field found: {len(reasoning)} chars")
                            
                            # Check for specific technical indicator mentions with values
                            technical_indicators_found = {}
                            specific_values_found = {}
                            
                            # Core technical indicators to check
                            core_indicators = ['RSI', 'MACD', 'MFI', 'VWAP']
                            
                            for indicator in core_indicators:
                                # Count mentions
                                mentions = reasoning.upper().count(indicator.upper())
                                if mentions > 0:
                                    technical_indicators_found[indicator] = mentions
                                    
                                    # Look for specific values (e.g., "RSI 32.1", "MACD -0.015")
                                    import re
                                    value_pattern = rf'{indicator}\s*[:\s]*[\d\.\-]+%?'
                                    matches = re.findall(value_pattern, reasoning, re.IGNORECASE)
                                    if matches:
                                        specific_values_found[indicator] = matches
                                        logger.info(f"         ‚úÖ {indicator} with values: {matches}")
                                    else:
                                        logger.info(f"         üìä {indicator} mentioned {mentions} times (no specific values)")
                            
                            # Check for patterns mentions for balance assessment
                            pattern_keywords = ['pattern', 'support', 'resistance', 'trend', 'channel']
                            pattern_mentions = sum(reasoning.lower().count(keyword) for keyword in pattern_keywords)
                            
                            technical_mentions = sum(technical_indicators_found.values())
                            
                            # Calculate balance
                            total_mentions = technical_mentions + pattern_mentions
                            if total_mentions > 0:
                                technical_ratio = technical_mentions / total_mentions
                                integration_results['balance_technical_vs_patterns'] += technical_ratio
                                logger.info(f"         üìä Technical/Patterns balance: {technical_ratio:.2f} ({technical_mentions}/{total_mentions})")
                            
                            # Store results
                            if technical_indicators_found:
                                integration_results['analyses_with_technical_indicators'] += 1
                                integration_results['successful_analyses'].append({
                                    'symbol': symbol,
                                    'technical_indicators': technical_indicators_found,
                                    'specific_values': specific_values_found,
                                    'reasoning_sample': reasoning[:300] + "..." if len(reasoning) > 300 else reasoning
                                })
                                
                                # Update global counters
                                for indicator, count in technical_indicators_found.items():
                                    if indicator not in integration_results['technical_indicators_mentioned']:
                                        integration_results['technical_indicators_mentioned'][indicator] = 0
                                    integration_results['technical_indicators_mentioned'][indicator] += count
                                
                                # Update specific values
                                for indicator, values in specific_values_found.items():
                                    if indicator not in integration_results['specific_values_found']:
                                        integration_results['specific_values_found'][indicator] = []
                                    integration_results['specific_values_found'][indicator].extend(values)
                                
                                logger.info(f"      ‚úÖ Technical indicators integration SUCCESS for {symbol}")
                            else:
                                logger.warning(f"      ‚ùå No technical indicators found in {symbol} reasoning")
                        else:
                            logger.warning(f"      ‚ùå No reasoning field found for {symbol}")
                    
                    else:
                        logger.error(f"      ‚ùå IA1 analysis for {symbol} failed: HTTP {response.status_code}")
                
                except Exception as e:
                    logger.error(f"      ‚ùå IA1 analysis for {symbol} exception: {e}")
                
                # Wait between analyses
                if symbol != test_symbols[-1]:
                    await asyncio.sleep(10)
            
            # Calculate final metrics
            if integration_results['total_analyses'] > 0:
                technical_integration_rate = integration_results['analyses_with_technical_indicators'] / integration_results['total_analyses']
                avg_balance = integration_results['balance_technical_vs_patterns'] / integration_results['total_analyses']
            else:
                technical_integration_rate = 0.0
                avg_balance = 0.0
            
            logger.info(f"\n   üìä TECHNICAL INDICATORS INTEGRATION VERIFICATION RESULTS:")
            logger.info(f"      Total analyses: {integration_results['total_analyses']}")
            logger.info(f"      Analyses with technical indicators: {integration_results['analyses_with_technical_indicators']}")
            logger.info(f"      Technical integration rate: {technical_integration_rate:.2f}")
            logger.info(f"      Average technical/patterns balance: {avg_balance:.2f}")
            
            if integration_results['technical_indicators_mentioned']:
                logger.info(f"      üìä Technical indicators mentioned:")
                for indicator, count in integration_results['technical_indicators_mentioned'].items():
                    logger.info(f"         - {indicator}: {count} mentions")
            
            if integration_results['specific_values_found']:
                logger.info(f"      üìä Specific values found:")
                for indicator, values in integration_results['specific_values_found'].items():
                    logger.info(f"         - {indicator}: {len(values)} specific values")
            
            # Calculate test success based on review requirements (>70% technical indicators usage)
            success_criteria = [
                integration_results['total_analyses'] >= 2,  # At least 2 analyses
                integration_results['analyses_with_technical_indicators'] >= 1,  # At least 1 with indicators
                technical_integration_rate >= 0.7,  # >70% usage rate (target from review)
                len(integration_results['specific_values_found']) >= 2,  # At least 2 indicators with values
                avg_balance >= 0.3  # At least 30% technical mentions vs patterns
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.8:  # 80% success threshold
                self.log_test_result("Technical Indicators Integration Verification", True, 
                                   f"Technical indicators integration successful: {success_count}/{len(success_criteria)} criteria met. Integration rate: {technical_integration_rate:.2f}, Balance: {avg_balance:.2f}, Specific values: {len(integration_results['specific_values_found'])}")
            else:
                self.log_test_result("Technical Indicators Integration Verification", False, 
                                   f"Technical indicators integration issues: {success_count}/{len(success_criteria)} criteria met. Integration rate may be below 70% target or insufficient specific values")
                
        except Exception as e:
            self.log_test_result("Technical Indicators Integration Verification", False, f"Exception: {str(e)}")

    async def test_3_json_schema_validation_fix(self):
        """Test 3: JSON Schema Validation Fix - Verify IA1 uses 'signal' field correctly without fallback"""
        logger.info("\nüîç TEST 3: JSON Schema Validation Fix")
        
        try:
            json_results = {
                'total_analyses': 0,
                'valid_json_responses': 0,
                'fallback_analyses': 0,
                'signal_field_present': 0,
                'complete_json_structure': 0,
                'json_parsing_errors': 0,
                'successful_responses': []
            }
            
            logger.info("   üöÄ Testing JSON schema validation fix - 'signal' field usage...")
            logger.info("   üìä Expected: 100% JSON valid responses without 'JSON parsing failed' fallback")
            
            # Use symbols from previous tests
            test_symbols = self.actual_test_symbols if hasattr(self, 'actual_test_symbols') and self.actual_test_symbols else self.test_symbols
            logger.info(f"   üìä Testing symbols: {test_symbols}")
            
            # Test each symbol for JSON schema validation
            for symbol in test_symbols:
                logger.info(f"\n   üìû Testing JSON schema validation for {symbol}...")
                json_results['total_analyses'] += 1
                
                try:
                    response = requests.post(
                        f"{self.api_url}/force-ia1-analysis",
                        json={"symbol": symbol},
                        timeout=120
                    )
                    
                    if response.status_code == 200:
                        analysis_data = response.json()
                        
                        # Check for valid JSON structure
                        is_valid_json = True
                        has_signal_field = False
                        is_fallback = False
                        has_complete_structure = False
                        
                        # Extract IA1 analysis
                        ia1_analysis = analysis_data.get('ia1_analysis', {})
                        
                        if isinstance(ia1_analysis, dict):
                            json_results['valid_json_responses'] += 1
                            logger.info(f"      ‚úÖ Valid JSON response received")
                            
                            # Check for 'signal' field
                            if 'signal' in ia1_analysis:
                                has_signal_field = True
                                json_results['signal_field_present'] += 1
                                signal_value = ia1_analysis['signal']
                                logger.info(f"         ‚úÖ 'signal' field present: {signal_value}")
                            else:
                                logger.warning(f"         ‚ùå 'signal' field missing")
                            
                            # Check for complete JSON structure
                            required_fields = ['signal', 'confidence', 'reasoning']
                            optional_fields = ['technical_indicators', 'patterns', 'entry_price']
                            
                            required_present = sum(1 for field in required_fields if field in ia1_analysis)
                            optional_present = sum(1 for field in optional_fields if field in ia1_analysis)
                            
                            if required_present == len(required_fields):
                                has_complete_structure = True
                                json_results['complete_json_structure'] += 1
                                logger.info(f"         ‚úÖ Complete JSON structure: {required_present}/{len(required_fields)} required, {optional_present}/{len(optional_fields)} optional")
                            else:
                                logger.warning(f"         ‚ö†Ô∏è Incomplete JSON structure: {required_present}/{len(required_fields)} required fields")
                            
                            # Check for fallback analysis indicators
                            reasoning = ia1_analysis.get('reasoning', '')
                            if reasoning:
                                fallback_indicators = [
                                    'fallback analysis', 'json parsing failed', 'pattern-only analysis',
                                    'technical analysis fallback', 'using detected pattern'
                                ]
                                
                                is_fallback = any(indicator in reasoning.lower() for indicator in fallback_indicators)
                                
                                if is_fallback:
                                    json_results['fallback_analyses'] += 1
                                    logger.warning(f"         ‚ùå Fallback analysis detected")
                                else:
                                    logger.info(f"         ‚úÖ No fallback analysis - full JSON processing")
                            
                            # Store successful response
                            if has_signal_field and has_complete_structure and not is_fallback:
                                json_results['successful_responses'].append({
                                    'symbol': symbol,
                                    'signal': ia1_analysis.get('signal'),
                                    'confidence': ia1_analysis.get('confidence'),
                                    'has_technical_indicators': 'technical_indicators' in ia1_analysis,
                                    'has_patterns': 'patterns' in ia1_analysis,
                                    'reasoning_length': len(reasoning)
                                })
                                logger.info(f"      ‚úÖ JSON schema validation SUCCESS for {symbol}")
                            else:
                                logger.warning(f"      ‚ùå JSON schema validation issues for {symbol}")
                        
                        else:
                            json_results['json_parsing_errors'] += 1
                            logger.error(f"      ‚ùå Invalid JSON structure for {symbol}")
                    
                    else:
                        logger.error(f"      ‚ùå IA1 analysis for {symbol} failed: HTTP {response.status_code}")
                        json_results['json_parsing_errors'] += 1
                
                except Exception as e:
                    logger.error(f"      ‚ùå IA1 analysis for {symbol} exception: {e}")
                    json_results['json_parsing_errors'] += 1
                
                # Wait between analyses
                if symbol != test_symbols[-1]:
                    await asyncio.sleep(10)
            
            # Calculate final metrics
            if json_results['total_analyses'] > 0:
                valid_json_rate = json_results['valid_json_responses'] / json_results['total_analyses']
                signal_field_rate = json_results['signal_field_present'] / json_results['total_analyses']
                fallback_rate = json_results['fallback_analyses'] / json_results['total_analyses']
                complete_structure_rate = json_results['complete_json_structure'] / json_results['total_analyses']
            else:
                valid_json_rate = signal_field_rate = fallback_rate = complete_structure_rate = 0.0
            
            logger.info(f"\n   üìä JSON SCHEMA VALIDATION FIX RESULTS:")
            logger.info(f"      Total analyses: {json_results['total_analyses']}")
            logger.info(f"      Valid JSON responses: {json_results['valid_json_responses']} ({valid_json_rate:.2f})")
            logger.info(f"      'signal' field present: {json_results['signal_field_present']} ({signal_field_rate:.2f})")
            logger.info(f"      Complete JSON structure: {json_results['complete_json_structure']} ({complete_structure_rate:.2f})")
            logger.info(f"      Fallback analyses: {json_results['fallback_analyses']} ({fallback_rate:.2f})")
            logger.info(f"      JSON parsing errors: {json_results['json_parsing_errors']}")
            
            if json_results['successful_responses']:
                logger.info(f"      üìä Successful responses:")
                for response in json_results['successful_responses']:
                    logger.info(f"         - {response['symbol']}: signal={response['signal']}, confidence={response['confidence']}")
            
            # Calculate test success based on review requirements (100% JSON valid without fallback)
            success_criteria = [
                json_results['total_analyses'] >= 2,  # At least 2 analyses
                json_results['valid_json_responses'] >= 2,  # At least 2 valid JSON
                valid_json_rate >= 0.67,  # At least 67% valid JSON (2/3)
                json_results['fallback_analyses'] == 0,  # No fallback analyses
                signal_field_rate >= 0.67  # At least 67% with signal field
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.8:  # 80% success threshold
                self.log_test_result("JSON Schema Validation Fix", True, 
                                   f"JSON schema validation successful: {success_count}/{len(success_criteria)} criteria met. Valid JSON rate: {valid_json_rate:.2f}, No fallbacks: {json_results['fallback_analyses'] == 0}, Signal field rate: {signal_field_rate:.2f}")
            else:
                self.log_test_result("JSON Schema Validation Fix", False, 
                                   f"JSON schema validation issues: {success_count}/{len(success_criteria)} criteria met. May still have JSON parsing failures or fallback analyses")
                
        except Exception as e:
            self.log_test_result("JSON Schema Validation Fix", False, f"Exception: {str(e)}")

    async def test_4_enhanced_rr_calculation_verification(self):
        """Test 4: Enhanced RR Calculation Verification - Test new mathematical RR formulas vs fixed values"""
        logger.info("\nüîç TEST 4: Enhanced RR Calculation Verification")
        
        try:
            rr_results = {
                'total_analyses': 0,
                'analyses_with_calculated_rr': 0,
                'analyses_with_rr_reasoning': 0,
                'mathematical_formulas_used': 0,
                'technical_levels_coherent': 0,
                'rr_values_found': [],
                'rr_reasoning_samples': [],
                'escalation_criteria_met': 0,
                'successful_analyses': []
            }
            
            logger.info("   üöÄ Testing enhanced RR calculation system with mathematical formulas...")
            logger.info("   üìä Expected: calculated_rr uses real formulas, rr_reasoning explains technical levels")
            
            # Use symbols from previous tests or get from opportunities
            test_symbols = self.actual_test_symbols if hasattr(self, 'actual_test_symbols') and self.actual_test_symbols else ['BTCUSDT', 'ETHUSDT']
            logger.info(f"   üìä Testing symbols: {test_symbols}")
            
            # Test each symbol for enhanced RR calculation
            for symbol in test_symbols:
                logger.info(f"\n   üìû Testing enhanced RR calculation for {symbol}...")
                rr_results['total_analyses'] += 1
                
                try:
                    response = requests.post(
                        f"{self.api_url}/force-ia1-analysis",
                        json={"symbol": symbol},
                        timeout=120
                    )
                    
                    if response.status_code == 200:
                        analysis_data = response.json()
                        
                        # Extract IA1 analysis data
                        ia1_analysis = analysis_data.get('ia1_analysis', {})
                        
                        # Check for calculated_rr field
                        calculated_rr = ia1_analysis.get('calculated_rr') or ia1_analysis.get('risk_reward_ratio')
                        if calculated_rr is not None:
                            rr_results['analyses_with_calculated_rr'] += 1
                            rr_results['rr_values_found'].append({
                                'symbol': symbol,
                                'calculated_rr': calculated_rr
                            })
                            logger.info(f"      ‚úÖ calculated_rr found: {calculated_rr}")
                            
                            # Check if RR is not a fixed value (1.0, 2.2, etc.)
                            fixed_values = [1.0, 2.0, 2.2, 3.0]
                            is_calculated = not any(abs(float(calculated_rr) - fixed) < 0.01 for fixed in fixed_values)
                            
                            if is_calculated:
                                rr_results['mathematical_formulas_used'] += 1
                                logger.info(f"         ‚úÖ RR appears calculated (not fixed): {calculated_rr}")
                            else:
                                logger.warning(f"         ‚ö†Ô∏è RR appears to be fixed value: {calculated_rr}")
                        else:
                            logger.warning(f"      ‚ùå No calculated_rr field found for {symbol}")
                        
                        # Check for rr_reasoning field
                        rr_reasoning = ia1_analysis.get('rr_reasoning', '')
                        if rr_reasoning:
                            rr_results['analyses_with_rr_reasoning'] += 1
                            rr_results['rr_reasoning_samples'].append({
                                'symbol': symbol,
                                'rr_reasoning': rr_reasoning[:200] + "..." if len(rr_reasoning) > 200 else rr_reasoning
                            })
                            logger.info(f"      ‚úÖ rr_reasoning found: {len(rr_reasoning)} chars")
                            
                            # Check for technical level mentions (VWAP, EMA21, SMA50)
                            technical_levels = ['VWAP', 'EMA21', 'EMA', 'SMA50', 'SMA', 'support', 'resistance']
                            technical_mentions = sum(1 for level in technical_levels if level.lower() in rr_reasoning.lower())
                            
                            if technical_mentions >= 2:
                                rr_results['technical_levels_coherent'] += 1
                                logger.info(f"         ‚úÖ Technical levels mentioned: {technical_mentions} references")
                            else:
                                logger.warning(f"         ‚ö†Ô∏è Limited technical level mentions: {technical_mentions}")
                        else:
                            logger.warning(f"      ‚ùå No rr_reasoning field found for {symbol}")
                        
                        # Check technical levels coherence
                        entry_price = ia1_analysis.get('entry_price')
                        stop_loss = ia1_analysis.get('stop_loss_price') or ia1_analysis.get('stop_loss')
                        take_profit = ia1_analysis.get('take_profit_price') or ia1_analysis.get('take_profit')
                        signal = ia1_analysis.get('signal', '').upper()
                        
                        if all([entry_price, stop_loss, take_profit]):
                            logger.info(f"      üìä Technical levels: Entry={entry_price}, SL={stop_loss}, TP={take_profit}, Signal={signal}")
                            
                            # Verify logical order for LONG/SHORT
                            levels_coherent = False
                            if signal == 'LONG' and stop_loss < entry_price < take_profit:
                                levels_coherent = True
                                logger.info(f"         ‚úÖ LONG levels coherent: SL < Entry < TP")
                            elif signal == 'SHORT' and take_profit < entry_price < stop_loss:
                                levels_coherent = True
                                logger.info(f"         ‚úÖ SHORT levels coherent: TP < Entry < SL")
                            else:
                                logger.warning(f"         ‚ö†Ô∏è Levels may not be coherent for {signal} signal")
                            
                            # Calculate expected RR manually to verify
                            if levels_coherent:
                                if signal == 'LONG':
                                    expected_rr = (take_profit - entry_price) / (entry_price - stop_loss)
                                else:  # SHORT
                                    expected_rr = (entry_price - take_profit) / (stop_loss - entry_price)
                                
                                if calculated_rr:
                                    rr_difference = abs(float(calculated_rr) - expected_rr)
                                    if rr_difference < 0.1:  # Within 0.1 tolerance
                                        logger.info(f"         ‚úÖ RR calculation verified: Expected={expected_rr:.2f}, Got={calculated_rr}")
                                    else:
                                        logger.warning(f"         ‚ö†Ô∏è RR calculation mismatch: Expected={expected_rr:.2f}, Got={calculated_rr}")
                        
                        # Check escalation criteria (RR > 2.0 or confidence > 70%)
                        confidence = ia1_analysis.get('confidence', 0)
                        if calculated_rr and (float(calculated_rr) > 2.0 or confidence > 0.7):
                            rr_results['escalation_criteria_met'] += 1
                            logger.info(f"      üöÄ Escalation criteria met: RR={calculated_rr}, Confidence={confidence}")
                        
                        # Store successful analysis
                        if calculated_rr and rr_reasoning:
                            rr_results['successful_analyses'].append({
                                'symbol': symbol,
                                'calculated_rr': calculated_rr,
                                'rr_reasoning_length': len(rr_reasoning),
                                'technical_levels_present': all([entry_price, stop_loss, take_profit]),
                                'escalation_eligible': calculated_rr and float(calculated_rr) > 2.0
                            })
                            logger.info(f"      ‚úÖ Enhanced RR calculation SUCCESS for {symbol}")
                    
                    else:
                        logger.error(f"      ‚ùå IA1 analysis for {symbol} failed: HTTP {response.status_code}")
                
                except Exception as e:
                    logger.error(f"      ‚ùå IA1 analysis for {symbol} exception: {e}")
                
                # Wait between analyses
                if symbol != test_symbols[-1]:
                    await asyncio.sleep(10)
            
            # Calculate final metrics
            if rr_results['total_analyses'] > 0:
                calculated_rr_rate = rr_results['analyses_with_calculated_rr'] / rr_results['total_analyses']
                rr_reasoning_rate = rr_results['analyses_with_rr_reasoning'] / rr_results['total_analyses']
                mathematical_formula_rate = rr_results['mathematical_formulas_used'] / rr_results['total_analyses']
                technical_coherence_rate = rr_results['technical_levels_coherent'] / rr_results['total_analyses']
            else:
                calculated_rr_rate = rr_reasoning_rate = mathematical_formula_rate = technical_coherence_rate = 0.0
            
            logger.info(f"\n   üìä ENHANCED RR CALCULATION VERIFICATION RESULTS:")
            logger.info(f"      Total analyses: {rr_results['total_analyses']}")
            logger.info(f"      Analyses with calculated_rr: {rr_results['analyses_with_calculated_rr']} ({calculated_rr_rate:.2f})")
            logger.info(f"      Analyses with rr_reasoning: {rr_results['analyses_with_rr_reasoning']} ({rr_reasoning_rate:.2f})")
            logger.info(f"      Mathematical formulas used: {rr_results['mathematical_formulas_used']} ({mathematical_formula_rate:.2f})")
            logger.info(f"      Technical levels coherent: {rr_results['technical_levels_coherent']} ({technical_coherence_rate:.2f})")
            logger.info(f"      Escalation criteria met: {rr_results['escalation_criteria_met']}")
            
            if rr_results['rr_values_found']:
                logger.info(f"      üìä RR values found:")
                for rr_data in rr_results['rr_values_found']:
                    logger.info(f"         - {rr_data['symbol']}: {rr_data['calculated_rr']}")
            
            if rr_results['rr_reasoning_samples']:
                logger.info(f"      üìä RR reasoning samples:")
                for reasoning_data in rr_results['rr_reasoning_samples'][:2]:  # Show first 2
                    logger.info(f"         - {reasoning_data['symbol']}: {reasoning_data['rr_reasoning']}")
            
            # Calculate test success based on review requirements
            success_criteria = [
                rr_results['total_analyses'] >= 1,  # At least 1 analysis
                rr_results['analyses_with_calculated_rr'] >= 1,  # At least 1 with calculated_rr
                calculated_rr_rate >= 0.5,  # At least 50% with calculated_rr
                rr_results['analyses_with_rr_reasoning'] >= 1,  # At least 1 with rr_reasoning
                mathematical_formula_rate >= 0.5  # At least 50% using mathematical formulas
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.8:  # 80% success threshold
                self.log_test_result("Enhanced RR Calculation Verification", True, 
                                   f"Enhanced RR calculation successful: {success_count}/{len(success_criteria)} criteria met. Calculated RR rate: {calculated_rr_rate:.2f}, Mathematical formulas: {mathematical_formula_rate:.2f}, RR reasoning: {rr_reasoning_rate:.2f}")
            else:
                self.log_test_result("Enhanced RR Calculation Verification", False, 
                                   f"Enhanced RR calculation issues: {success_count}/{len(success_criteria)} criteria met. May still be using fixed RR values or missing rr_reasoning")
                
        except Exception as e:
            self.log_test_result("Enhanced RR Calculation Verification", False, f"Exception: {str(e)}")

    async def test_5_confluence_6_indicators_reasoning_verification(self):
        """Test 5: Confluence 6 Indicators Reasoning Verification - Verify explicit mentions of RSI, MACD, MFI, VWAP, EMA, patterns"""
        logger.info("\nüîç TEST 5: Confluence 6 Indicators Reasoning Verification")
        
        try:
            confluence_results = {
                'total_analyses': 0,
                'analyses_with_confluence_reasoning': 0,
                'indicators_mentioned': {},
                'confluence_format_found': 0,
                'technical_indicators_analysis_present': 0,
                'reasoning_quality_scores': [],
                'successful_analyses': []
            }
            
            logger.info("   üöÄ Testing confluence 6 indicators reasoning with explicit mentions...")
            logger.info("   üìä Expected: 'CONFLUENCE ANALYSIS: RSI X, MACD Y, MFI Z...' format")
            
            # Core 6 indicators to verify
            core_6_indicators = ['RSI', 'MACD', 'MFI', 'VWAP', 'EMA', 'patterns']
            
            # Use symbols from previous tests
            test_symbols = self.actual_test_symbols if hasattr(self, 'actual_test_symbols') and self.actual_test_symbols else ['BTCUSDT', 'ETHUSDT']
            logger.info(f"   üìä Testing symbols: {test_symbols}")
            
            # Test each symbol for confluence reasoning
            for symbol in test_symbols:
                logger.info(f"\n   üìû Testing confluence 6 indicators reasoning for {symbol}...")
                confluence_results['total_analyses'] += 1
                
                try:
                    response = requests.post(
                        f"{self.api_url}/force-ia1-analysis",
                        json={"symbol": symbol},
                        timeout=120
                    )
                    
                    if response.status_code == 200:
                        analysis_data = response.json()
                        
                        # Extract reasoning from IA1 analysis
                        ia1_analysis = analysis_data.get('ia1_analysis', {})
                        reasoning = ia1_analysis.get('reasoning', '')
                        
                        if reasoning:
                            logger.info(f"      ‚úÖ Reasoning field found: {len(reasoning)} chars")
                            
                            # Check for explicit indicator mentions with states/values
                            indicators_found = {}
                            for indicator in core_6_indicators:
                                # Count mentions and look for state descriptions
                                mentions = reasoning.upper().count(indicator.upper())
                                if mentions > 0:
                                    indicators_found[indicator] = mentions
                                    
                                    # Look for state descriptions (overbought, oversold, bullish, bearish, etc.)
                                    import re
                                    if indicator == 'RSI':
                                        states = re.findall(r'RSI[^.]*?(overbought|oversold|neutral|bullish|bearish)', reasoning, re.IGNORECASE)
                                    elif indicator == 'MACD':
                                        states = re.findall(r'MACD[^.]*?(bullish|bearish|crossover|divergence|histogram)', reasoning, re.IGNORECASE)
                                    elif indicator == 'MFI':
                                        states = re.findall(r'MFI[^.]*?(accumulation|distribution|neutral|institutional)', reasoning, re.IGNORECASE)
                                    elif indicator == 'VWAP':
                                        states = re.findall(r'VWAP[^.]*?(above|below|precision|overbought|oversold)', reasoning, re.IGNORECASE)
                                    elif indicator == 'EMA':
                                        states = re.findall(r'EMA[^.]*?(hierarchy|cross|golden|death|bullish|bearish)', reasoning, re.IGNORECASE)
                                    else:
                                        states = []
                                    
                                    if states:
                                        logger.info(f"         ‚úÖ {indicator} with states: {states}")
                                    else:
                                        logger.info(f"         üìä {indicator} mentioned {mentions} times")
                            
                            # Check for confluence format
                            confluence_patterns = [
                                r'confluence[^.]*?rsi[^.]*?macd[^.]*?mfi',
                                r'6[/-]6|5[/-]6|4[/-]6',
                                r'confluence.*?analysis',
                                r'indicators.*?align'
                            ]
                            
                            confluence_format_found = False
                            for pattern in confluence_patterns:
                                if re.search(pattern, reasoning, re.IGNORECASE):
                                    confluence_format_found = True
                                    confluence_results['confluence_format_found'] += 1
                                    logger.info(f"         ‚úÖ Confluence format detected")
                                    break
                            
                            if not confluence_format_found:
                                logger.info(f"         üìä No explicit confluence format found")
                            
                            # Check for technical_indicators_analysis field
                            technical_indicators_analysis = ia1_analysis.get('technical_indicators_analysis', {})
                            if technical_indicators_analysis:
                                confluence_results['technical_indicators_analysis_present'] += 1
                                logger.info(f"      ‚úÖ technical_indicators_analysis field present")
                                
                                # Check for specific indicator analysis
                                indicator_fields = ['rsi_impact', 'macd_influence', 'mfi_score', 'vwap_score', 'ema_hierarchy_analysis']
                                present_fields = sum(1 for field in indicator_fields if field in technical_indicators_analysis)
                                logger.info(f"         üìä Indicator analysis fields: {present_fields}/{len(indicator_fields)}")
                            
                            # Calculate reasoning quality score
                            quality_factors = [
                                len(indicators_found) >= 4,  # At least 4 of 6 indicators mentioned
                                confluence_format_found,  # Confluence format present
                                len(reasoning) >= 200,  # Substantial reasoning
                                'technical' in reasoning.lower(),  # Technical analysis mentioned
                                any(state in reasoning.lower() for state in ['overbought', 'oversold', 'bullish', 'bearish'])  # Technical states
                            ]
                            quality_score = sum(quality_factors) / len(quality_factors)
                            confluence_results['reasoning_quality_scores'].append(quality_score)
                            
                            # Store results
                            if len(indicators_found) >= 3:  # At least 3 indicators mentioned
                                confluence_results['analyses_with_confluence_reasoning'] += 1
                                confluence_results['successful_analyses'].append({
                                    'symbol': symbol,
                                    'indicators_found': indicators_found,
                                    'confluence_format': confluence_format_found,
                                    'quality_score': quality_score,
                                    'reasoning_sample': reasoning[:300] + "..." if len(reasoning) > 300 else reasoning
                                })
                                
                                # Update global indicator counters
                                for indicator, count in indicators_found.items():
                                    if indicator not in confluence_results['indicators_mentioned']:
                                        confluence_results['indicators_mentioned'][indicator] = 0
                                    confluence_results['indicators_mentioned'][indicator] += count
                                
                                logger.info(f"      ‚úÖ Confluence reasoning SUCCESS for {symbol} (quality: {quality_score:.2f})")
                            else:
                                logger.warning(f"      ‚ùå Insufficient confluence reasoning for {symbol}")
                        else:
                            logger.warning(f"      ‚ùå No reasoning field found for {symbol}")
                    
                    else:
                        logger.error(f"      ‚ùå IA1 analysis for {symbol} failed: HTTP {response.status_code}")
                
                except Exception as e:
                    logger.error(f"      ‚ùå IA1 analysis for {symbol} exception: {e}")
                
                # Wait between analyses
                if symbol != test_symbols[-1]:
                    await asyncio.sleep(10)
            
            # Calculate final metrics
            if confluence_results['total_analyses'] > 0:
                confluence_reasoning_rate = confluence_results['analyses_with_confluence_reasoning'] / confluence_results['total_analyses']
                confluence_format_rate = confluence_results['confluence_format_found'] / confluence_results['total_analyses']
                technical_analysis_rate = confluence_results['technical_indicators_analysis_present'] / confluence_results['total_analyses']
                avg_quality_score = sum(confluence_results['reasoning_quality_scores']) / len(confluence_results['reasoning_quality_scores']) if confluence_results['reasoning_quality_scores'] else 0.0
            else:
                confluence_reasoning_rate = confluence_format_rate = technical_analysis_rate = avg_quality_score = 0.0
            
            logger.info(f"\n   üìä CONFLUENCE 6 INDICATORS REASONING VERIFICATION RESULTS:")
            logger.info(f"      Total analyses: {confluence_results['total_analyses']}")
            logger.info(f"      Analyses with confluence reasoning: {confluence_results['analyses_with_confluence_reasoning']} ({confluence_reasoning_rate:.2f})")
            logger.info(f"      Confluence format found: {confluence_results['confluence_format_found']} ({confluence_format_rate:.2f})")
            logger.info(f"      Technical indicators analysis present: {confluence_results['technical_indicators_analysis_present']} ({technical_analysis_rate:.2f})")
            logger.info(f"      Average reasoning quality score: {avg_quality_score:.2f}")
            
            if confluence_results['indicators_mentioned']:
                logger.info(f"      üìä Indicators mentioned:")
                for indicator, count in confluence_results['indicators_mentioned'].items():
                    logger.info(f"         - {indicator}: {count} mentions")
            
            # Calculate test success based on review requirements
            success_criteria = [
                confluence_results['total_analyses'] >= 1,  # At least 1 analysis
                confluence_results['analyses_with_confluence_reasoning'] >= 1,  # At least 1 with confluence reasoning
                confluence_reasoning_rate >= 0.5,  # At least 50% with confluence reasoning
                len(confluence_results['indicators_mentioned']) >= 4,  # At least 4 different indicators mentioned
                avg_quality_score >= 0.6  # Average quality score >= 60%
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.8:  # 80% success threshold
                self.log_test_result("Confluence 6 Indicators Reasoning Verification", True, 
                                   f"Confluence reasoning successful: {success_count}/{len(success_criteria)} criteria met. Confluence rate: {confluence_reasoning_rate:.2f}, Quality score: {avg_quality_score:.2f}, Indicators: {len(confluence_results['indicators_mentioned'])}")
            else:
                self.log_test_result("Confluence 6 Indicators Reasoning Verification", False, 
                                   f"Confluence reasoning issues: {success_count}/{len(success_criteria)} criteria met. May be missing explicit indicator mentions or confluence format")
                
        except Exception as e:
            self.log_test_result("Confluence 6 Indicators Reasoning Verification", False, f"Exception: {str(e)}")

    async def test_6_system_quality_assessment(self):
        """Test 6: System Quality Assessment - Overall system stability and quality"""
        logger.info("\nüîç TEST 6: System Quality Assessment")
        
        try:
            quality_results = {
                'backend_stability': True,
                'api_response_times': [],
                'system_errors': 0,
                'http_502_errors': 0,
                'analyses_quality_score': 0.0,
                'overall_system_health': 'Unknown'
            }
            
            logger.info("   üöÄ Assessing overall system quality and stability...")
            logger.info("   üìä Expected: Stable backend, no HTTP 502 errors, quality analyses")
            
            # Test system stability with multiple API calls
            logger.info("   üìû Testing system stability with multiple API calls...")
            
            test_endpoints = [
                '/opportunities',
                '/analyses',
                '/debug-anti-doublon'
            ]
            
            for endpoint in test_endpoints:
                try:
                    start_time = time.time()
                    response = requests.get(f"{self.api_url}{endpoint}", timeout=30)
                    response_time = time.time() - start_time
                    
                    quality_results['api_response_times'].append({
                        'endpoint': endpoint,
                        'response_time': response_time,
                        'status_code': response.status_code
                    })
                    
                    if response.status_code == 200:
                        logger.info(f"      ‚úÖ {endpoint}: HTTP 200 ({response_time:.2f}s)")
                    elif response.status_code == 502:
                        quality_results['http_502_errors'] += 1
                        quality_results['backend_stability'] = False
                        logger.error(f"      ‚ùå {endpoint}: HTTP 502 - Backend Error")
                    else:
                        quality_results['system_errors'] += 1
                        logger.warning(f"      ‚ö†Ô∏è {endpoint}: HTTP {response.status_code}")
                
                except Exception as e:
                    quality_results['system_errors'] += 1
                    quality_results['backend_stability'] = False
                    logger.error(f"      ‚ùå {endpoint}: Exception - {e}")
                
                await asyncio.sleep(2)  # Brief pause between calls
            
            # Check backend logs for system health
            logger.info("   üìã Checking backend logs for system health...")
            
            try:
                backend_logs = await self._capture_backend_logs()
                if backend_logs:
                    # Look for error patterns
                    error_patterns = [
                        'error', 'exception', 'failed', 'crash', 'timeout'
                    ]
                    
                    error_count = 0
                    for log_line in backend_logs:
                        if any(pattern in log_line.lower() for pattern in error_patterns):
                            error_count += 1
                    
                    total_logs = len(backend_logs)
                    error_rate = error_count / max(total_logs, 1)
                    
                    logger.info(f"      üìä Backend logs: {total_logs} lines, {error_count} errors ({error_rate:.2f} error rate)")
                    
                    if error_rate > 0.1:  # More than 10% error rate
                        quality_results['backend_stability'] = False
                        logger.warning(f"      ‚ö†Ô∏è High error rate in backend logs")
                    else:
                        logger.info(f"      ‚úÖ Backend logs show good stability")
                        
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Could not analyze backend logs: {e}")
            
            # Calculate average response times
            if quality_results['api_response_times']:
                avg_response_time = sum(rt['response_time'] for rt in quality_results['api_response_times']) / len(quality_results['api_response_times'])
                logger.info(f"      üìä Average API response time: {avg_response_time:.2f}s")
            
            # Determine overall system health
            health_factors = [
                quality_results['backend_stability'],
                quality_results['http_502_errors'] == 0,
                quality_results['system_errors'] <= 1,
                len(quality_results['api_response_times']) >= 2
            ]
            
            health_score = sum(health_factors) / len(health_factors)
            
            if health_score >= 0.75:
                quality_results['overall_system_health'] = 'Good'
            elif health_score >= 0.5:
                quality_results['overall_system_health'] = 'Fair'
            else:
                quality_results['overall_system_health'] = 'Poor'
            
            logger.info(f"\n   üìä SYSTEM QUALITY ASSESSMENT RESULTS:")
            logger.info(f"      Backend stability: {quality_results['backend_stability']}")
            logger.info(f"      HTTP 502 errors: {quality_results['http_502_errors']}")
            logger.info(f"      System errors: {quality_results['system_errors']}")
            logger.info(f"      API endpoints tested: {len(quality_results['api_response_times'])}")
            logger.info(f"      Overall system health: {quality_results['overall_system_health']}")
            
            # Calculate test success
            success_criteria = [
                quality_results['backend_stability'],
                quality_results['http_502_errors'] == 0,
                quality_results['system_errors'] <= 1,
                quality_results['overall_system_health'] in ['Good', 'Fair']
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("System Quality Assessment", True, 
                                   f"System quality good: {success_count}/{len(success_criteria)} criteria met. Health: {quality_results['overall_system_health']}, Stability: {quality_results['backend_stability']}, HTTP 502 errors: {quality_results['http_502_errors']}")
            else:
                self.log_test_result("System Quality Assessment", False, 
                                   f"System quality issues: {success_count}/{len(success_criteria)} criteria met. May have stability problems or HTTP 502 errors")
                
        except Exception as e:
            self.log_test_result("System Quality Assessment", False, f"Exception: {str(e)}")
        
        try:
            scout_results = {
                'opportunities_api_accessible': False,
                'opportunities_found': 0,
                'target_cryptos_present': {},
                'timestamp_persistence_working': False,
                'scout_data_quality': {},
                'technical_data_available': False,
                'opportunities_data': []
            }
            
            logger.info("   üöÄ Testing Scout System with limited cryptos (BTCUSDT, ETHUSDT, ADAUSDT)...")
            logger.info("   üìä Expected: Scout system returns opportunities for target cryptos with persistent timestamps")
            
            # Step 1: Test /api/opportunities endpoint
            logger.info("   üìû Testing /api/opportunities endpoint...")
            
            try:
                response = requests.get(f"{self.api_url}/opportunities", timeout=60)
                
                if response.status_code == 200:
                    response_data = response.json()
                    
                    # Handle both direct list and wrapped response
                    if isinstance(response_data, dict) and 'opportunities' in response_data:
                        opportunities = response_data['opportunities']
                    elif isinstance(response_data, list):
                        opportunities = response_data
                    else:
                        opportunities = []
                    
                    scout_results['opportunities_api_accessible'] = True
                    scout_results['opportunities_found'] = len(opportunities)
                    scout_results['opportunities_data'] = opportunities
                    
                    logger.info(f"      ‚úÖ Opportunities API accessible: {len(opportunities)} opportunities found")
                    
                    # Check for target cryptos (preferred) and get available symbols
                    target_cryptos = ['BTCUSDT', 'ETHUSDT', 'ADAUSDT']
                    found_cryptos = {}
                    available_symbols = []
                    
                    for opportunity in opportunities:
                        symbol = opportunity.get('symbol', '')
                        available_symbols.append(symbol)
                        
                        if symbol in target_cryptos:
                            found_cryptos[symbol] = {
                                'current_price': opportunity.get('current_price'),
                                'volume_24h': opportunity.get('volume_24h'),
                                'price_change_24h': opportunity.get('price_change_24h'),
                                'volatility': opportunity.get('volatility'),
                                'timestamp': opportunity.get('timestamp'),
                                'market_cap': opportunity.get('market_cap')
                            }
                            logger.info(f"         ‚úÖ {symbol} found: Price=${opportunity.get('current_price')}, Vol=${opportunity.get('volume_24h')}")
                    
                    # If preferred symbols not found, use first 3 available symbols for testing
                    if len(found_cryptos) < 2 and len(available_symbols) >= 2:
                        logger.info(f"      üìã Preferred symbols not available, using first available symbols: {available_symbols[:3]}")
                        self.actual_test_symbols = available_symbols[:3]
                        
                        # Add first 3 available symbols to found_cryptos for testing
                        for symbol in self.actual_test_symbols:
                            for opportunity in opportunities:
                                if opportunity.get('symbol') == symbol:
                                    found_cryptos[symbol] = {
                                        'current_price': opportunity.get('current_price'),
                                        'volume_24h': opportunity.get('volume_24h'),
                                        'price_change_24h': opportunity.get('price_change_24h'),
                                        'volatility': opportunity.get('volatility'),
                                        'timestamp': opportunity.get('timestamp'),
                                        'market_cap': opportunity.get('market_cap')
                                    }
                                    break
                    else:
                        self.actual_test_symbols = list(found_cryptos.keys())
                    
                    scout_results['target_cryptos_present'] = found_cryptos
                    
                    if len(found_cryptos) >= 2:
                        logger.info(f"      ‚úÖ Target cryptos found: {list(found_cryptos.keys())}")
                    else:
                        logger.warning(f"      ‚ö†Ô∏è Limited target cryptos found: {list(found_cryptos.keys())}")
                    
                    # Test timestamp persistence by calling API twice
                    logger.info("   üïê Testing timestamp persistence...")
                    await asyncio.sleep(5)  # Wait 5 seconds
                    
                    response2 = requests.get(f"{self.api_url}/opportunities", timeout=60)
                    if response2.status_code == 200:
                        opportunities2 = response2.json()
                        
                        # Compare timestamps for same symbols
                        timestamp_matches = 0
                        timestamp_total = 0
                        
                        for opp1 in opportunities:
                            symbol1 = opp1.get('symbol')
                            timestamp1 = opp1.get('timestamp')
                            
                            for opp2 in opportunities2:
                                symbol2 = opp2.get('symbol')
                                timestamp2 = opp2.get('timestamp')
                                
                                if symbol1 == symbol2:
                                    timestamp_total += 1
                                    if timestamp1 == timestamp2:
                                        timestamp_matches += 1
                                    break
                        
                        if timestamp_total > 0:
                            persistence_rate = timestamp_matches / timestamp_total
                            scout_results['timestamp_persistence_working'] = persistence_rate >= 0.8
                            logger.info(f"      üìä Timestamp persistence: {timestamp_matches}/{timestamp_total} ({persistence_rate:.2f})")
                        
                    # Analyze data quality
                    quality_metrics = {
                        'with_price': sum(1 for opp in opportunities if opp.get('current_price')),
                        'with_volume': sum(1 for opp in opportunities if opp.get('volume_24h')),
                        'with_change': sum(1 for opp in opportunities if opp.get('price_change_24h') is not None),
                        'with_volatility': sum(1 for opp in opportunities if opp.get('volatility')),
                        'with_timestamp': sum(1 for opp in opportunities if opp.get('timestamp'))
                    }
                    
                    scout_results['scout_data_quality'] = quality_metrics
                    
                    # Check if technical data is available
                    technical_fields = ['rsi_signal', 'macd_trend', 'mfi_signal', 'vwap_signal']
                    technical_count = 0
                    for opp in opportunities[:10]:  # Check first 10
                        for field in technical_fields:
                            if field in opp and opp[field] not in [None, '', 'unknown']:
                                technical_count += 1
                                break
                    
                    scout_results['technical_data_available'] = technical_count > 0
                    
                    logger.info(f"      üìä Data quality: Price={quality_metrics['with_price']}, Volume={quality_metrics['with_volume']}, Technical={technical_count}")
                
                else:
                    logger.error(f"      ‚ùå Opportunities API failed: HTTP {response.status_code}")
                    if response.text:
                        logger.error(f"         Error response: {response.text[:200]}...")
            
            except Exception as e:
                logger.error(f"      ‚ùå Opportunities API exception: {e}")
            
            # Final analysis
            logger.info(f"\n   üìä SCOUT SYSTEM LIMITED TEST RESULTS:")
            logger.info(f"      Opportunities API accessible: {scout_results['opportunities_api_accessible']}")
            logger.info(f"      Total opportunities found: {scout_results['opportunities_found']}")
            logger.info(f"      Target cryptos present: {len(scout_results['target_cryptos_present'])}/3")
            logger.info(f"      Timestamp persistence working: {scout_results['timestamp_persistence_working']}")
            logger.info(f"      Technical data available: {scout_results['technical_data_available']}")
            
            # Calculate test success
            success_criteria = [
                scout_results['opportunities_api_accessible'],
                scout_results['opportunities_found'] > 0,
                len(scout_results['target_cryptos_present']) >= 2,  # At least 2 target cryptos
                scout_results['timestamp_persistence_working']
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("Scout System Limited Test", True, 
                                   f"Scout system working: {success_count}/{len(success_criteria)} criteria met. Target cryptos: {len(scout_results['target_cryptos_present'])}, Opportunities: {scout_results['opportunities_found']}")
            else:
                self.log_test_result("Scout System Limited Test", False, 
                                   f"Scout system issues: {success_count}/{len(success_criteria)} criteria met. May have API or timestamp persistence problems")
                
        except Exception as e:
            self.log_test_result("Scout System Limited Test", False, f"Exception: {str(e)}")

    async def test_1_json_parsing_recovery_test(self):
        """Test 1: JSON Parsing Recovery Test - Verify simplified approach fixes fundamental parsing issues"""
        logger.info("\nüîç TEST 1: JSON Parsing Recovery Test")
        
        try:
            parsing_results = {
                'successful_analyses': 0,
                'total_analyses_attempted': len(self.test_symbols),
                'json_parsing_success': 0,
                'json_parsing_failures': 0,
                'fallback_analyses': 0,
                'valid_json_responses': [],
                'parsing_error_logs': [],
                'datetime_errors': 0,
                'system_stability_issues': 0,
                'backend_logs_analysis': {}
            }
            
            logger.info("   üöÄ Testing SIMPLIFIED IA1 Prompt approach for JSON parsing recovery...")
            
            # Use actual available symbols from scout test
            symbols_to_test = self.actual_test_symbols if hasattr(self, 'actual_test_symbols') and self.actual_test_symbols else self.test_symbols
            logger.info(f"   üìä Testing symbols: {symbols_to_test}")
            logger.info("   üìä Expected: IA1 returns valid JSON without 'JSON parsing failed' errors, no fallback to pattern-only analysis")
            
            # Update parsing results with actual symbol count
            parsing_results['total_analyses_attempted'] = len(symbols_to_test)
            
            # Test each symbol
            for symbol in symbols_to_test:
                logger.info(f"\n   üìû Testing IA1 simplified JSON analysis for {symbol}")
                
                try:
                    start_time = time.time()
                    response = requests.post(
                        f"{self.api_url}/force-ia1-analysis",
                        json={"symbol": symbol},
                        timeout=120  # Longer timeout for IA1 analysis
                    )
                    response_time = time.time() - start_time
                    
                    if response.status_code == 200:
                        analysis_data = response.json()
                        parsing_results['successful_analyses'] += 1
                        
                        logger.info(f"      ‚úÖ IA1 analysis for {symbol} successful (response time: {response_time:.2f}s)")
                        
                        # Check for JSON parsing success indicators
                        is_valid_json = True
                        is_fallback_analysis = False
                        has_simplified_fields = False
                        
                        # Look for fallback analysis indicators
                        if 'reasoning' in analysis_data:
                            reasoning_text = str(analysis_data['reasoning']).lower()
                            if any(fallback_indicator in reasoning_text for fallback_indicator in [
                                'fallback analysis', 'json parsing failed', 'pattern-only analysis',
                                'technical analysis fallback', 'using detected pattern'
                            ]):
                                is_fallback_analysis = True
                                parsing_results['fallback_analyses'] += 1
                                logger.warning(f"      ‚ùå Fallback analysis detected for {symbol}")
                        
                        # Check for simplified JSON schema fields in IA1 response
                        ia1_response = analysis_data.get('ia1_analysis', {})
                        if isinstance(ia1_response, dict):
                            simplified_fields_found = []
                            for field in self.simplified_json_fields:
                                if field in ia1_response:
                                    simplified_fields_found.append(field)
                            
                            if len(simplified_fields_found) >= 4:  # At least 4 simplified fields
                                has_simplified_fields = True
                                logger.info(f"      ‚úÖ Simplified JSON fields found: {simplified_fields_found}")
                            else:
                                logger.warning(f"      ‚ö†Ô∏è Limited simplified fields: {simplified_fields_found}")
                        
                        # Determine JSON parsing success
                        if not is_fallback_analysis and has_simplified_fields:
                            parsing_results['json_parsing_success'] += 1
                            parsing_results['valid_json_responses'].append({
                                'symbol': symbol,
                                'response_time': response_time,
                                'simplified_fields': simplified_fields_found,
                                'analysis_data': analysis_data
                            })
                            logger.info(f"      ‚úÖ JSON parsing SUCCESS for {symbol}")
                        else:
                            parsing_results['json_parsing_failures'] += 1
                            logger.warning(f"      ‚ùå JSON parsing FAILURE for {symbol} (fallback: {is_fallback_analysis}, simplified: {has_simplified_fields})")
                        
                        # Store full analysis for later use
                        self.ia1_analyses.append({
                            'symbol': symbol,
                            'analysis_data': analysis_data,
                            'is_valid_json': is_valid_json and not is_fallback_analysis,
                            'has_simplified_fields': has_simplified_fields
                        })
                    
                    else:
                        logger.error(f"      ‚ùå IA1 analysis for {symbol} failed: HTTP {response.status_code}")
                        if response.text:
                            logger.error(f"         Error response: {response.text[:200]}...")
                
                except Exception as e:
                    logger.error(f"      ‚ùå IA1 analysis for {symbol} exception: {e}")
                
                # Wait between analyses to avoid overwhelming the system
                if symbol != self.test_symbols[-1]:
                    logger.info(f"      ‚è≥ Waiting 10 seconds before next analysis...")
                    await asyncio.sleep(10)
            
            # Capture backend logs for JSON parsing analysis
            logger.info("   üìã Capturing backend logs for JSON parsing analysis...")
            try:
                backend_logs = await self._capture_backend_logs()
                if backend_logs:
                    self.backend_logs = backend_logs  # Store for later use
                    
                    # Look for JSON parsing errors
                    json_error_patterns = [
                        'json parsing failed', 'fallback analysis', 'json decode error',
                        'invalid json', 'parsing error', 'malformed json'
                    ]
                    
                    json_errors_found = 0
                    for log_line in backend_logs:
                        for pattern in json_error_patterns:
                            if pattern.lower() in log_line.lower():
                                json_errors_found += 1
                                parsing_results['parsing_error_logs'].append(log_line.strip())
                                break
                    
                    # Look for datetime errors
                    datetime_error_patterns = [
                        'datetime', 'timezone', 'offset-naive', 'offset-aware',
                        'can\'t subtract', 'timezone error'
                    ]
                    
                    datetime_errors_found = 0
                    for log_line in backend_logs:
                        for pattern in datetime_error_patterns:
                            if pattern.lower() in log_line.lower() and 'error' in log_line.lower():
                                datetime_errors_found += 1
                                break
                    
                    parsing_results['datetime_errors'] = datetime_errors_found
                    parsing_results['backend_logs_analysis'] = {
                        'total_logs': len(backend_logs),
                        'json_errors': json_errors_found,
                        'datetime_errors': datetime_errors_found
                    }
                    
                    logger.info(f"      üìä Backend logs analysis: {len(backend_logs)} lines, {json_errors_found} JSON errors, {datetime_errors_found} datetime errors")
                    
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Could not analyze backend logs: {e}")
            
            # Final analysis
            logger.info(f"\n   üìä JSON PARSING RECOVERY TEST RESULTS:")
            logger.info(f"      Successful analyses: {parsing_results['successful_analyses']}/{parsing_results['total_analyses_attempted']}")
            logger.info(f"      JSON parsing successes: {parsing_results['json_parsing_success']}")
            logger.info(f"      JSON parsing failures: {parsing_results['json_parsing_failures']}")
            logger.info(f"      Fallback analyses: {parsing_results['fallback_analyses']}")
            logger.info(f"      Datetime errors: {parsing_results['datetime_errors']}")
            
            # Calculate JSON parsing success rate
            json_success_rate = parsing_results['json_parsing_success'] / max(parsing_results['successful_analyses'], 1)
            fallback_rate = parsing_results['fallback_analyses'] / max(parsing_results['successful_analyses'], 1)
            
            logger.info(f"      üìä Performance metrics:")
            logger.info(f"         JSON parsing success rate: {json_success_rate:.2f} ({parsing_results['json_parsing_success']}/{parsing_results['successful_analyses']})")
            logger.info(f"         Fallback analysis rate: {fallback_rate:.2f}")
            logger.info(f"         System stability: {'Good' if parsing_results['datetime_errors'] == 0 else 'Issues detected'}")
            
            # Show valid JSON responses
            if parsing_results['valid_json_responses']:
                logger.info(f"      ‚úÖ Valid JSON responses:")
                for response in parsing_results['valid_json_responses']:
                    logger.info(f"         - {response['symbol']}: {len(response['simplified_fields'])} simplified fields")
            
            # Calculate test success based on simplified approach requirements
            success_criteria = [
                parsing_results['successful_analyses'] >= 1,  # At least 1 successful analysis
                parsing_results['json_parsing_success'] >= 1,  # At least 1 JSON parsing success
                parsing_results['fallback_analyses'] == 0,  # No fallback analyses
                parsing_results['datetime_errors'] == 0,  # No datetime errors
                json_success_rate >= 0.5  # 50% JSON success rate (target from review)
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.8:  # 80% success threshold for simplified approach
                self.log_test_result("JSON Parsing Recovery Test", True, 
                                   f"Simplified approach successful: {success_count}/{len(success_criteria)} criteria met. JSON success rate: {json_success_rate:.2f}, No fallbacks: {parsing_results['fallback_analyses'] == 0}, No datetime errors: {parsing_results['datetime_errors'] == 0}")
            else:
                self.log_test_result("JSON Parsing Recovery Test", False, 
                                   f"Simplified approach issues: {success_count}/{len(success_criteria)} criteria met. JSON parsing may still be failing or system has stability issues")
                
        except Exception as e:
            self.log_test_result("JSON Parsing Recovery Test", False, f"Exception: {str(e)}")
    
    async def test_2_technical_indicators_integration_final_check(self):
        """Test 2: Technical Indicators Integration Final Check - Test if simplified format promotes indicators usage"""
        logger.info("\nüîç TEST 2: Technical Indicators Integration Final Check")
        
        try:
            integration_results = {
                'reasoning_analysis': {},
                'technical_indicators_mentioned': {},
                'specific_values_found': {},
                'technical_indicators_object_present': False,
                'patterns_vs_indicators_ratio': 0.0,
                'simplified_format_compliance': False,
                'analyses_with_indicators': 0,
                'total_valid_analyses': 0,
                'backend_calculations_verified': False
            }
            
            logger.info("   üöÄ Testing technical indicators integration in simplified JSON format...")
            logger.info("   üìä Expected: IA1 reasoning mentions specific calculated values: RSI 32.1, MACD -0.015, MFI 20.2, VWAP -3.9%")
            
            # Step 1: Analyze valid JSON responses from previous test
            logger.info("   üìä Analyzing valid JSON responses for technical indicators integration...")
            
            valid_analyses = []
            if hasattr(self, 'ia1_analyses') and self.ia1_analyses:
                for analysis in self.ia1_analyses:
                    if analysis.get('is_valid_json', False):
                        valid_analyses.append(analysis)
                        integration_results['total_valid_analyses'] += 1
                
                logger.info(f"      ‚úÖ Found {len(valid_analyses)} valid JSON analyses to examine")
            else:
                logger.warning(f"      ‚ö†Ô∏è No valid JSON analyses available from previous test")
                return
            
            # Step 2: Analyze reasoning field for specific technical indicator values
            logger.info("   üìã Analyzing reasoning field for specific technical indicator values...")
            
            for analysis in valid_analyses:
                symbol = analysis['symbol']
                analysis_data = analysis['analysis_data']
                ia1_response = analysis_data.get('ia1_analysis', {})
                
                logger.info(f"      üìä Analyzing {symbol} reasoning for technical indicators...")
                
                # Check reasoning field for specific values
                reasoning = ia1_response.get('reasoning', '')
                if reasoning:
                    logger.info(f"         ‚úÖ Reasoning field found: {len(reasoning)} chars")
                    
                    # Look for specific technical indicator values (RSI 32.1, MACD -0.015, etc.)
                    specific_values_found = {}
                    for pattern in self.specific_value_patterns:
                        matches = re.findall(pattern, reasoning, re.IGNORECASE)
                        if matches:
                            indicator = pattern.split('\\')[0]  # Extract indicator name
                            specific_values_found[indicator] = matches
                            logger.info(f"            ‚úÖ {indicator} specific values found: {matches}")
                    
                    integration_results['specific_values_found'][symbol] = specific_values_found
                    
                    # Count technical indicators vs patterns mentions
                    technical_count = 0
                    patterns_count = 0
                    
                    for indicator in self.core_technical_indicators:
                        count = reasoning.lower().count(indicator.lower())
                        if count > 0:
                            technical_count += count
                            if indicator not in integration_results['technical_indicators_mentioned']:
                                integration_results['technical_indicators_mentioned'][indicator] = 0
                            integration_results['technical_indicators_mentioned'][indicator] += count
                    
                    for pattern in self.chartist_patterns:
                        count = reasoning.lower().count(pattern.lower())
                        patterns_count += count
                    
                    # Calculate ratio
                    total_mentions = technical_count + patterns_count
                    if total_mentions > 0:
                        tech_ratio = technical_count / total_mentions
                        integration_results['reasoning_analysis'][symbol] = {
                            'technical_count': technical_count,
                            'patterns_count': patterns_count,
                            'technical_ratio': tech_ratio,
                            'specific_values': len(specific_values_found),
                            'reasoning_sample': reasoning[:300] + "..." if len(reasoning) > 300 else reasoning
                        }
                        
                        logger.info(f"            üìä Technical: {technical_count}, Patterns: {patterns_count}, Ratio: {tech_ratio:.2f}")
                
                # Check technical_indicators object
                technical_indicators_obj = ia1_response.get('technical_indicators', {})
                if isinstance(technical_indicators_obj, dict) and technical_indicators_obj:
                    integration_results['technical_indicators_object_present'] = True
                    logger.info(f"         ‚úÖ technical_indicators object found with {len(technical_indicators_obj)} fields")
                    
                    # Check for specific fields
                    found_fields = []
                    for field in self.technical_indicators_fields:
                        if field in technical_indicators_obj:
                            found_fields.append(field)
                    
                    logger.info(f"            üìä Technical indicator fields: {found_fields}")
                
                # Check if this analysis has technical indicators
                if (symbol in integration_results['specific_values_found'] and 
                    integration_results['specific_values_found'][symbol]) or \
                   any(indicator in integration_results['technical_indicators_mentioned'] 
                       for indicator in self.core_technical_indicators):
                    integration_results['analyses_with_indicators'] += 1
            
            # Step 3: Verify backend calculations
            logger.info("   üîç Verifying backend technical indicator calculations...")
            
            try:
                if hasattr(self, 'backend_logs') and self.backend_logs:
                    backend_logs = self.backend_logs
                else:
                    backend_logs = await self._capture_backend_logs()
                
                if backend_logs:
                    # Look for technical indicator calculations
                    calculation_patterns = [
                        r'RSI[:\s]+[\d\.]+',
                        r'MACD[:\s]+[\d\.\-]+',
                        r'MFI[:\s]+[\d\.]+',
                        r'VWAP[:\s]+[\d\.\-]+%?'
                    ]
                    
                    calculations_found = {}
                    for log_line in backend_logs:
                        for pattern in calculation_patterns:
                            matches = re.findall(pattern, log_line, re.IGNORECASE)
                            if matches:
                                indicator = pattern.split('[')[0]
                                if indicator not in calculations_found:
                                    calculations_found[indicator] = []
                                calculations_found[indicator].extend(matches)
                    
                    if calculations_found:
                        integration_results['backend_calculations_verified'] = True
                        logger.info(f"      ‚úÖ Backend calculations found:")
                        for indicator, values in calculations_found.items():
                            logger.info(f"         - {indicator}: {values[:3]}...")  # Show first 3
                    else:
                        logger.warning(f"      ‚ùå No backend technical indicator calculations found")
                        
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Backend calculations check error: {e}")
            
            # Step 4: Calculate overall technical indicators usage
            logger.info("   üìä Calculating overall technical indicators usage...")
            
            total_technical_mentions = sum(integration_results['technical_indicators_mentioned'].values())
            total_specific_values = sum(len(values) for values in integration_results['specific_values_found'].values())
            
            # Calculate improvement over previous 0% usage
            indicators_usage_rate = integration_results['analyses_with_indicators'] / max(integration_results['total_valid_analyses'], 1)
            
            logger.info(f"      üìä Technical indicators usage rate: {indicators_usage_rate:.2f} ({integration_results['analyses_with_indicators']}/{integration_results['total_valid_analyses']})")
            logger.info(f"      üìä Total technical mentions: {total_technical_mentions}")
            logger.info(f"      üìä Specific values found: {total_specific_values}")
            
            # Final analysis
            logger.info(f"\n   üìä TECHNICAL INDICATORS INTEGRATION FINAL CHECK RESULTS:")
            logger.info(f"      Total valid analyses: {integration_results['total_valid_analyses']}")
            logger.info(f"      Analyses with indicators: {integration_results['analyses_with_indicators']}")
            logger.info(f"      Technical indicators object present: {integration_results['technical_indicators_object_present']}")
            logger.info(f"      Backend calculations verified: {integration_results['backend_calculations_verified']}")
            logger.info(f"      Indicators usage rate: {indicators_usage_rate:.2f}")
            
            if integration_results['technical_indicators_mentioned']:
                logger.info(f"      üìä Technical indicators mentioned:")
                for indicator, count in integration_results['technical_indicators_mentioned'].items():
                    logger.info(f"         - {indicator}: {count} mentions")
            
            if integration_results['specific_values_found']:
                logger.info(f"      üìä Specific values found in reasoning:")
                for symbol, values in integration_results['specific_values_found'].items():
                    if values:
                        logger.info(f"         - {symbol}: {len(values)} indicators with specific values")
            
            # Calculate test success based on review requirements (>50% technical indicators usage)
            success_criteria = [
                integration_results['total_valid_analyses'] > 0,  # Have valid analyses
                integration_results['analyses_with_indicators'] > 0,  # Some analyses have indicators
                indicators_usage_rate >= 0.5,  # >50% usage rate (target from review)
                total_specific_values > 0,  # Some specific values found
                integration_results['backend_calculations_verified']  # Backend calculations working
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.8:  # 80% success threshold
                self.log_test_result("Technical Indicators Integration Final Check", True, 
                                   f"Technical indicators integration successful: {success_count}/{len(success_criteria)} criteria met. Usage rate: {indicators_usage_rate:.2f}, Specific values: {total_specific_values}, Backend verified: {integration_results['backend_calculations_verified']}")
            else:
                self.log_test_result("Technical Indicators Integration Final Check", False, 
                                   f"Technical indicators integration issues: {success_count}/{len(success_criteria)} criteria met. Usage rate may still be below 50% target or backend calculations not working")
                
        except Exception as e:
            self.log_test_result("Technical Indicators Integration Final Check", False, f"Exception: {str(e)}")
    
    async def test_3_prompt_effectiveness_check(self):
        """Test 3: Prompt Effectiveness Check - Analyze if prompt instructions are clear"""
        logger.info("\nüîç TEST 3: Prompt Effectiveness Check")
        
        try:
            prompt_results = {
                'server_code_analyzed': False,
                'prompt_instructions_found': False,
                'technical_indicators_in_prompt': {},
                'confluence_matrix_mentioned': False,
                'six_indicator_system_found': False,
                'prompt_analysis_successful': False,
                'prompt_samples': [],
                'instruction_clarity_score': 0
            }
            
            logger.info("   üöÄ Analyzing IA1 prompt instructions for technical indicators emphasis...")
            logger.info("   üìä Expected: Prompt explicitly asks IA1 to use RSI, MACD, MFI, VWAP, 6-indicator confluence")
            
            # Step 1: Analyze server.py code for IA1 prompt construction
            logger.info("   üìã Analyzing server.py code for IA1 prompt instructions...")
            
            try:
                with open('/app/backend/server.py', 'r') as f:
                    server_code = f.read()
                
                prompt_results['server_code_analyzed'] = True
                logger.info(f"      ‚úÖ Server code loaded: {len(server_code)} characters")
                
                # Look for IA1 chat initialization and system message
                ia1_patterns = [
                    'get_ia1_chat', 'system_message', 'technical analyst',
                    'RSI', 'MACD', 'MFI', 'VWAP', 'Bollinger', 'Stochastic',
                    'confluence', '6-indicator', 'technical indicators'
                ]
                
                found_patterns = {}
                for pattern in ia1_patterns:
                    count = server_code.count(pattern)
                    if count > 0:
                        found_patterns[pattern] = count
                        if pattern in ['RSI', 'MACD', 'MFI', 'VWAP', 'Bollinger', 'Stochastic']:
                            prompt_results['technical_indicators_in_prompt'][pattern] = count
                
                if found_patterns:
                    prompt_results['prompt_instructions_found'] = True
                    logger.info(f"      ‚úÖ IA1 prompt patterns found:")
                    for pattern, count in found_patterns.items():
                        logger.info(f"         - {pattern}: {count} occurrences")
                else:
                    logger.warning(f"      ‚ùå No IA1 prompt patterns found")
                
                # Look for confluence matrix instructions
                confluence_patterns = [
                    'confluence matrix', '6-indicator', 'CONFLUENCE', 'voting system',
                    'indicator confluence', 'technical confluence'
                ]
                
                confluence_found = False
                for pattern in confluence_patterns:
                    if pattern.lower() in server_code.lower():
                        confluence_found = True
                        logger.info(f"      ‚úÖ Confluence pattern found: {pattern}")
                        break
                
                prompt_results['confluence_matrix_mentioned'] = confluence_found
                
                # Look for 6-indicator system
                six_indicator_patterns = [
                    '6-indicator', 'six indicator', 'MFI', 'VWAP', 'RSI', 'Multi-Timeframe', 'Volume', 'EMA'
                ]
                
                six_indicator_count = 0
                for pattern in six_indicator_patterns:
                    if pattern.lower() in server_code.lower():
                        six_indicator_count += 1
                
                if six_indicator_count >= 4:  # At least 4/8 patterns found
                    prompt_results['six_indicator_system_found'] = True
                    logger.info(f"      ‚úÖ 6-indicator system found: {six_indicator_count}/{len(six_indicator_patterns)} patterns")
                else:
                    logger.warning(f"      ‚ùå 6-indicator system incomplete: {six_indicator_count}/{len(six_indicator_patterns)} patterns")
                
                # Extract sample prompt instructions
                try:
                    # Look for system message content
                    system_message_pattern = r'system_message="""(.*?)"""'
                    matches = re.findall(system_message_pattern, server_code, re.DOTALL)
                    
                    if matches:
                        for i, match in enumerate(matches[:2]):  # First 2 matches
                            sample = match[:500] + "..." if len(match) > 500 else match
                            prompt_results['prompt_samples'].append(f"Sample {i+1}: {sample}")
                            logger.info(f"      üìù Prompt sample {i+1}: {sample[:150]}...")
                    
                except Exception as e:
                    logger.debug(f"      Could not extract prompt samples: {e}")
                
                prompt_results['prompt_analysis_successful'] = True
                
            except Exception as e:
                logger.error(f"      ‚ùå Server code analysis error: {e}")
            
            # Step 2: Check backend logs for prompt construction
            logger.info("   üìã Checking backend logs for prompt construction evidence...")
            
            try:
                if hasattr(self, 'backend_logs') and self.backend_logs:
                    backend_logs = self.backend_logs
                else:
                    backend_logs = await self._capture_backend_logs()
                
                if backend_logs:
                    # Look for prompt-related logs
                    prompt_logs = []
                    prompt_patterns = [
                        'prompt', 'system message', 'IA1 analysis', 'technical indicators',
                        'confluence', 'sending to IA1'
                    ]
                    
                    for log_line in backend_logs:
                        for pattern in prompt_patterns:
                            if pattern.lower() in log_line.lower():
                                prompt_logs.append(log_line.strip())
                                break
                    
                    if prompt_logs:
                        logger.info(f"      ‚úÖ Prompt construction logs found: {len(prompt_logs)} entries")
                        for log in prompt_logs[:2]:  # Show first 2
                            logger.info(f"         - {log[:100]}...")
                    else:
                        logger.info(f"      üìã No specific prompt construction logs found")
                        
                else:
                    logger.warning(f"      ‚ö†Ô∏è Could not access backend logs")
                    
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Log analysis error: {e}")
            
            # Step 3: Calculate instruction clarity score
            logger.info("   üìä Calculating prompt instruction clarity score...")
            
            clarity_factors = [
                prompt_results['prompt_instructions_found'],  # Basic prompt found
                len(prompt_results['technical_indicators_in_prompt']) >= 4,  # At least 4 indicators mentioned
                prompt_results['confluence_matrix_mentioned'],  # Confluence matrix mentioned
                prompt_results['six_indicator_system_found'],  # 6-indicator system found
                len(prompt_results['prompt_samples']) > 0  # Prompt samples extracted
            ]
            
            clarity_score = sum(clarity_factors) / len(clarity_factors)
            prompt_results['instruction_clarity_score'] = clarity_score
            
            logger.info(f"      üìä Instruction clarity score: {clarity_score:.2f} ({sum(clarity_factors)}/{len(clarity_factors)} factors)")
            
            # Final analysis
            logger.info(f"\n   üìä PROMPT EFFECTIVENESS ANALYSIS:")
            logger.info(f"      Server code analyzed: {prompt_results['server_code_analyzed']}")
            logger.info(f"      Prompt instructions found: {prompt_results['prompt_instructions_found']}")
            logger.info(f"      Technical indicators in prompt: {len(prompt_results['technical_indicators_in_prompt'])}")
            logger.info(f"      Confluence matrix mentioned: {prompt_results['confluence_matrix_mentioned']}")
            logger.info(f"      6-indicator system found: {prompt_results['six_indicator_system_found']}")
            logger.info(f"      Instruction clarity score: {prompt_results['instruction_clarity_score']:.2f}")
            
            if prompt_results['technical_indicators_in_prompt']:
                logger.info(f"      üìä Technical indicators in prompt:")
                for indicator, count in prompt_results['technical_indicators_in_prompt'].items():
                    logger.info(f"         - {indicator}: {count} mentions")
            
            # Calculate test success
            success_criteria = [
                prompt_results['server_code_analyzed'],
                prompt_results['prompt_instructions_found'],
                len(prompt_results['technical_indicators_in_prompt']) >= 3,  # At least 3 indicators
                prompt_results['confluence_matrix_mentioned'] or prompt_results['six_indicator_system_found']
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("Prompt Effectiveness Check", True, 
                                   f"Prompt effectiveness good: {success_count}/{len(success_criteria)} criteria met. Clarity score: {prompt_results['instruction_clarity_score']:.2f}, Indicators: {len(prompt_results['technical_indicators_in_prompt'])}")
            else:
                self.log_test_result("Prompt Effectiveness Check", False, 
                                   f"Prompt effectiveness issues: {success_count}/{len(success_criteria)} criteria met. May need clearer technical indicator instructions")
                
        except Exception as e:
            self.log_test_result("Prompt Effectiveness Check", False, f"Exception: {str(e)}")
    
    async def test_4_database_storage_verification(self):
        """Test 4: Database Storage Verification - Check stored IA1 analyses"""
        logger.info("\nüîç TEST 4: Database Storage Verification")
        
        try:
            storage_results = {
                'analyses_api_accessible': False,
                'recent_analyses_found': False,
                'technical_indicators_in_db': {},
                'reasoning_fields_present': False,
                'database_integration_working': False,
                'analyses_data': [],
                'technical_references_preserved': False,
                'sample_analyses': []
            }
            
            logger.info("   üöÄ Testing database storage of IA1 analyses with technical indicators...")
            logger.info("   üìä Expected: Database stores IA1 analyses with technical indicator references")
            
            # Step 1: Query /api/analyses to get recent IA1 analyses
            logger.info("   üìû Querying /api/analyses for recent IA1 analyses...")
            
            try:
                response = requests.get(f"{self.api_url}/analyses", timeout=60)
                
                if response.status_code == 200:
                    analyses = response.json()
                    storage_results['analyses_api_accessible'] = True
                    
                    logger.info(f"      ‚úÖ Analyses API accessible: {len(analyses)} analyses retrieved")
                    
                    if len(analyses) > 0:
                        storage_results['recent_analyses_found'] = True
                        storage_results['analyses_data'] = analyses
                        
                        # Analyze each analysis for technical indicators
                        technical_indicators_found = {}
                        reasoning_count = 0
                        
                        for i, analysis in enumerate(analyses[:10]):  # Check first 10 analyses
                            # Check if reasoning field is present
                            reasoning = ""
                            if 'reasoning' in analysis:
                                reasoning = analysis['reasoning']
                                reasoning_count += 1
                            elif 'ia1_reasoning' in analysis:
                                reasoning = analysis['ia1_reasoning']
                                reasoning_count += 1
                            
                            if reasoning:
                                # Look for technical indicators in stored reasoning
                                for indicator in self.core_technical_indicators:
                                    if indicator.lower() in reasoning.lower():
                                        if indicator not in technical_indicators_found:
                                            technical_indicators_found[indicator] = 0
                                        technical_indicators_found[indicator] += 1
                                
                                # Store sample analysis
                                if len(storage_results['sample_analyses']) < 3:
                                    storage_results['sample_analyses'].append({
                                        'symbol': analysis.get('symbol', 'Unknown'),
                                        'reasoning_length': len(reasoning),
                                        'reasoning_sample': reasoning[:200] + "..." if len(reasoning) > 200 else reasoning,
                                        'timestamp': analysis.get('timestamp', 'Unknown')
                                    })
                        
                        storage_results['technical_indicators_in_db'] = technical_indicators_found
                        
                        if reasoning_count > 0:
                            storage_results['reasoning_fields_present'] = True
                            logger.info(f"      ‚úÖ Reasoning fields found in {reasoning_count}/{len(analyses[:10])} analyses")
                        else:
                            logger.warning(f"      ‚ùå No reasoning fields found in stored analyses")
                        
                        if technical_indicators_found:
                            storage_results['technical_references_preserved'] = True
                            logger.info(f"      ‚úÖ Technical indicators found in database:")
                            for indicator, count in sorted(technical_indicators_found.items(), key=lambda x: x[1], reverse=True):
                                logger.info(f"         - {indicator}: {count} references")
                        else:
                            logger.warning(f"      ‚ùå No technical indicators found in stored analyses")
                        
                        # Show sample analyses
                        logger.info(f"      üìù Sample stored analyses:")
                        for sample in storage_results['sample_analyses']:
                            logger.info(f"         - {sample['symbol']}: {sample['reasoning_sample'][:100]}...")
                    
                    else:
                        logger.warning(f"      ‚ö†Ô∏è No analyses found in database")
                
                else:
                    logger.error(f"      ‚ùå Analyses API failed: HTTP {response.status_code}")
                    if response.text:
                        logger.error(f"         Error response: {response.text[:200]}...")
            
            except Exception as e:
                logger.error(f"      ‚ùå Analyses API exception: {e}")
            
            # Step 2: Check database integration with recent IA1 analyses
            logger.info("   üìä Checking database integration with recent IA1 analyses...")
            
            try:
                # Compare with our IA1 analyses from test 1
                if hasattr(self, 'ia1_analyses') and self.ia1_analyses:
                    logger.info(f"      üìã Comparing with {len(self.ia1_analyses)} IA1 analyses from earlier tests...")
                    
                    # Check if recent analyses match our test analyses
                    test_symbols = [analysis['symbol'] for analysis in self.ia1_analyses]
                    db_symbols = [analysis.get('symbol') for analysis in storage_results['analyses_data'][:20]]
                    
                    matching_symbols = set(test_symbols) & set(db_symbols)
                    
                    if matching_symbols:
                        storage_results['database_integration_working'] = True
                        logger.info(f"      ‚úÖ Database integration working: {len(matching_symbols)} matching symbols found")
                        logger.info(f"         Matching symbols: {list(matching_symbols)}")
                    else:
                        logger.warning(f"      ‚ö†Ô∏è No matching symbols between test analyses and database")
                        logger.info(f"         Test symbols: {test_symbols}")
                        logger.info(f"         DB symbols: {db_symbols[:5]}...")
                
                else:
                    logger.info(f"      üìã No previous IA1 analyses to compare with")
                    
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Database integration check error: {e}")
            
            # Step 3: Verify technical indicator data persistence
            logger.info("   üîç Verifying technical indicator data persistence...")
            
            try:
                # Look for specific technical indicator fields in analyses
                indicator_fields = ['rsi_signal', 'macd_trend', 'mfi_signal', 'vwap_signal', 'bollinger_position']
                
                field_counts = {}
                for analysis in storage_results['analyses_data'][:10]:
                    for field in indicator_fields:
                        if field in analysis and analysis[field] not in [None, '', 'unknown']:
                            if field not in field_counts:
                                field_counts[field] = 0
                            field_counts[field] += 1
                
                if field_counts:
                    logger.info(f"      ‚úÖ Technical indicator fields found in database:")
                    for field, count in field_counts.items():
                        logger.info(f"         - {field}: {count} non-null values")
                else:
                    logger.warning(f"      ‚ùå No technical indicator fields with values found")
                
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è Technical indicator field check error: {e}")
            
            # Final analysis
            logger.info(f"\n   üìä DATABASE STORAGE VERIFICATION ANALYSIS:")
            logger.info(f"      Analyses API accessible: {storage_results['analyses_api_accessible']}")
            logger.info(f"      Recent analyses found: {storage_results['recent_analyses_found']}")
            logger.info(f"      Reasoning fields present: {storage_results['reasoning_fields_present']}")
            logger.info(f"      Technical references preserved: {storage_results['technical_references_preserved']}")
            logger.info(f"      Database integration working: {storage_results['database_integration_working']}")
            logger.info(f"      Technical indicators in DB: {len(storage_results['technical_indicators_in_db'])}")
            
            # Calculate test success
            success_criteria = [
                storage_results['analyses_api_accessible'],
                storage_results['recent_analyses_found'],
                storage_results['reasoning_fields_present'],
                storage_results['technical_references_preserved'] or len(storage_results['technical_indicators_in_db']) >= 2
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("Database Storage Verification", True, 
                                   f"Database storage working: {success_count}/{len(success_criteria)} criteria met. Technical indicators: {len(storage_results['technical_indicators_in_db'])}, Reasoning preserved: {storage_results['reasoning_fields_present']}")
            else:
                self.log_test_result("Database Storage Verification", False, 
                                   f"Database storage issues: {success_count}/{len(success_criteria)} criteria met. Technical references may not be preserved properly")
                
        except Exception as e:
            self.log_test_result("Database Storage Verification", False, f"Exception: {str(e)}")
    
    async def test_5_ia1_response_analysis_comparison(self):
        """Test 5: IA1 Response Analysis - Compare chartist vs technical indicator usage"""
        logger.info("\nüîç TEST 5: IA1 Response Analysis Comparison")
        
        try:
            comparison_results = {
                'analyses_available': False,
                'technical_vs_chartist_analysis': {},
                'balance_assessment': {},
                'usage_patterns': {},
                'recommendation_analysis': {},
                'overall_balance_score': 0.0,
                'detailed_breakdown': []
            }
            
            logger.info("   üöÄ Analyzing IA1 responses for technical indicators vs chartist patterns balance...")
            logger.info("   üìä Expected: IA1 balances both technical indicators and chartist patterns")
            
            # Use analyses from previous tests
            if hasattr(self, 'ia1_analyses') and self.ia1_analyses:
                comparison_results['analyses_available'] = True
                analyses_to_analyze = self.ia1_analyses
                logger.info(f"      ‚úÖ Using {len(analyses_to_analyze)} IA1 analyses from previous tests")
            else:
                logger.warning(f"      ‚ö†Ô∏è No previous IA1 analyses available, will analyze stored analyses")
                analyses_to_analyze = []
            
            # If no previous analyses, try to get some from database
            if not analyses_to_analyze:
                try:
                    response = requests.get(f"{self.api_url}/analyses", timeout=60)
                    if response.status_code == 200:
                        db_analyses = response.json()[:5]  # Get first 5
                        for analysis in db_analyses:
                            if 'reasoning' in analysis or 'ia1_reasoning' in analysis:
                                reasoning = analysis.get('reasoning') or analysis.get('ia1_reasoning', '')
                                analyses_to_analyze.append({
                                    'symbol': analysis.get('symbol', 'Unknown'),
                                    'reasoning': reasoning,
                                    'analysis_data': analysis
                                })
                        logger.info(f"      üìã Retrieved {len(analyses_to_analyze)} analyses from database")
                except Exception as e:
                    logger.warning(f"      ‚ö†Ô∏è Could not retrieve analyses from database: {e}")
            
            if analyses_to_analyze:
                comparison_results['analyses_available'] = True
                
                # Analyze each IA1 response
                total_technical_mentions = 0
                total_chartist_mentions = 0
                analysis_breakdown = []
                
                for analysis in analyses_to_analyze:
                    symbol = analysis['symbol']
                    reasoning = analysis['reasoning']
                    
                    if not reasoning:
                        continue
                    
                    # Count technical indicators
                    technical_count = 0
                    technical_found = []
                    for indicator in self.core_technical_indicators:
                        count = reasoning.lower().count(indicator.lower())
                        if count > 0:
                            technical_count += count
                            technical_found.append(f"{indicator}({count})")
                    
                    # Count chartist patterns
                    chartist_count = 0
                    chartist_found = []
                    for pattern in self.chartist_patterns:
                        count = reasoning.lower().count(pattern.lower())
                        if count > 0:
                            chartist_count += count
                            chartist_found.append(f"{pattern}({count})")
                    
                    # Calculate balance for this analysis
                    total_mentions = technical_count + chartist_count
                    technical_ratio = technical_count / total_mentions if total_mentions > 0 else 0
                    
                    analysis_result = {
                        'symbol': symbol,
                        'technical_count': technical_count,
                        'chartist_count': chartist_count,
                        'technical_ratio': technical_ratio,
                        'technical_found': technical_found,
                        'chartist_found': chartist_found,
                        'reasoning_length': len(reasoning),
                        'balance_category': self._categorize_balance(technical_ratio)
                    }
                    
                    analysis_breakdown.append(analysis_result)
                    total_technical_mentions += technical_count
                    total_chartist_mentions += chartist_count
                    
                    logger.info(f"      üìä {symbol}: Technical={technical_count}, Chartist={chartist_count}, Ratio={technical_ratio:.2f}")
                
                comparison_results['detailed_breakdown'] = analysis_breakdown
                
                # Overall analysis
                total_mentions = total_technical_mentions + total_chartist_mentions
                overall_technical_ratio = total_technical_mentions / total_mentions if total_mentions > 0 else 0
                
                comparison_results['technical_vs_chartist_analysis'] = {
                    'total_technical_mentions': total_technical_mentions,
                    'total_chartist_mentions': total_chartist_mentions,
                    'overall_technical_ratio': overall_technical_ratio,
                    'total_analyses': len(analysis_breakdown)
                }
                
                # Balance assessment
                balance_categories = {}
                for analysis in analysis_breakdown:
                    category = analysis['balance_category']
                    if category not in balance_categories:
                        balance_categories[category] = 0
                    balance_categories[category] += 1
                
                comparison_results['balance_assessment'] = balance_categories
                
                # Usage patterns
                technical_usage = sum(1 for a in analysis_breakdown if a['technical_count'] > 0)
                chartist_usage = sum(1 for a in analysis_breakdown if a['chartist_count'] > 0)
                both_usage = sum(1 for a in analysis_breakdown if a['technical_count'] > 0 and a['chartist_count'] > 0)
                
                comparison_results['usage_patterns'] = {
                    'analyses_with_technical': technical_usage,
                    'analyses_with_chartist': chartist_usage,
                    'analyses_with_both': both_usage,
                    'technical_only': technical_usage - both_usage,
                    'chartist_only': chartist_usage - both_usage
                }
                
                # Calculate overall balance score (0.5 = perfect balance)
                balance_score = 1.0 - abs(0.5 - overall_technical_ratio)
                comparison_results['overall_balance_score'] = balance_score
                
                # Detailed reporting
                logger.info(f"\n      üìä DETAILED ANALYSIS RESULTS:")
                logger.info(f"         Total technical mentions: {total_technical_mentions}")
                logger.info(f"         Total chartist mentions: {total_chartist_mentions}")
                logger.info(f"         Overall technical ratio: {overall_technical_ratio:.2f}")
                logger.info(f"         Balance score: {balance_score:.2f} (1.0 = perfect)")
                
                logger.info(f"      üìä Usage patterns:")
                logger.info(f"         Analyses with technical indicators: {technical_usage}/{len(analysis_breakdown)}")
                logger.info(f"         Analyses with chartist patterns: {chartist_usage}/{len(analysis_breakdown)}")
                logger.info(f"         Analyses with both: {both_usage}/{len(analysis_breakdown)}")
                
                logger.info(f"      üìä Balance categories:")
                for category, count in balance_categories.items():
                    logger.info(f"         {category}: {count} analyses")
                
            else:
                logger.warning(f"      ‚ùå No IA1 analyses available for comparison")
            
            # Final analysis
            logger.info(f"\n   üìä IA1 RESPONSE ANALYSIS COMPARISON:")
            logger.info(f"      Analyses available: {comparison_results['analyses_available']}")
            if comparison_results['analyses_available']:
                logger.info(f"      Overall balance score: {comparison_results['overall_balance_score']:.2f}")
                logger.info(f"      Technical ratio: {comparison_results['technical_vs_chartist_analysis'].get('overall_technical_ratio', 0):.2f}")
                logger.info(f"      Analyses with both types: {comparison_results['usage_patterns'].get('analyses_with_both', 0)}")
            
            # Calculate test success
            success_criteria = [
                comparison_results['analyses_available'],
                comparison_results['overall_balance_score'] >= 0.3,  # At least some balance
                comparison_results['usage_patterns'].get('analyses_with_technical', 0) > 0,  # Some technical usage
                comparison_results['usage_patterns'].get('analyses_with_both', 0) > 0  # Some analyses use both
            ]
            success_count = sum(success_criteria)
            success_rate = success_count / len(success_criteria)
            
            if success_rate >= 0.75:  # 75% success threshold
                self.log_test_result("IA1 Response Analysis Comparison", True, 
                                   f"Response analysis successful: {success_count}/{len(success_criteria)} criteria met. Balance score: {comparison_results['overall_balance_score']:.2f}, Both types used: {comparison_results['usage_patterns'].get('analyses_with_both', 0)}")
            else:
                self.log_test_result("IA1 Response Analysis Comparison", False, 
                                   f"Response analysis issues: {success_count}/{len(success_criteria)} criteria met. May be focusing too much on one type of analysis")
                
        except Exception as e:
            self.log_test_result("IA1 Response Analysis Comparison", False, f"Exception: {str(e)}")
    
    def _categorize_balance(self, technical_ratio):
        """Categorize the balance between technical and chartist analysis"""
        if technical_ratio == 0:
            return "Chartist Only"
        elif technical_ratio == 1:
            return "Technical Only"
        elif 0.4 <= technical_ratio <= 0.6:
            return "Balanced"
        elif technical_ratio > 0.6:
            return "Technical Heavy"
        else:
            return "Chartist Heavy"
    
    async def _capture_backend_logs(self):
        """Capture recent backend logs"""
        try:
            log_files = [
                "/var/log/supervisor/backend.out.log",
                "/var/log/supervisor/backend.err.log"
            ]
            
            all_logs = []
            for log_file in log_files:
                if os.path.exists(log_file):
                    try:
                        result = subprocess.run(['tail', '-n', '100', log_file], 
                                              capture_output=True, text=True, timeout=10)
                        if result.returncode == 0:
                            all_logs.extend(result.stdout.split('\n'))
                    except Exception:
                        pass
            
            return [log for log in all_logs if log.strip()]
        except Exception:
            return []
    
    async def _capture_backend_logs(self):
        """Capture recent backend logs for analysis"""
        try:
            # Try to capture supervisor backend logs
            result = subprocess.run(
                ["tail", "-n", "100", "/var/log/supervisor/backend.out.log"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0 and result.stdout:
                return result.stdout.split('\n')
            
            # Fallback: try error log
            result = subprocess.run(
                ["tail", "-n", "100", "/var/log/supervisor/backend.err.log"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0 and result.stdout:
                return result.stdout.split('\n')
            
            return []
            
        except Exception as e:
            logger.warning(f"Could not capture backend logs: {e}")
            return []
    
    async def run_all_tests(self):
        """Run all tests focusing on Dynamic RR Integration Phase 1 validation"""
        logger.info("üöÄ STARTING DYNAMIC RR INTEGRATION TESTING SUITE - PHASE 1")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # Run Dynamic RR Integration tests (Phase 1 focus)
        await self.test_1_field_name_validation()
        await self.test_2_dynamic_rr_escalation_logic()
        await self.test_3_database_persistence_validation()
        
        # Calculate overall results
        total_time = time.time() - start_time
        passed_tests = sum(1 for result in self.test_results if result['success'])
        total_tests = len(self.test_results)
        
        logger.info("\n" + "=" * 80)
        logger.info("üéØ DYNAMIC RR INTEGRATION PHASE 1 VALIDATION SUMMARY")
        logger.info("=" * 80)
        
        # Check specific test results
        field_name_result = next((r for r in self.test_results if 'field name' in r['test'].lower()), None)
        escalation_result = next((r for r in self.test_results if 'escalation' in r['test'].lower()), None)
        persistence_result = next((r for r in self.test_results if 'persistence' in r['test'].lower()), None)
        
        if field_name_result:
            status = "‚úÖ SUCCESSFUL" if field_name_result['success'] else "‚ùå FAILED"
            logger.info(f"{status} FIELD NAME VALIDATION: {field_name_result['details']}")
        
        if escalation_result:
            status = "‚úÖ SUCCESSFUL" if escalation_result['success'] else "‚ùå FAILED"
            logger.info(f"{status} DYNAMIC RR ESCALATION: {escalation_result['details']}")
        
        if persistence_result:
            status = "‚úÖ SUCCESSFUL" if persistence_result['success'] else "‚ùå FAILED"
            logger.info(f"{status} DATABASE PERSISTENCE: {persistence_result['details']}")
        
        for result in self.test_results:
            status = "‚úÖ PASS" if result['success'] else "‚ùå FAIL"
            logger.info(f"{status}: {result['test']}")
            if result['details']:
                logger.info(f"   Details: {result['details']}")
        
        logger.info(f"\nüìä OVERALL RESULTS:")
        logger.info(f"   Tests passed: {passed_tests}/{total_tests}")
        logger.info(f"   Success rate: {passed_tests/total_tests:.2f}")
        logger.info(f"   Total time: {total_time:.2f} seconds")
        
        # Determine overall success based on Phase 1 requirements
        field_name_success = field_name_result['success'] if field_name_result else False
        escalation_success = escalation_result['success'] if escalation_result else False
        persistence_success = persistence_result['success'] if persistence_result else False
        
        phase1_success = field_name_success and escalation_success and persistence_success
        overall_success = phase1_success and (passed_tests >= (total_tests * 0.67))  # 67% pass rate + all core tests
        
        if phase1_success:
            logger.info("üéâ DYNAMIC RR INTEGRATION PHASE 1 SUCCESSFUL:")
            logger.info("   ‚úÖ Field names updated: trade_type and minimum_rr_threshold")
            logger.info("   ‚úÖ Dynamic RR escalation logic working")
            logger.info("   ‚úÖ Database persistence with new field names")
            if overall_success:
                logger.info("‚úÖ READY FOR PHASE 2: Advanced technical indicators integration")
            else:
                logger.info("‚ö†Ô∏è PARTIAL SUCCESS: Core functionality working but some issues need attention")
        else:
            logger.info("‚ùå DYNAMIC RR INTEGRATION PHASE 1 FAILED:")
            if not field_name_success:
                logger.info("   ‚ùå Field name validation failed")
            if not escalation_success:
                logger.info("   ‚ùå Dynamic RR escalation logic failed")
            if not persistence_success:
                logger.info("   ‚ùå Database persistence failed")
            logger.info("üö® REQUIRES IMMEDIATE ATTENTION: Phase 1 implementation not working correctly")
        
        return overall_success

async def main():
    """Main test execution function"""
    logger.info("üöÄ Starting Confluence Analysis Fix Testing Suite")
    logger.info("=" * 80)
    
    # Initialize test suite
    test_suite = ConfluenceAnalysisTestSuite()
    
    try:
        # Run all confluence analysis tests
        logger.info("Running Test 1: API Force IA1 Analysis - Confluence Values")
        await test_suite.test_1_api_force_ia1_analysis_confluence()
        
        logger.info("Running Test 2: API Analyses Endpoint - Confluence Consistency")
        await test_suite.test_2_api_analyses_confluence()
        
        logger.info("Running Test 3: Confluence Calculation Logic - Validation and Diversity")
        await test_suite.test_3_confluence_calculation_logic()
        
        logger.info("Running Test 4: Backend Logs Confluence Validation")
        await test_suite.test_4_backend_logs_confluence_validation()
        
        # Print final summary
        logger.info("\n" + "=" * 80)
        logger.info("üéØ CONFLUENCE ANALYSIS FIX TEST RESULTS SUMMARY")
        logger.info("=" * 80)
        
        passed_tests = sum(1 for result in test_suite.test_results if result['success'])
        total_tests = len(test_suite.test_results)
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        for result in test_suite.test_results:
            status = "‚úÖ PASS" if result['success'] else "‚ùå FAIL"
            logger.info(f"{status}: {result['test']}")
            if result['details']:
                logger.info(f"   Details: {result['details']}")
        
        logger.info(f"\nOverall Success Rate: {passed_tests}/{total_tests} ({success_rate:.1%})")
        
        if success_rate >= 0.67:  # 67% success threshold
            logger.info("üéâ MFI AND STOCHASTIC REMOVAL TESTING COMPLETED SUCCESSFULLY")
            return True
        else:
            logger.info("‚ùå MFI AND STOCHASTIC REMOVAL TESTING FAILED")
            return False
        
    except Exception as e:
        logger.error(f"‚ùå Test suite execution failed: {e}")
        return False


if __name__ == "__main__":
    asyncio.run(main())